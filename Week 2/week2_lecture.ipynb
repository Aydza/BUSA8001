{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Week 2 Lecture - Classification Algorithms (Part 1) {-}\n",
    "\n",
    "\n",
    "### Unit Convenor & Lecturer {-}\n",
    "[George Milunovich](https://www.georgemilunovich.com)  \n",
    "[george.milunovich@mq.edu.au](mailto:george.milunovich@mq.edu.au)\n",
    "\n",
    "### References {-}\n",
    "\n",
    "1. Python Machine Learning 3rd Edition by Raschka & Mirjalili - Chapter 2\n",
    "2. Various open-source material\n",
    "\n",
    "### Week 2 Learning Objectives {-}\n",
    "\n",
    "- Understanding the Classification Problem\n",
    "- Binary Classification and Artificial Neurons\n",
    "- Modelling Binary Classification\n",
    "- Numpy - Numerical Python library\n",
    "- The Perceptron Learning Algorithm\n",
    "    - Implementing Perceptron in Python\n",
    "    - Visualising Decision Boundries\n",
    "- Adaptive Linear Neurons - Adaline\n",
    "    - Minimizing Cost Functions with Gradient Descent\n",
    "    - Implementing Adaline in Python\n",
    "    - Hyperparameters\n",
    "- Improving Gradient Descent\n",
    "    - Feature Scaling\n",
    "    - Stochastic Gradient Descent\n",
    "    - \n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neurons & Binary Classification {-}\n",
    "\n",
    "**Biological** neurons are interconnected nerve cells in the brain that are involved in the processing and transmitting of chemical and electrical signals.\n",
    "\n",
    "\n",
    "<img src=\"images/pic1.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "<!-- ![](images/pic1.png) -->\n",
    "\n",
    "They are simple logic gates with binary (False/0 or True/1) outputs.   \n",
    "\n",
    "- If the accumulated signal exceeds a certain **threshold** the output signal (represented as 1 or True) is generated that will be passed on by the axon.  \n",
    "- This is the basic idea behind a binary classification task.   \n",
    "\n",
    "**Binary classification** is the task of classifying the elements of a given set into two groups/classes \n",
    "\n",
    "- E.g. we could have a **positive class** denoted 1 and a **negative class** denoted -1\n",
    "\n",
    "Some typical binary classification include   \n",
    "\n",
    "- Whether a bank customer defaults on a credit card payment   \n",
    "- Medical testing to determine if a patient has a certain disease or not   \n",
    "- A \"pass or fail\" test method or quality control in factories   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Binary Classification of the Iris Dataset {-}\n",
    "\n",
    "In Week 1 Lecture we discussed the Iris [dataset](https://archive.ics.uci.edu/ml/datasets/Iris/). \n",
    "\n",
    "It has 4 features on the basis of which we classify and 3 classes defining our label $y$.   \n",
    "In particular we have,\n",
    "\n",
    "- $x_1$ - Sepal Length in cm\n",
    "- $x_2$ - Sepal Width in cm\n",
    "- $x_3$ - Petal Length in cm\n",
    "- $x_4$ - Petal Width in cm\n",
    "\n",
    "- $y=\\{$Iris Setosa, Iris Versicolour, Iris Virginica$\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the dataset again and view the dataframe.\n",
    "\n",
    "\n",
    "In the cell below type in the following:\n",
    "\n",
    "```\n",
    "import pandas as pd  # import pandas library \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, \"display.width\", None) # pretty printing\n",
    "np.set_printoptions(precision=3, suppress = True)       # format printing to 3 decimal places in numpy\n",
    "\n",
    "\n",
    "column_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Class Label']  # define column names\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', names = column_names) # read data from URL\n",
    "df.to_excel('data/iris.xlsx')\n",
    "df.info()\n",
    "```\n",
    "\n",
    "We can also check the entire `Datafame` \n",
    "\n",
    "```\n",
    "df\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "Since we currently consider **Binary Classification** lets limit our analysis to two classes \n",
    "\n",
    "$y=\\left\\{ \n",
    "\\begin{array}{cc}\n",
    "1 & \\text{if versicolor} \\\\ \n",
    "-1 & \\text{if setosa} \\hfill\n",
    "\\end{array}\n",
    "\\right.$\n",
    "\n",
    "We will also limit our set of features to    \n",
    "\n",
    "- $x_1$ - Sepal Length   \n",
    "- $x_3$ - Petal Length (Lets call this $x_2$ for now for easier notation)   \n",
    "\n",
    "As seen above setosa and versicolor are contained in rows 0:99. There are 50 rows of setosa and 50 rows of versicolor.  \n",
    "So we will do the following:\n",
    "\n",
    "- Create a new DataFrame called df2 that contains rows 0:99   \n",
    "- Create a new column called $y$ as defined above   \n",
    "- Export our data from pandas DataFrame into NumPy arrays of y and X   \n",
    "\n",
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "\n",
    "\n",
    "In the cell below type in the following:\n",
    "\n",
    "```\n",
    "df2 = df.loc[0:99, ['Sepal Length', 'Petal Length', 'Class Label']].copy()  # copy selected data into df2\n",
    "df2['y'] = -1  # create a new variable y and give it values -1 for all observations\n",
    "df2.loc[df2['Class Label'] == 'Iris-versicolor', 'y'] = 1 # for observations where Class Label == Iris-versicolor set y = 1\n",
    "df2.head()\n",
    "\n",
    "y = df2['y'].values\n",
    "X = df2[['Sepal Length', 'Petal Length']].values\n",
    "\n",
    "print(type(y), y.shape)\n",
    "print(type(X), X.shape)\n",
    "\n",
    "print(y[:10])\n",
    "print(X[:10,:])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Lets now plot the data in a scatter plot**\n",
    "```\n",
    "plt.scatter(X[:50, 0], X[:50, 1], color = 'red', marker='o', label='setosa')\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1], color = 'blue', marker='x', label='versicolor')\n",
    "\n",
    "plt.xlabel ('Sepal Length [cm]')\n",
    "plt.ylabel('Petal Length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Modelling Binary Classification {-}   \n",
    "\n",
    "So in this situation we wish to predict whether a flower is either a setosa or a versicolor      \n",
    "\n",
    "$y=\\left\\{ \n",
    "\\begin{array}{cc}\n",
    "1 & \\text{if versicolor} \\\\ \n",
    "-1 & \\text{if setosa} \\hfill\n",
    "\\end{array}\n",
    "\\right.$\n",
    "\n",
    "on the basis of its two features:\n",
    "\n",
    "- $x_1$ - Sepal Length   \n",
    "- $x_2$ - Petal Length   \n",
    "\n",
    "Now consider predicting $y$ (lets call the prediction $z$) on the basis of $x_1$ and $x_2$ in the context of the linear regression:   \n",
    "\n",
    "$z=w_0+w_1x_1 +  w_2x_2$,  \n",
    "\n",
    "- $w_1$ and $w_2$ are estimated coefficients, also called **weights** (beta coefficients in the regression context)   \n",
    "- $w_0$, i.e. the intercept, is called the **bias unit** in ML literature   \n",
    "\n",
    "\n",
    "We can see that *z* is not going to be a great predictor of y because *z* is almost never going to be equal to 1 or -1. \n",
    "\n",
    "So now we introduce a **decision function** $\\phi(z)$ which takes a form of a **unit step function** in the perceptron algorithm:   \n",
    "\n",
    "<br>\n",
    "\n",
    "$\\phi(z)=\\left\\{ \n",
    "\\begin{array}{cc}\n",
    "1 & \\text{if } z \\ge 0 \\hfill \\\\ \n",
    "-1 & \\text{otherwise.}\n",
    "\\end{array}\n",
    "\\right.$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Using this notation we summarise the model as follows:   \n",
    "\n",
    "\n",
    "<br>\n",
    "<hr style=\"width:50%;margin-left:0;\">\n",
    "\n",
    "$z=w_0 + w_1x_1 +  w_2x_2=\\sum_{j=0}^{2}w_jx_j$\n",
    "\n",
    "$\\phi(z)=\\left\\{ \n",
    "\\begin{array}{cc}\n",
    "1 & \\text{if } z \\ge 0 \\hfill \\\\ \n",
    "-1 & \\text{otherwise,}\n",
    "\\end{array}\n",
    "\\right.$\n",
    "\n",
    "\n",
    "<hr style=\"width:50%;margin-left:0;\">\n",
    "\n",
    "**Summary:** Decision function $\\phi(z)$ squashes the net input $z=\\sum_{j=0}^{2}w_jx_j$ into a binary output $(-1 \\text{ or } 1)$ which can be used to discriminate between **two linearly separable classes**, as shown below.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/pic2.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "<!-- ![](images/pic2.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** \n",
    "The above algorithm will settle on certain values of $w$, i.e. converge, only if the two classes are **linearly separable** \n",
    "- Linearly separable means that the classes can be separated by a linear decision boundry \n",
    "\n",
    "<img src=\"images/pic3.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "<!-- ![](images/pic3.png) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:40%;margin-left:0;\">\n",
    "\n",
    "### Summary of the Perceptron {-}\n",
    "\n",
    "<img src=\"images/pic4.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "<!-- ![](images/pic4.png) -->\n",
    "\n",
    "\n",
    "1. Perceptron receives the inputs of an example, $x$   \n",
    "2. $x$ is combined with weights $w$ to get net input   \n",
    "3. Net input is passed on the the threshold function which produces a prediction - class label of +1 or -1   \n",
    "4. Prediction errors are computed as $y - \\phi(z)$   \n",
    "5. The algorithm learns by updating weights in order to minimise prediction errors   \n",
    "\n",
    "\n",
    "### Extending Perceptron to a Multi-Class Classification {-}\n",
    "The `perceptron` algorithm can be extented to multi-class (more than two classes) classification using the **one-vs.-all (OvA)**/**one-vs.rest (OvR)** technique.   \n",
    "\n",
    "- If there are *N* classes then we train *N* separate classifiers.   \n",
    "- So we end up with one classifier per class, where a particular class is treated as a positive class and the examples from all other classes are considered negative classes.   \n",
    "- When classifying an example from a new dataset we would assign the class label that is associated with the largest absolute net input value $z=\\sum_{j=0}^{k}w_jx_j$ across all *N* classifiers.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Implementing Perceptron in Python {-}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code we crate a class called ```Perceptron```, which is trained using a ```fit``` method, and creates predictions using a ```predict``` method. \n",
    "- You are not required to know how to code a perceptron algorithm like this yourselves, but now you should have the ability to read it under understand its building blocks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    ''' Perceptron classifier\n",
    "    \n",
    "    Parameters: \n",
    "    ----------\n",
    "    eta: learning Rate between 0.0 and 1.0\n",
    "    n_iter: to go over the training set\n",
    "    random_state: random number generator seed for random weight initialisation\n",
    "    \n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    w_ : weights after fitting\n",
    "    errors_ number of misclassifications in each epoch\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, eta=0.01, n_iter=10, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        ''' Fit training data\n",
    "    \n",
    "        Paramters:\n",
    "        ----------\n",
    "        X: shape = [number_examples, number_features] - training data\n",
    "        y: shape = [number_examples] - target values\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self: object\n",
    "        '''\n",
    "\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])  # scale is standard deviation\n",
    "        self.errors_ = []\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            print(f'---------------- Epoch:{_} -----------------')\n",
    "            errors = 0\n",
    "            for i, (xi, target) in enumerate(zip(X, y)): # for each row in zip(X,y)\n",
    "                update = self.eta * (target - self.predict(xi)) # for each row xi\n",
    "                errors += int(update != 0)\n",
    "             #   print(f'observation: {i}, x: {xi}, w: {self.w_} y: {target}, prediction: {self.predict(xi)}, update factor: {update}, errors: {errors}')\n",
    "                self.w_[1:] = self.w_[1:] + update*xi \n",
    "                self.w_[0] = self.w_[0] + update  \n",
    "            self.errors_.append(errors)\n",
    "        print(len(self.errors_))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):   # X is a row of X\n",
    "        ''' Calculate net input '''\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]  # np.dot function computes the vector dot (inner) product w'x\n",
    "    \n",
    "    def predict(self, X):  # X is a row of X\n",
    "        ''' Return class label after unit step '''\n",
    "        return np.where(self.net_input(X) >= 0, 1, -1)\n",
    "    \n",
    "    def print_weights(self):\n",
    "        print(f'Optimized weights: {self.w_}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our class is ready we will use it by   \n",
    "\n",
    "1. Creating an instance of the `Perceptron` class   \n",
    "\n",
    "```\n",
    "ppn = Perceptron(eta=0.1, random_state = 1) \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train the instance on our dataset\n",
    "\n",
    "```\n",
    "ppn.fit(X, y)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print the trained weights\n",
    "\n",
    "\n",
    "```\n",
    "print(ppn.print_weights())\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Print predictions together with true y and prediction error\n",
    "\n",
    "```\n",
    "\n",
    "predictions_df = pd.DataFrame(ppn.predict(X), columns = ['predictions'])\n",
    "\n",
    "predictions_df[['x1', 'x2']] = X\n",
    "\n",
    "predictions_df['true_y'] = y\n",
    "\n",
    "predictions_df['prediction error'] = predictions_df['predictions'] - predictions_df['true_y']\n",
    "\n",
    "predictions_df.head()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Summary {-}   \n",
    "\n",
    "In the above code we did the following:   \n",
    "\n",
    "- Initialise a new ```Perceptron``` object, and set eta and random_state   \n",
    "- ```fit``` method \n",
    "    - Initialise the weights by setting them to small random numbers, \n",
    "    - Loops over all examples in the training dataset and updates the weights according to the perceptron learning rule   \n",
    "- Class labels are predicted by the ```predict``` method, which is called in the ```fit``` method, but can also be used to predict class labels of new data   \n",
    "- Number of misclassifications during each epoch are collected in the ```self.errors_``` list   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Visualising Decision Boundries \n",
    "\n",
    "- The code below provides a ``plot_decision_regions`` function to visualise the decision boundries for two-dimensional datasets.   \n",
    "- In order to use the code we just need to set the values for the following parameters  \n",
    "    - X\n",
    "    - y\n",
    "    - classifier\n",
    "    \n",
    "\n",
    "```\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ['red', 'blue', 'lightgreen', 'gray', 'cyan']  # GM: changed this from tuple to list\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class examples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=cl, \n",
    "                    edgecolor='black')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will now use the above ```plot_decision_regions``` function to plot our classified data together with decision regions\n",
    "```\n",
    "plot_decision_regions(X=X, y=y, classifier=ppn)\n",
    "\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Training a Perceptron with `scikit-learn` {-}    \n",
    "\n",
    "In practice we will not need to code our algorithms, e.g. `perceptron`, from scratch in Python and NumPy.    \n",
    "\n",
    "Although a good exercise to gain a better understanding of various algorithms we are not software engineers  \n",
    "- Our code in the best case scenario will be inefficient, and in the worst case incorrect   \n",
    "\n",
    "We are going to use [`scikit-learn`](https://scikit-learn.org/stable/) which has many machine learning algorithms programmed in it   \n",
    "- Scikit-learn is a free software machine learning library for the Python programming language. \n",
    "- It contains various classification, regression and clustering algorithms   \n",
    "\n",
    "\n",
    "Lets try to repeat the previous exercise using `sklearn` Perceptron class   \n",
    "\n",
    "```\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn2 = Perceptron(eta0=0.1, random_state=1)\n",
    "ppn2.fit(X, y)\n",
    "\n",
    "print('--- Estimated Weights ---')\n",
    "print('intercept:\\n', ppn2.intercept_)\n",
    "print('coefficients:\\n', ppn2.coef_)\n",
    "print(ppn2.predict(X))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Adaptive Linear Neurons {-}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron can be viewed as a single-layer **neural network**. An extension of the perceptron algorithm is Adaptive Linear Neuron (Adaline)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Linear Neuron (Adaline) {-}\n",
    "\n",
    "The **key difference** between the Adaline rule and the Perceptron is that the weights are updated based on a **linear activation function** rather than a unit step function like in the perceptron model.   \n",
    "\n",
    "- In Adaline the linear activation function $\\phi(z)$ is the identity function of the net input $\\phi(z)=\\phi(\\sum_{j=0}^{k}w_jx_j)=\\sum_{j=0}^{k}w_jx_j=z$    \n",
    "- The algorithm compares the true class labels with the linear activation function's continuous valued output to compute the model error and update the weights   \n",
    "    - In contrast, the perceptron compares the true class labels with the predicted class labels   \n",
    "- A threshold function is still used to make final predictions (unit step function as in perceptron)   \n",
    "\n",
    "<img src=\"images/pic5.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "<!-- ![](images/pic5.png) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Minimizing Cost Functions with Gradient Descent {-}\n",
    "\n",
    "Model parameters, i.e. elements of the weights vector $w$, are often chosen optimally during the learning process by minimizing an **objective function**. \n",
    "\n",
    "- This objective function, also known as the **cost function** - $J(w)$ - is just the **sum of squared errors (SSE)** (we know what this means from the OLS model)   \n",
    "<br>\n",
    "\n",
    "$J(w)=\\frac{1}{2}\\sum_{i=1}^n(y^{(i)}-\\phi(z^{(i)})^2=\\frac{1}{2}\\sum_{i=1}^n(y^{(i)}-z^{(i)})^2=\\frac{1}{2}\\sum_{i=1}^n(y^{(i)}-\\sum_{j=0}^{k}w_jx_j)^2$\n",
    "\n",
    "<br>\n",
    "\n",
    "1. $\\frac{1}{2}$ is just added for convenience - will make deriving the gradient easier   \n",
    "2. Main advantage of a linear activation function (in contrast to the unit step function) is that it makes the cost function differentiable -> can compute derivatives analytically    \n",
    "3. SSE is convex - there is a global cost minimum -> there are optimal weights which minimize the cost function -> use **gradient descent** (optimization algorithm) to find optimal weights   \n",
    "\n",
    "<img src=\"images/pic6.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "<!-- ![](images/pic6.png) -->\n",
    "\n",
    "\n",
    "- Start with an initial weight $w$   \n",
    "- Step size determined by the value of the **learning rate**   \n",
    "- Update weights in the **opposite direction** of the gradient   \n",
    "\n",
    "\n",
    "In the case of the linear activation function of Adaline the weight update is computed on the basis of all examples (observations) in the training dataset, which is referred to as **batch gradient descent**. \n",
    "- Note that in the case of perceptron the updating was done incrementally after each training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Implementing Adaline in Python {-}\n",
    "\n",
    "We can reuse the perceptron code to implement Adaline. \n",
    "\n",
    "- Need to do is change the ```fit``` method and add a ```cost_``` attribute.   \n",
    "- `activation` function is a simple identity function here and it doesn't have an effect on the estimation of the model   \n",
    "    - It is is used to illustrate the principle of activation function when we employ nonlinear activation functions in later chapters.   \n",
    "\n",
    "\n",
    "```\n",
    "class AdalineGD:\n",
    "    ''' ADaptive LInear NEuron (Adaline) classifier\n",
    "    \n",
    "    Paramters: \n",
    "    ----------\n",
    "    eta: learning Rate between 0.0 and 1.0\n",
    "    n_iter: to go over the training set\n",
    "    random_state: random number generator seed for random weight initialisation\n",
    "    \n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    w_ : weights after fitting\n",
    "    cost_: sum of squares cost function value in each epoch\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        ''' Fit training data\n",
    "    \n",
    "        Paramters:\n",
    "        ----------\n",
    "        X: shape = [number_examples, number_features] - training data\n",
    "        y: shape = [number_examples] - target values\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self: object\n",
    "        '''\n",
    "\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])  # scale is standard deviation\n",
    "#         self.w_ = rgen.normal(loc=0.0, scale=0.0, size=1 + X.shape[1])  # scale is standard deviation\n",
    "        \n",
    "        self.cost_ = []\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "#             print(f'---------------- Epoch:{_} -----------------')\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = (y - output)\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
    "            self.w_[0] += self.eta * errors.sum()\n",
    "            cost = (errors**2).sum() / 2.0\n",
    "            self.cost_.append(cost)\n",
    "    \n",
    "        print(self.w_)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):   # X is a row of X\n",
    "        ''' Calculate net input '''\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]  # np.dot function computes the vector dot (inner) product w'x\n",
    "    \n",
    "    def activation(self, X):\n",
    "        '''Compute linear activation'''\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):  # X is a row of X\n",
    "        ''' Return class label after unit step '''\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0, 1, -1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the AdalineGD class we can use it as follows\n",
    "\n",
    "```\n",
    "ada = AdalineGD(n_iter=300, eta=0.0001, random_state = 2)\n",
    "\n",
    "ada.fit(X,y) # function fit will optimise the weights -> find optimal weights \n",
    "\n",
    "ada.predict(X)   # function predict will make the prediction given X \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Hyperparameters {-}\n",
    "\n",
    "The **learning rate** ($\\eta$) and the **number of epochs** are **hyperparameters** (tuning parameters) in perceptron (as well as in Adaline below).    \n",
    "- An **epoch** in machine learning means one complete pass of the training dataset through the algorithm\n",
    "    - Multiple epochs are often required for the model to converge to a satisfactory level of performance\n",
    "    - The number of epochs needed can vary significantly depending on the complexity of the model\n",
    "    - Too few epochs can result in an underfitted model that performs poorly on both training and unseen data\n",
    "    - Too many epochs can lead to overfitting, where the model performs well on the training data but poorly on new, unseen data\n",
    "- These parameters are not fitted (estimated) in the same way that the rest of the parameters, i.e. weights, are in the model.   \n",
    "- **Hyperparamters are usually set by the investigator before the fitting of the model is done.**  \n",
    "- There are techniques for automatically chosing the values of hyperparameters that produce optimal performance.   \n",
    "    - We will consider these later in the course.\n",
    "\n",
    "Note that if the learning rate is too high we may never reach the minimum value of the cost function, as we will overshoot the minimum.  \n",
    "\n",
    "<img src=\"images/pic7.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "<!-- ![](images/pic7.png) -->\n",
    "\n",
    "\n",
    "**Learning rates**  \n",
    "\n",
    "- Can impact the values of optimised parameters \n",
    "    - This in turn can impact the forecast accuracy   \n",
    "- Try several different learning rates   \n",
    "- If the model is trully optimised (minimum cost function is found) parameters should not change   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Improving Gradient Descent {-}\n",
    "\n",
    "- Feature Scaling  \n",
    "- Stochastic Gradient Descent   \n",
    "- Mini-Batch Gradient Descent  \n",
    "- Implementing Adaline with Stochastic Gradient Descent  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling {-}\n",
    "\n",
    "**Feature Scaling** is method used to transform the range of independent variables or features of data. \n",
    "\n",
    "- Feature scaling, e.g. standarization, leads to quicker convergence of optimization algorithms such as gradient descent.  \n",
    "\n",
    "Feature **standardization** makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. \n",
    "\n",
    "If $x \\sim(\\mu, \\sigma^2)$ then $z=\\frac{x-\\mu}{\\sigma}$ is distributed as $z\\sim(0,1)$\n",
    "\n",
    "Try the following:\n",
    "\n",
    "\n",
    "```\n",
    "X_stand = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "\n",
    "print(X_stand[:10])\n",
    "print(X_stand.mean(axis=0))\n",
    "print(X_stand.std(axis=0))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "Lets compare our old and new data \n",
    "\n",
    "```\n",
    "plt.plot(X)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(X_stand)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lets train Adaline again on standardized data\n",
    "\n",
    "```\n",
    "ada_stand = AdalineGD(n_iter= 200, eta=0.01)\n",
    "ada_stand.fit(X_stand, y)\n",
    "\n",
    "# plot_decision_regions(X, y, classifier = ada_stand)\n",
    "plot_decision_regions(X_stand, y, classifier=ada_stand)\n",
    "plt.title('Adaline - Gradient Descent')\n",
    "plt.xlabel('Sepal Length [standardized]')\n",
    "plt.ylabel('Petal Length [standarized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(ada_stand.cost_) + 1), ada_stand.cost_, marker = 'o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sum Squared Errors')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Estimating Linear Regression in Python {-}\n",
    "\n",
    "Python has several libraries that implement Ordinary Least Square (OLS) method to fitting a linear regression to data.\n",
    "\n",
    "- `statsmodels` is a libarary that implements many *statistics* and *econometrics* models.   \n",
    "- Lets use it and estimate a linear regression on our data and confirm that Adaline indeed minimises the Sum of Squred Errors.  \n",
    "\n",
    "```\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X2 = sm.add_constant(X_stand)\n",
    "print(X2[:10])\n",
    "\n",
    "model = sm.OLS(y,X2)\n",
    "results = model.fit()\n",
    "\n",
    "results.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Stochastic Gradient Descent {-}\n",
    "\n",
    "Previously we considered computing the gradient using the entire training dataset - a method called **batch gradient descent**.  \n",
    "\n",
    "- In some machine learning applications we have millions of datapoints -> batch gradient descent is computionally costly.  \n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** (iterative or online gradient descent) updates the weights incrementally for each training example (observation):   \n",
    "\n",
    "$\\Delta w=\\eta(y^{(i)} - \\phi(z^{(i)})x^{(i)}$\n",
    "\n",
    "SGD:\n",
    "\n",
    "- is an approximation of gradient descent   \n",
    "- typically reaches convergence faster because of more frequent updates  \n",
    "- does not always converge to global maximum but an area close to it   \n",
    "- can escape local minima when dealing with nonlinear cost functions  \n",
    "- useful for dealing with online data where new data arrives in real time e.g. financial data or web applications  \n",
    "\n",
    "With SGD it is important to:\n",
    "\n",
    "- use data in a random order  \n",
    "- shuffle the training dataset for every epoch to prevent cycles  \n",
    "- don't use a fixed learning rate but rather and adaptive rate decreases over time  \n",
    "\n",
    "### Mini-Batch Gradient Descent {-}\n",
    "\n",
    "- Use subsets of training data to compute the gradient, e.g 10 observations at a time.   \n",
    "- Advantage over gradient descent: convergence is reached faster beause of more frequent weights update.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
