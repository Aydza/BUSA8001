{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51973ae3-b650-4cf2-9d33-b9568cf31c54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Week 7: Best Practices for Model Evaluation and Hyperparameter Tuning\n",
    "\n",
    "\n",
    "### Unit Convenor & Lecturer   \n",
    "[George Milunovich](https://www.georgemilunovich.com)  \n",
    "[george.milunovich@mq.edu.au](mailto:george.milunovich@mq.edu.au)\n",
    "\n",
    "### References    \n",
    "\n",
    "1. Python Machine Learning 3rd Edition by Raschka & Mirjalili - Chapter 6\n",
    "2. Various open-source material\n",
    "\n",
    "### Week 7 Learning Objectives  \n",
    "\n",
    "1. Streamlining Workflows with Pipelines\n",
    "2. The Holdout Method and K-Fold Cross-Validation to Assess Model Performance\n",
    "3. Debugging algorithms with learning and validation curves\n",
    "4. Fine-tuning machine learning models via grid search\n",
    "5. Looking at different performance evaluation metrics\n",
    "  - The confusion matrix\n",
    "  - Optimizing the precision and recall of a classification model\n",
    "  - Plotting a receiver operating characteristic\n",
    "  - The scoring metrics for multiclass classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36d6ec-8f88-492c-a72f-891f956402dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "First, lets update `seaborn` first to make sure our plots look right\n",
    "- Before running any of the code in this notebook execute the following command\n",
    "\n",
    "```\n",
    "!pip install seaborn --upgrade\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a84d2a-dc29-49c8-858e-b492b1d12076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.12.2)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas>=1.2->seaborn) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "  Attempting uninstall: seaborn\n",
      "    Found existing installation: seaborn 0.12.2\n",
      "    Uninstalling seaborn-0.12.2:\n",
      "      Successfully uninstalled seaborn-0.12.2\n",
      "Successfully installed seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a049d2-9309-4fd8-9346-c94bed8cbb31",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Streamlining Workflows with Pipelines {-}\n",
    "\n",
    "- **Pipelines** (in scikit-learn) are tools for combining multiple preprocessing steps into a single scikit-learn estimator   \n",
    "    - Bundle together a sequence of data transforms along with a final estimator\n",
    "- The primary advantages and uses of pipelines in scikit-learn include:\n",
    "    - **Simplicity and Convenience**: Instead of running multiple steps manually, like fit and transform on the training set, then transform on the test set, pipelines allow you to encapsulate this workflow into a single object. This simplifies the process, especially when dealing with complex data transformations and modeling steps.\n",
    "    - **Reproducibility**: Pipelines make preprocessing steps explicit, ensuring that transformations are applied consistently. This is crucial for reproducibility and for avoiding mistakes like applying transformations before fitting the models or forgetting to apply the same transformations on the test set as on the training set.\n",
    "    - **Code Maintenance**: By having preprocessing and modeling steps within a pipeline, the code is easier to maintain. If a step needs to be added or changed, it can be done in one place, and the pipeline ensures that all steps are applied in the correct order.\n",
    "    - **Parameter Tuning**: Pipelines allow for the setting and tuning of parameters for both the preprocessing steps and the final model simultaneously using grid search or random search. This integrated approach can lead to better model performance since it considers the preprocessing steps as part of the hyperparameter tuning process.\n",
    "    \n",
    "---\n",
    "\n",
    "\n",
    "- `make_pipeline` function takes an arbitrary number of scikit-learn transformers (objects which support `fit` and `transform` methods) followed by an estimator that implements `fit` and `predict` methods\n",
    "- `fit` method of `Pipeline` will pass the data down a series of transformers via `fit` and `transform` calls, until it reaches the estimator object\n",
    "    - The estimator will then be fitted to the transformed training data   \n",
    "- `predict` method of `Pipeline` will pass the data through the intermediate steps via `transform` calls\n",
    "    - In the final step the estimator wil return a prediction on the transformed data\n",
    "    \n",
    "<img src=\"images/06_01.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<!-- Image(filename='images/06_01.png', width=500)  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ec144b-f6dd-4b18-bda8-07ca6ace3144",
   "metadata": {},
   "source": [
    "\n",
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "### Loading the Breast Cancer Wisconsin dataset {-}\n",
    "\n",
    "Breast Cancer Wisconsin dataset\n",
    "- 569 examples of malignant or benign tumor cells\n",
    "- 1st column - unique ID number of patient\n",
    "- 2nd column - diagnosis: M = Malignant, B = Benign\n",
    "- Columns 2 - 31 represent 30 features computed from digitized images of the cell nuclei used to build a model to predict whether a tumor is benign or malignant\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', header=None)\n",
    "# df.to_csv('data/wdbc.data', index=False, header = None)\n",
    "\n",
    "# if the Breast Cancer dataset is temporarily unavailable from the\n",
    "# UCI machine learning repository, un-comment the following line\n",
    "# of code to load the dataset from a local path:\n",
    "\n",
    "df = pd.read_csv('data/wdbc.data', header = None)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(10)\n",
    "\n",
    "df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ffb237-a0df-427f-adf9-7dc46350456a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ce75056-f9ef-4a9b-b4c3-8d629e77d9e2",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "<span style='background:orange'>  Example: Combining transformers and estimators in a pipeline   \n",
    "\n",
    "1. Create $X$ and $y$ vectors \n",
    "2. Encode target with `LabelEncoder`\n",
    "3. Split dataset into train & test (20%) datasets stratifying by $y$   \n",
    "4. Use `make_pipeline` to\n",
    "    - Standardize data (`StandardScaler`),\n",
    "    - Extract two principal components (`PCA`)\n",
    "    - Logistic Regression (`LogisticRegression`)\n",
    "    \n",
    "5. Fit to train dataset and compute accuracy on training data\n",
    "6. Make predictions using test data and compute accuracy on test data \n",
    "7. Repeat steps 4 - 6 by doing step-by-step of\n",
    "    - Scaling data\n",
    "    - Extracting principal components\n",
    "    - Fitting Logistic Regression\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc4bf96-0904-46c0-9414-9c357e528ead",
   "metadata": {},
   "source": [
    "1. Create $X$ and $y$ variables \n",
    "\n",
    "```\n",
    "print(df.shape)\n",
    "\n",
    "y = df.loc[:, 1].values\n",
    "X = df.loc[:, 2:].values\n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "y[:25]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26c6d2-785d-4acf-8252-aa920d20c757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "166df5af-d66c-4ded-9c0b-74a3854c19f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2. Encode target with `LabelEncoder`\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(le.classes_)\n",
    "y  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17dcd8f-6ed3-451f-9720-e4439ac85af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74fefc55-578e-4553-9eff-e9dc0df5e8b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "3. Split dataset into train & test (20%) datasets stratifying by $y$   \n",
    "\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=1)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07781b36-3ed4-4b7c-a32c-cad7783b7d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc59ca3c-e0fa-4ac2-a029-2dc344a8c31a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "4. Use `make_pipeline` to\n",
    "    - Standardise data (`StandardScaler`),\n",
    "    - Extract two principal components (`PCA`)\n",
    "    - Logistic Regression (`LogisticRegression`)\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe_lr = make_pipeline(StandardScaler(),\n",
    "                        PCA(n_components=2),\n",
    "                        LogisticRegression(random_state=1, C=1.5))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9aae1e-5094-4508-8245-bb7c4150309f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9dbda2-75c2-408d-9fc6-0a1535eb91c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27dffdba-fe08-4cd6-b405-35c798068c9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "5. Fit to train dataset and compute accuracy on training data\n",
    "\n",
    "```\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "print(f'Test Accuracy: {pipe_lr.score(X_train, y_train):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94c3bb0-2ad4-47a1-ba0a-3367d72fce6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1644cc6-0f98-4f7a-9867-0aaa8a0898bc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "6. Make predictions using test data and compute accuracy on test data \n",
    "\n",
    "\n",
    "```\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "print(f'Test Accuracy: {pipe_lr.score(X_test, y_test):.3f}')\n",
    "\n",
    "# y_pred = pipe_lr.predict(X_test[0:1, :])\n",
    "# y_pred\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250d87b-8dfe-4a28-8eda-03abd4d2f4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263489b4-5c3c-4034-8b05-67abcc7f0a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bc4a443-f01f-44b3-9b5c-8448eb290d37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "7. Repeat steps 4 - 6 by doing step-by-step of\n",
    "    - Scaling data\n",
    "    - Extracting principal components\n",
    "    - Fitting Logistic Regression\n",
    "    \n",
    "    \n",
    "```\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X_train_scaled_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_scaled_pca = pca.transform(X_test_scaled)    \n",
    "\n",
    "lr = LogisticRegression(random_state=1, C=1.5)\n",
    "lr.fit(X_train_scaled_pca, y_train)\n",
    "print(f'Train Accuracy: {lr.score(X_train_scaled_pca, y_train):.3f}')\n",
    "\n",
    "print(f'Test Accuracy: {lr.score(X_test_scaled_pca, y_test):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9aa8d-7069-41b6-be68-a701a5e15f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ca441-71df-44ba-8367-feb7d768d037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b79439-e1db-4858-bd5e-db0ad18fdb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b39316e-a6b7-4104-8851-4de0f692b442",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# The Holdout Method and K-Fold Cross Validation to Assess Model Performance {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce417fae-0fc4-48d5-afe7-082758b49dcd",
   "metadata": {},
   "source": [
    "- Choosing the best possible model for forecasting relies on \n",
    "    - Training the model on the training dataset\n",
    "    - Testing how well the model generalises on test data\n",
    "    - However... \n",
    "        - Also need to **tune hyperparameters** and compare different settings to further improve performance on unseen data\n",
    "      \n",
    "        \n",
    "A **hyperparameter** is a parameter whose value is used to control the learning process which is set prior to fitting the model   \n",
    "- By contrast, the values of other parameters (weights) are computed via training/fitting\n",
    "\n",
    "  \n",
    "**Model selection**: Selection of optimal values of **tuning parameters** (hyperparameters)\n",
    "- Problem: Need to be able to try different values of hyperparameters on the test dataset\n",
    "- *If we choose a hyperparameter value based on test dataset performance of different values then the test data become part of our training dataset and thus the model will be more likely to overfit*\n",
    "- Solutions:\n",
    "  - Holdout Cross-Validation\n",
    "  - K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ac776-d108-48ee-b87e-9ef3066193f3",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "## The Holdout Method {-}\n",
    "\n",
    "- Split data into three parts:\n",
    "1. Training dataset - fit different models and same models with different hyperparameter values\n",
    "2. Validation dataset\n",
    "    - **Model Selection**: repeatedly evaluate the performance of the model after training using different hyperparameter values\n",
    "        - Choose best hyperparameter value as the value that gives best accuracy on the validation dataset\n",
    "4. Test dataset - unseen data used to estimate final model's ability to generalise to new data\n",
    "\n",
    "  \n",
    "- **Disadvantage**: \n",
    "    - Performance estimate is sensitive to how we partition the training dataset into the training and validation subsets\n",
    "\n",
    "<img src=\"images/06_02.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7149eb5-a3e5-4694-98d9-c67eade90420",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "## K-fold Cross-Validation {-}\n",
    "\n",
    "- K-fold cross-validation is a widely used method for evaluating the performance of machine learning models\n",
    "- It aims to provide a more reliable assessment of performance (e.g. accuracy) than the simple holdout method\n",
    "- Cross-validation addresses the dependency of accuracy on a particular train-test split and the reduced amount of data used for training the model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37799a30-0051-49c2-a96b-541a46c3e141",
   "metadata": {},
   "source": [
    "\n",
    "### How cross-validation works\n",
    "\n",
    "- Split the **training set** into $k$ folds without replacement\n",
    "    - $k-1$ folds are used for model training\n",
    "    - 1 fold is used for performance evaluation\n",
    "- Repeat this procedure $k$ times until all $k$ folds are used for evaluation (and training)\n",
    "    - *Compute average performance across all k folds to obtain a performance estimate that is less sensitive to sub-partitioning of the training data* \n",
    "- Once best hyperparameter values are found **retrain the model** on the complete training set (in order to fit on a maximum number of observations)\n",
    "    - Final performance accuracy is computed using the independent test dataset\n",
    "    \n",
    "      \n",
    "Advantages of K-fold Cross-Validation    \n",
    "- Averaging model performance (accuracy) across different validation datasets results in a more **precise** measure of performance, e.g. accuracy will be more precisely computed\n",
    "- Each example (observation) is used for validation exactly once\n",
    "    \n",
    "   \n",
    "In the image below $E_i$'s represent Estimated Performance measure (e.g accuracy, for other performance see last part of today's lecture)\n",
    "\n",
    "<img src=\"images/06_03.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "- Typically we set $k=10$\n",
    "    - If we work with a **small dataset** increase $k$ in order to be able to train the model on more data\n",
    "        - If very small dataset use $k=n$. This is called Leave-one-out cross-validation (LOOCV)\n",
    "    - If we work with a **larger dataset** we can decrease $k$, e.g. $k=5$\n",
    "    - If we have unequal class proportions do **stratified cross-validation**\n",
    "        - Use `from sklearn.model_selection import StratifiedKFold`\n",
    "\n",
    "\n",
    "Previously we computed training set accuracy to be 0.954\n",
    "- Lets re-compute this accuracy using K-Fold Cross-Validation\n",
    "    - Note: in this example we do not tune hyperparameters yet\n",
    "    - We just want to see how to average validation accuracies for a given value of the hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f385c80-8531-4758-a8bd-ec1b58d1c4fc",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "In practice we are likely to use `from sklearn.model_selection import cross_val_score`\n",
    "- `cross_val_score` library implements a **k-fold cross-validation** scorer\n",
    "- Allows us to do **stratified k-fold cross-validation** with less code\n",
    "    - We don't need to loop through folds using `for`\n",
    "- [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores_v2 = cross_val_score(estimator=pipe_lr, X=X_train, y=y_train, cv=10, n_jobs=1)\n",
    "\n",
    "# print(scores_v2)\n",
    "\n",
    "print(f'CV accuracy scores\\n {scores_v2.reshape(-1,1)}')\n",
    "print(f'CV accuracy: {np.mean(scores_v2):.3f} +/- {np.std(scores_v2):.3f}')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec92c00-c807-46d4-8e7d-48838b4b13a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c972b5a-c3eb-4495-b80c-6c108b14d969",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Fine-Tuning Hyperparameter via Grid Search {-}\n",
    "\n",
    "There are two types of parameters in Machine Learning\n",
    "- Weights which are learned (estimated) from the training data\n",
    "- Hyperparameters (tuning parameters) which are set by the investigator, e.g. $C$ regularization parameter in Logistic Regression \n",
    "    - Different values of hyperparameters result in different forecast accuracy\n",
    "    - How can we choose best hyperparameter values?\n",
    "        - Try a lot of different values \n",
    "        - **Grid Search** - a popular hyperparameter optimization technique\n",
    "    \n",
    "      \n",
    "<hr style=\"width:35%;margin-left:0;\">    \n",
    "\n",
    "\n",
    "\n",
    "## Tuning hyperparameters via grid search {-}\n",
    "\n",
    "Hyperparameters are the configuration settings used to structure a machine learning model (for example, the learning rate or the number of decision trees in a random forest).  \n",
    "- Not learned from the data but are set prior to the training process\n",
    "- Have a significant impact on the performance of the model \n",
    "\n",
    "*Grid search automates the process of finding the best combination of hyperparameters by exhaustively searching through a specified subset of hyperparameters.*\n",
    "- Brute-force exhaustive search method\n",
    "    - Systematically list a lot of (sometimes all) possible values for the solution and check which value provides best solution  \n",
    "- Specify a list of values for different hyperparameters\n",
    "    - This list is a grid of values we are going to search for the best value\n",
    "- *Algorithm evaluates the model performance for each combination to obtain the **optimal combination** of hyperparameter values from the specified set*\n",
    "- `from sklearn.model_selection import GridSearchCV`\n",
    "- It is also possible to use `RandomizedSearchCV` to randomly sample parameter combinations via randomized search\n",
    "    - It has been shown that if we sample 60 parameter combinations we have 95% chance of obtaining solutions within 5% of the optimal performance \n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "\n",
    "\n",
    "<span style='background:orange'>   **Example**\n",
    "- Use `GridSearchCV` with a Support Vector Machine classifier\n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "- Optimize the following hyperparameters\n",
    "    - Regularization parameters $C$ - inverse of regularization strength\n",
    "    - Kernel: linear and rbf\n",
    "    - $\\gamma$ parameter for rbf kernel\n",
    "- Print parameters and accuracy of the best model\n",
    "    - Export best model and print accuracy on the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ae3eb-f54c-4d20-a609-b69e5439b7a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1. Create a `pipeline` containing\n",
    "- `StandardScaler`\n",
    "- `SVC`\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe_svc = make_pipeline(StandardScaler(),\n",
    "                         SVC(random_state=1, probability=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504baeb7-ea8c-4a02-8e55-b2bd172a54c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a908c5e-8398-4029-99fb-37d27dd82d79",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2. Define a parameter range & parameter grid\n",
    "\n",
    "```\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]  # range of values for C and gamma\n",
    "\n",
    "\n",
    "param_grid2 = [{'svc__C': param_range,   # range of values for all parameters\n",
    "               'svc__kernel': ['linear']},\n",
    "              {'svc__C': param_range, \n",
    "               'svc__gamma': param_range, \n",
    "               'svc__kernel': ['rbf']}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d32005-219a-40c3-99e5-469e9538e418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f5ff7-0d72-4666-932a-9fbdc875b6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79b0498c-94cf-4ee5-b347-66ff4a8f628f",
   "metadata": {},
   "source": [
    "---\n",
    "3. \n",
    "- Initialize `GridSearchCV`\n",
    "- Fit to data\n",
    "- Print accuracy and optimized hyperparameters of best model\n",
    "\n",
    "```\n",
    "gs = GridSearchCV(estimator=pipe_svc,      # initialise gs object\n",
    "                  param_grid=param_grid2, \n",
    "                  scoring='accuracy',      # this can also be 'precision', etc.\n",
    "                  refit=True,              # this will refit the best estimator to the whole dataset automatically\n",
    "                  cv=10,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "\n",
    "gs = gs.fit(X_train, y_train)            # fit gs\n",
    "\n",
    "print(gs.best_score_)\n",
    "\n",
    "print(gs.best_params_)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03366e5-42d6-49a2-a388-dff4fb86c140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38f47c65-31ec-413a-9dc0-c1b8a3d26ce9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "4. Export best model \n",
    "- Compute accuracy on test data\n",
    "- Print predicted labels for test data \n",
    "\n",
    "```\n",
    "best_classifier = gs.best_estimator_                # copy best estimator\n",
    "\n",
    "print(best_classifier)\n",
    "print(f'Test accuracy: {best_classifier.score(X_test, y_test):.3f}')\n",
    "print(best_classifier.predict(X_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b28e821-fcee-4030-b03a-70b9020ee7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5db0d3-22d6-4ff2-ad90-3f741fe9b713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41cd31-11ae-4b72-8417-bf75da89654c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1f637b0-f9bc-4dd0-a0ed-e70ae8a215e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Debugging Algorithms with Learning and Validation Curves {-}\n",
    "\n",
    "Two diagnostic tools to help improve the performance of a learning algorithm\n",
    "- Learning Curves - performance as a function of sample size\n",
    "- Validation Curves - performance as a function of model hyperparameters\n",
    "\n",
    "## Diagnosing Bias and Variance Problems with Learning Curves {-}\n",
    "\n",
    "If a model is too complex (it has too many parameters) it is likely to overfit\n",
    "- By **increasing the sample size we can reduce the extent of overfitting**\n",
    "- *When we add more data false patterns tend to lessen or disappear*. For example consider flipping a coin\n",
    "    - 3 times with the following outcomes: H, H, H\n",
    "    - 10 times with the following outcomes: H, H, H, T, H, T, T, H, H, T\n",
    "- In practice it may be very costly or impossible to collect more data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86840b97-64a5-426a-94fe-a48a1a2d12f6",
   "metadata": {},
   "source": [
    "### Learning Curves \n",
    "\n",
    "- Graphical representations that show how the performance of a machine learning model changes as the amount of training data changes\n",
    "- Diagnostic tool used to understand the behavior of a model during the training process and to identify issues such a overfitting or underfitting\n",
    "- Plot model **training** and **validation** accuracy as functions of training dataset size\n",
    "    - Detect whether the model suffers from high variance (overfitting) or high bias (underfitting)\n",
    "    - Find out if collecting more data can reduce these problems\n",
    "   \n",
    "<img src=\"images/06_04.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b19e41-733d-4cac-bc49-d5b3377eb25c",
   "metadata": {},
   "source": [
    "Model in upper-left corner\n",
    "- Low training and cross-validation accuracy -> model underfits -> high bias\n",
    "- Possible solution - increase the number of parameters\n",
    "    - Collect or construct additional features\n",
    "    - Decrease the degree of regularization\n",
    "    \n",
    "  \n",
    "Model in upper-right corner\n",
    "- Large variability between training and cross-validation accuracy -> model overfits -> high variance\n",
    "    - Collect more training data\n",
    "    - Reduce the complexity of the model (number of parameters/features) - e.g. via Feature Extraction/Selection\n",
    "    - Increase the degree of regularization\n",
    "    \n",
    "In scikit-learn use `from sklearn.model_selection import learning_curve`\n",
    "- By default uses **stratified k-fold cross-validation**\n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html)\n",
    "- Lets implement this on our training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502aec05-5972-46d2-bd0b-f530b25a368c",
   "metadata": {},
   "source": [
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "pipe_lr = make_pipeline(StandardScaler(), \n",
    "                        LogisticRegression(penalty='l2', \n",
    "                                           random_state=1, \n",
    "                                           solver='lbfgs', \n",
    "                                           max_iter=10000)) # increase number of iterations for optimization algorithm in order to avoid potential problems with small datasets\n",
    "\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr, \n",
    "                               X=X_train,\n",
    "                               y=y_train,\n",
    "                               train_sizes=np.linspace(0.1, 1.0, 11), # train_sizes are set up here, split interval [0,1] into 11 evenly spaced pieces\n",
    "                               cv=10,\n",
    "                               n_jobs=1)\n",
    "\n",
    "\n",
    "# # print('train sizes', train_sizes)\n",
    "# # print('train scores', train_scores.shape, train_scores)\n",
    "# print('test scores', test_scores.shape, test_scores)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,      # plot mean +- one standard deviation          \n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training examples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.8, 1.03])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/06_05.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9906fffc-e764-4d26-957c-f7249b985385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db402434-6772-4b5f-baae-f31b7164480e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Conclusion**\n",
    "- The model performs quite well for sample sizes of about 250 observations or greater"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d76bb5-4331-4b5d-becc-7cb0b6d5c278",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "## Addressing Over- and Under-Fitting with Validation Curves {-}\n",
    "\n",
    "- Validation curves are used to assess how the performance of a machine learning model changes as the hyperparameter values are varied.\n",
    "- Unlike learning curves, which focus on the amount of training data, validation curves are primarily concerned with understanding the relationship between a model's hyperparameters and its performance.\n",
    "    - This can help in tuning hyperparameters to improve the model's ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2bd8c1-5732-4f62-9921-d36e09614bd4",
   "metadata": {},
   "source": [
    "- **Validation Curves** - performance as a function of model hyperparameters\n",
    "    - `from sklearn.model_selection import validation_curve`\n",
    "    - `validation_curve` employs **stratified k-fold cross-validation** to estimate the performance of the classifier\n",
    "- Need to specify the parameter we wish to evaluate    \n",
    "    - Also need to specify a parameter range that the evaluated parameter will take\n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html)\n",
    "    \n",
    "Example\n",
    "- Employ a validation curve on LogisticRegression \n",
    "- Vary $C$ - the inverse of the regularization parameter - over the range `[0.001, 0.01, 0.1, 1.0, 10.0, 100.0]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a447ce3-26f9-4283-8183-cef077f7bfde",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "                estimator=pipe_lr, \n",
    "                X=X_train, \n",
    "                y=y_train, \n",
    "                param_name='logisticregression__C', \n",
    "                param_range=param_range,\n",
    "                cv=10)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_mean, \n",
    "         color='blue', marker='o', \n",
    "         markersize=5, label='Training accuracy')\n",
    "\n",
    "plt.fill_between(param_range, train_mean + train_std,\n",
    "                 train_mean - train_std, alpha=0.15,\n",
    "                 color='blue')\n",
    "\n",
    "plt.plot(param_range, test_mean, \n",
    "         color='green', linestyle='--', \n",
    "         marker='s', markersize=5, \n",
    "         label='Validation accuracy')\n",
    "\n",
    "plt.fill_between(param_range, \n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std, \n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/06_06.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef074bb7-ab99-42b5-9327-f000b744c53c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dd8328c-eadf-4b3a-8f0e-4eb7b9fd4e46",
   "metadata": {},
   "source": [
    "- Small $C$ -> regularization is too strong -> both training and validation accuracy is relatively low -> underfitting\n",
    "- $C\\in\\{0.1, 1\\}$ seems to be best in terms of both training and validation accuracy\n",
    "- $C > 1$ -> regularization is too weak -> too many parameters -> training accuracy is good but validation accuracy decreases -> overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7703713f-2a12-4ce3-bf2a-0d39105e81a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Looking at Different Performance Evaluation Metrics {-}\n",
    "\n",
    "- So far we have relied on `prediction accuracy` as a measure of performance\n",
    "- Now we'll look at several other measures of performance\n",
    "\n",
    "### The Confusion Matrix {-}\n",
    "This should be familiar from basic statistics \n",
    "\n",
    "- The null hypothesis $H_0$ is usually something that we assume prior to conducting a statistical test, e.g. innocent until proven guilty, healthy until tested positive for a desease, etc\n",
    "    - Negative Result -> Accept $H_0$: e.g. negative COVID test -> accept $H_0$: no covid\n",
    "    - Positive Result -> Reject $H_0$: e.g. positive COVID test -> reject $H_0$: no covid\n",
    "- We can extend this as follows\n",
    "    - False Positive (FP) -> False = Incorrect, Positive = Reject $H_0$ -> Reject $H_0$ when True = Type I Error, e.g. diagnosed with COVID when in reality not sick\n",
    "    - False Negative (FN) -> False = Incorrect, Negative = Accept $H_0$ -> Accept $H_0$ when False = Type II Error, e.g. not diagnosed with COVID when in reality sick\n",
    "\n",
    "\n",
    "- Use `from sklearn.metrics import confusion_matrix`\n",
    "    - Need to specify true classes (target) and predicted classes (prediction of target)\n",
    "\n",
    "<img src=\"images/06_08.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "<span style='background:orange'>  **Example** \n",
    "   \n",
    "- Retrain `pipe_svc` on train dataset\n",
    "- Produce predictions using `X_test` dataset\n",
    "- Compute and plot confusion matrix\n",
    "- Interpret the results\n",
    "    \n",
    "<hr style=\"width:35%;margin-left:0;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a75408-89a3-422a-8e0e-d35cf5a64d5d",
   "metadata": {},
   "source": [
    "---\n",
    "1. \n",
    "- Retrain `pipe_svc` on train dataset\n",
    "- Produce predictions using `X_test` dataset\n",
    "\n",
    "```\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe_svc.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9a0e5-39e1-4fbc-8511-891ad9da9e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4670e310-b3b6-4a0c-a181-53fe8f718c39",
   "metadata": {},
   "source": [
    "---\n",
    "2. \n",
    "\n",
    "- Compute and plot confusion matrix\n",
    "- Interpret the results\n",
    "\n",
    "```\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "# ---------- Plotting \n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "        \n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/06_09.png', dpi=300)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daaddf9-12df-4f14-9946-d580456b7022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e1c9399-54bc-48d0-ab75-e5bca51c936a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1359947-4cda-4dba-a9f9-6cdba6e57083",
   "metadata": {},
   "source": [
    "<span style='background:orange'>  \n",
    "Plotting Confusion Matrix v.2\n",
    "</span>\n",
    "\n",
    "<br>\n",
    "    \n",
    "    \n",
    "```\n",
    "labels = [1,0]\n",
    "confmat2 = confusion_matrix(y_true=y_test, y_pred=y_pred, labels = labels)\n",
    "\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in confmat2.flatten()]\n",
    "# group_percentages = [\"{0:.2%}\".format(value) for value in confmat2.flatten()/np.sum(confmat2)]\n",
    "group_names = ['True Pos', 'False Neg','False Pos', 'True Neg']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "labels3 = [f\"{v2}\\n{v3}\" for v2, v3 in zip(group_names, group_counts)]\n",
    "# labels3 = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "\n",
    "\n",
    "\n",
    "labels3 = np.asarray(labels3).reshape(2,2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(confmat2, annot=labels3, fmt='',xticklabels=labels, yticklabels=labels, cmap='Blues', cbar=False)\n",
    "\n",
    "ax.set_xlabel('Predicted label')    \n",
    "ax.xaxis.set_label_position('top') \n",
    "ax.set_ylabel('True label')\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=10, labelbottom = False, bottom=False, top = False, labeltop=True)\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d5c5d-59bc-4ae5-ac06-d6430acaf23f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "823078b1-43ce-4658-8bce-1a5a40858faf",
   "metadata": {},
   "source": [
    "In our dataset there are two classes: \n",
    "- 0 = Benign tumor and \n",
    "- 1 = Malignant tumor  \n",
    "\n",
    "$H_0$: Benign tumor (class 0)  \n",
    "$H_1$: Malignant tumor (class 1)\n",
    "\n",
    "- Correctly Classified: 71 examples of class 0 and 40 examples of class 1\n",
    "- Misclassified: \n",
    "    - False Positive (false rejection of $H_0$) 1 example\n",
    "    - False Negative (false acceptance of $H_0$) 2 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d43a44-d642-42dc-9c63-d046fa583169",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "## Optimizing the Precision and Recall of a Classification Model {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca6123-746a-49d8-9027-f5271022dc6b",
   "metadata": {},
   "source": [
    "- Prediction Error: $ERR=\\frac{\\text{Misclassified}}{N}=\\frac{FP+FN}{FP+FN+TP+TN}$\n",
    "  \n",
    "- Accuracy: $ACC = \\frac{\\text{Correctly Classified}}{N}=\\frac{TP+TN}{FP+FN+TP+TN} = 1 - ERR$\n",
    "\n",
    "We select the best forecasting model based on some performance measure, such as accuracy above\n",
    "- However, in some circumstances we may want to choose a model according to some other performance measure    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93e66b-d690-48fb-b1c6-c4b3b3865516",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/06_08.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "  \n",
    "<hr style=\"width:20%;margin-left:0;\">   \n",
    "\n",
    "\n",
    "**True Positive and False Positive Rates**\n",
    "\n",
    "\n",
    "- True Positive Rate = $TPR = \\frac{TP}{P}=\\frac{TP}{TP+FN}$ \n",
    "    - Probability of correctly detecting effect\n",
    "    - TPR is known as **power** in statistical testing           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbcd207-a7d7-4ca3-8fbf-55bbfd65ca04",
   "metadata": {},
   "source": [
    "- False Positive Rate = $FPR = \\frac{FP}{N}=\\frac{FP}{FP+TN}$\n",
    "    - Probability of incorrectly detecting effect\n",
    "    - Known as the Probability of **Type I Error** in statistical testing\n",
    "\n",
    "\n",
    "\n",
    "<hr style=\"width:20%;margin-left:0;\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7af774-4b77-4b75-8279-8256c5130594",
   "metadata": {},
   "source": [
    "### Alternative Measures of Performance: **Recall (REC)** and **Precision (PRE)**\n",
    "\n",
    "- $REC=TPR=\\frac{TP}{TP+FN}=0.95$       \n",
    "- In our application we are more concerned with TPR - detecting malignant tumors\n",
    "- Want TPR as high as possible -> which will make FN small\n",
    "    - Optimizing (choosing) a model based on REC will minimize the chance of not detecting a malignant tumor\n",
    "    - Optimizing TPR therefore may be more important than choosing the model with the highest accuracy in this case\n",
    "    - This will however result in some predictions of malignant tumors in healthy patients (FP)\n",
    "- A business application where recall is imporant is *Fraud Detection*\n",
    "    - In fraud detection the goal is to identify as many faudulent transactions as possible\n",
    "    -  A high recall ensures that most of the fraudulent activities are flagged for further review\n",
    "        - This minimises financial loss (e.g. consider fraudulent credit card transactions)\n",
    "        - This will cccasionally flag some legitimate transactions as fraudulent (false positives and you may be given a call by your bank to verify a transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526681a9-843f-4d16-81c6-6095e09d404f",
   "metadata": {},
   "source": [
    "- $PRE=\\frac{TP}{TP+FP}=0.98$ \n",
    "    - PRE = 100% will result in having no False Positives\n",
    "        - This is not good for medical applications, e.g. likely to miss malignant tumors more frequently (FN)\n",
    "        - But good for email spam filtering -> don't want to classify real emails as spam \n",
    "        - FP = a regular email classified as spam \n",
    "        - Positive = spam here\n",
    "- A business application where precision is more important than recall is in *Product Recommendations*\n",
    "    -  Precision ensures that the recommended items are highly relevant to the user's interests and preferences\n",
    "        -  Users are more likely to engage with a few highly relevant recommendations than with a large number of irrelevant ones\n",
    "        -  This can increase customer satisfaction and retention, as customers feel the platform understands their needs\n",
    "    -  Too many irrelevant recommendations (false positives) can overwhelm and frustrate the user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e479f-f24f-4125-a9c9-3f8f128d1e74",
   "metadata": {},
   "source": [
    "- F1 score is a combination of PRE and REC $F1 = 2\\frac{PRE \\times REC}{PRE + REC}$\n",
    "    - Especially useful when we need to balance precision and recall, which is often the case in datasets where both false positives and false negatives carry a significant cost\n",
    "\n",
    "<hr style=\"width:20%;margin-left:0;\"> \n",
    "\n",
    "- In scikit-learn these scoring matrics are already implemented\n",
    "    - `from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score`\n",
    "- If we want to use a scoring metric other than `accuracy` in `GridSearchCV` we can change the `scoring` parameter\n",
    "    - [https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_true=y_test, y_pred=y_pred):.3f}')\n",
    "print(f'Precision: {precision_score(y_true=y_test, y_pred=y_pred):.3f}')\n",
    "print(f'Recall: {recall_score(y_true=y_test, y_pred=y_pred):.3f}')\n",
    "print(f'F1: {f1_score(y_true=y_test, y_pred=y_pred):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0757256-b7d8-44be-94ef-4e4105a63c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bceb8c0-5915-47b5-a65a-ee474f26bc72",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "## **Appendix 1:** Dealing with Class Imbalance {-}\n",
    "\n",
    "- Class Imbalance occures quite often in real world\n",
    "    - Examples from one class or multiple classes are over-represented in a dataset\n",
    "    - Lets say class1 = 90 examples, class2 = 10 examples -> if we just predict class1 for all examples -> 90% accuracy\n",
    "        - If an ML model returns 90% accuracy -> it hasn't really learned anything useful from the features\n",
    "    - In this case accuracy is not the most useful measure of performance\n",
    "        - If want to identify the majority of patients with malignant cancer to recommend an additional screening -> use REC (recall)\n",
    "        - If screening for spam email -> use PRE (precision) (minimise the prob of classifying real emails as spam)\n",
    "    - E.g. fraud detection, loan defaults, etc\n",
    "    \n",
    "\n",
    "**Potential solutions**\n",
    "- Assign a larger penalty to wrong predictions on the minority class\n",
    "    - Set `class_weight='balanced'` parameter\n",
    "    - Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n",
    "- Upsampling of the minority class\n",
    "    - `resample` in scikit-learn\n",
    "    - Repeatedly draw new samples of the minority class from the dataset with replacement\n",
    "    - Can also Downsample the majority class\n",
    "- Generate new (synthetic) examples of the minority class\n",
    "    - Too technical \n",
    "    - Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "\n",
    "- Lets create an imbalanced dataset -> 357 begnin tumors (class 0) and 40 malignant (class 1)\n",
    "\n",
    "```\n",
    "X_imb = np.vstack((X[y == 0], X[y == 1][:40]))\n",
    "y_imb = np.hstack((y[y == 0], y[y == 1][:40]))\n",
    "\n",
    "print(y_imb.shape)\n",
    "print(np.bincount(y_imb))\n",
    "print(np.bincount(y_imb)/len(y_imb))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe56b4c-ff06-45da-9a0c-4dda7d57495f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e26167d-1c0d-4fd0-a056-d299ee772e65",
   "metadata": {},
   "source": [
    "\n",
    "- So if we predict y = 0 for all samples -> about 90% accuracy\n",
    "\n",
    "```\n",
    "from sklearn.utils import resample\n",
    "\n",
    "print('Number of class 1 examples before:', X_imb[y_imb == 1].shape[0])\n",
    "\n",
    "X_upsampled, y_upsampled = resample(X_imb[y_imb == 1],\n",
    "                                    y_imb[y_imb == 1],\n",
    "                                    replace=True,\n",
    "                                    n_samples=X_imb[y_imb == 0].shape[0],\n",
    "                                    random_state=123)\n",
    "\n",
    "print('Number of class 1 examples after:', X_upsampled.shape[0])\n",
    "pd.DataFrame(X_upsampled)\n",
    "```\n",
    "\n",
    "\n",
    "- Then we can check proportions as follows\n",
    "\n",
    "```\n",
    "X_bal = np.vstack((X[y == 0], X_upsampled))\n",
    "y_bal = np.hstack((y[y == 0], y_upsampled))\n",
    "\n",
    "print(y_bal.shape)\n",
    "print(np.bincount(y_bal))\n",
    "print(np.bincount(y_bal)/len(y_bal))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f2e01-3ac6-42ed-945f-e16faf7e662e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1703df-3d23-4cf1-a3a4-77a67ddb38fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a2f42dd-fffb-4691-a5e6-da5bcbec553a",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "### **Appendix 2**: Receiver Operating Characteristic (ROC) {-}\n",
    "\n",
    "Receiver Operating Characteristic (ROC) graphs are useful to select models for classification based on their performance with respect to FPR and TPR  \n",
    "    - [https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)  \n",
    "    - Plot TPR (power) vs FPR (Type I Error)\n",
    "    - Shift the decision threshold of the classifier  \n",
    "    - Diagonal interpreted as random guessing   \n",
    "    - Classification models below the diagonal are worse than random guessing  \n",
    "    - A perfect classifier is in the top-lect corner with a TPR=1 and FPR=0  \n",
    "    - ROC area under the curve (ROC AUC) can be computed to characterize performance\n",
    "        - ROC AUC = 1 -> perfect classifier\n",
    "        - ROC AUC = 0.5 -> random guessing\n",
    "    \n",
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "<span style='background:orange'>  **Example** \n",
    "   \n",
    "- Implement ROC on Breast Cancer Data\n",
    "- Vary X from all features to only 2 features, see how it changes FPR and TPR\n",
    "    \n",
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from numpy import interp\n",
    "\n",
    "pipe_lr = make_pipeline(StandardScaler(),\n",
    "                        PCA(n_components=2),\n",
    "                        LogisticRegression(penalty='l2', \n",
    "                                           random_state=1,\n",
    "                                           solver='lbfgs',\n",
    "                                           C=100.0))\n",
    "\n",
    "# X_train2 = X_train[:, [4, 14]]\n",
    "X_train2 = X_train.copy()\n",
    "\n",
    "\n",
    "cv = list(StratifiedKFold(n_splits=3).split(X_train, y_train))\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "\n",
    "mean_tpr = 0.0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "all_tpr = []\n",
    "\n",
    "for i, (train, test) in enumerate(cv):\n",
    "    probas = pipe_lr.fit(X_train2[train],\n",
    "                         y_train[train]).predict_proba(X_train2[test])\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_train[test],\n",
    "                                     probas[:, 1],\n",
    "                                     pos_label=1)\n",
    "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr,\n",
    "             tpr,\n",
    "             label='ROC fold %d (area = %0.2f)'\n",
    "                   % (i+1, roc_auc))\n",
    "\n",
    "plt.plot([0, 1],\n",
    "         [0, 1],\n",
    "         linestyle='--',\n",
    "         color=(0.6, 0.6, 0.6),\n",
    "         label='Random guessing')\n",
    "\n",
    "mean_tpr /= len(cv)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--',\n",
    "         label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "plt.plot([0, 0, 1],\n",
    "         [0, 1, 1],\n",
    "         linestyle=':',\n",
    "         color='black',\n",
    "         label='Perfect performance')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/06_10.png', dpi=300)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28066371-e5f3-4526-925e-ff5e6c499717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197fb45-dcfc-440f-80dc-9b5068e2110b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f648a6d6-cfdd-4507-9628-e4a70f5442db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
