{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "997c89ec-1d30-4eb6-9bbc-05494c0d116c",
   "metadata": {},
   "source": [
    "# Introduction to Time Series Forecasting with `sktime`\n",
    "\n",
    "\n",
    "### Unit Convenor & Lecturer {-}\n",
    "[George Milunovich](https://www.georgemilunovich.com)  \n",
    "[george.milunovich@mq.edu.au](mailto:george.milunovich@mq.edu.au)\n",
    "\n",
    "### References {-}\n",
    "\n",
    "\n",
    "1. Various open-source material as provided in the URL's throughout this notebook\n",
    "\n",
    "### Overview {-}\n",
    "\n",
    "- Introduction to Time Series data\n",
    "- How is Time Series forecasting different\n",
    "- Univariate vs Multivariate Time Series\n",
    "- Autocorrelations and Partial Autocorrelations\n",
    "- White noise, MA, AR and ARMA models\n",
    "- Time Series forecasting libraries in Python\n",
    "- Introduction to `sktime` library\n",
    "- Some Basic Univariate Time Series Models\n",
    "- Predicting Airline Passenger Dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13530280-1208-4f18-8774-a7eed18e3cf2",
   "metadata": {},
   "source": [
    "## Time Series Data\n",
    "\n",
    "So far we have been studying prediction problems using mainly cross-sectional datasets\n",
    "- A **cross-sectional dataset** refers to data collected at a single point in time or over a very short period of time\n",
    "    - Provides a \"snapshot\" of various variables at that particular moment\n",
    "- Examples:\n",
    "    - House prices sold in a suburb on a given day\n",
    "    - Used cars listed for sale listed on an online marketplace\n",
    "    - Etc.\n",
    "\n",
    "\n",
    "**What is a Time Series?**\n",
    "\n",
    "- A time series is a sequence of data points recorded or measured at successive points in time, often at regular intervals\n",
    "- This type of data is particularly useful in statistical, economic, and business contexts where monitoring changes over time is crucial\n",
    "\n",
    "**Why Study Time Series**\n",
    "- Predict future values from historical data\n",
    "- Understand and quantify how underlying patterns evolve over time\n",
    "- Make informed strategic decisions to improve business operations or respond to changes in the environment  \n",
    "\n",
    "**Time Series Examples**\n",
    "- Economics: Forecasting stock prices, economic indicators, and market trends\n",
    "- Healthcare: Predicting disease outbreaks, patient admissions, and resource requirements\n",
    "- Retail: Estimating future product demand to optimise inventory management and reduce costs\n",
    "- Energy: Forecasting demand and supply to manage resources efficiently and plan for future energy needs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0c089-bc4f-4196-b93c-188dad965880",
   "metadata": {},
   "source": [
    "## Distinguishing Time Series Forecasting from Cross-Sectional Forecasting\n",
    "\n",
    "- **Cross-Sectional Forecasting**\n",
    "    - Data collected on multiple variables at a single point in time or over a very short period\n",
    "    - The data points are *not in a sequence* but are instead independent observations across various subjects or entities (e.g. Airbnb properties listed for rent within a suburb)\n",
    "    - Forecasting relies on the relationship between the target and explanatory variables measured **at the same time period**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c7579-67ff-4e8d-8708-5f640d12c4df",
   "metadata": {},
   "source": [
    "\n",
    "- **Time Series Forecasting**\n",
    "    - Focuses on data that is collected over time and is inherently sequential   \n",
    "    - Primary interest is in **predicting future values based on past and present data points**   \n",
    "    - Forecasting relies on the temporal (time) dependence/patterns between observations  \n",
    "    - The past and current values are used to predict future values, considering trends, seasonality, cycles, and other time-related patterns\n",
    "    - Past values of the target variable as well as past values of the explanatory variables can be utilized to build predictive models\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/week11_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>  \n",
    "</div>\n",
    "\n",
    "<hr style=\"width:30%;margin-left:0\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57afc76-954f-46d0-90f5-cd629cac4027",
   "metadata": {},
   "source": [
    "**Univariate vs Multivariate Time Series**\n",
    "\n",
    "- **Univariate Time Series**\n",
    "    - When we use a single time-dependent variable\n",
    "    - Analyse and forecast data based on past values of that single variable alone\n",
    "    - Suitable for situations where the interest is in predicting future values of a single series and\n",
    "        - The series itself contains all the necessary information for making such predictions, or\n",
    "        - There are no other relevant variables available\n",
    "    - $\\hat{y}_{t+1} = f(y_t, y_{t-1}, y_{t-2}, \\dots)$ where $f$ is some function of past and present values, and $t$ denotes the time period \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be6150-b6da-4250-a662-98f28d082be8",
   "metadata": {},
   "source": [
    "- **Multivariate Time Series**\n",
    "    - When we analyse and predict the behavior of a time series based on multiple variables\n",
    "    - The future value of a variable is predicted not only from its own past values but also from the past values of other related variables\n",
    "    - Used when the prediction performance can potentially improve by incorporating the information from additional variables  \n",
    "    - For instance, predicting economic growth might benefit from considering variables like\n",
    "        - Inflation rate\n",
    "        - Unemployment rate,\n",
    "        - Industrial production, etc.\n",
    "    - $\\hat{y}_{t+1} = f(y_t, y_{t-1}, y_{t-2}, \\dots, x_t, x_{t-1}, x_{t-2}, \\dots, z_t, z_{t-1}, z_{t-2}, \\dots)$  where $f$ is some function of past and present values \n",
    "     \n",
    "\n",
    "We will only consider univariate models in this Introduction to Time Series\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/week11_2.png\" alt=\"Drawing\" style=\"width: 400px;\"/>  \n",
    "</div>\n",
    "\n",
    "<hr style=\"width:50%;margin-left:auto\">\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e63839-d286-4d24-8b30-efd9d20e7c76",
   "metadata": {},
   "source": [
    "## Some Basic Concepts in Time Series Data Analysis\n",
    "\n",
    "- In time series forecasting, the future value of the target variable depends on its past and present data points\n",
    "    - Linear dependence, e.g. Autoregressive - AR(1) model $\\hat{y}_{t} = 1 + 0.7y_{t-1}$\n",
    "    - Nonlinear dependence, e.g. Logistic Map model $\\hat{y}_{t} = 0.5y_t(1 - y_{t-1})$\n",
    "- How do we measure such dependence?\n",
    "\n",
    "<span style=\"background-color: yellow\">**Concept 1: Autocorrelations** </span>\n",
    "- Autocorrelation is the correlation between a random variable and its own past values\n",
    "- If a time series is correlated with its past values, its values are not independent of each other over time and there is a pattern/trend in the data  \n",
    "- This information is vital for time series forecasting because it can be used to improve the accuracy of the models\n",
    "- Autocorrelation between $y_t$ and $y_{t-k}$ is defined as\n",
    "    - $\\tau_k = \\text{corr}(y_t, y_{t-k}) = \\frac{\\text{cov}(y_t, y_{t-k})}{\\sqrt{\\text{var}(y_t)\\text{var}(y_{t-k})}}=\\frac{\\text{cov}(y_t, y_{t-k})}{\\text{var}(y_t))}$ \n",
    "\n",
    "*Note*\n",
    "- Autocorrelations lie in the $[-1, 1]$ interval, i.e. $\\tau_k\\in[-1,1]$\n",
    "- Autocorrelations measure linear dependence (nonlinear dependence may be ignored)\n",
    "- Sample autocorrelations are random variables - they depend on the sample data from which they are computed\n",
    "    - We need to consider a statistical test / confidence interval around sample values to draw conclusions about true population parameters\n",
    " \n",
    "<span style=\"background-color: yellow\">**Concept 2: Autocorrelation Function (ACF)** </span>\n",
    "- Compute and plot autocorrelations for diffent lags (time intervals), e.g. compute all $\\tau_k$ for $k=0,1,2,3,\\dots$\n",
    "- ACF shows the amount of memory in a time series\n",
    "- Note that for lag 0 we have $\\tau_0 = \\text{corr}(y_t, y_{t-0})=\\text{corr}(y_t, y_{t})=1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c5d432-5e59-4fd1-b122-9065edd7b7cc",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"background-color: yellow\">**Concept 3: White Noise** </span>\n",
    "- A white noise process $x_t$ has the following characteristics:\n",
    "1. Zero Mean: The mean of the white noise process is zero $E[x_t] = 0$\n",
    "2. Constant Variance: The variance of the weak white noise process is the same for all periods $t$, i.e. $\\text{Var}(x_t) =\\text{Var}(x_{t-1}) = \\dots = \\sigma^2$\n",
    "3. Uncorrelated over time: The autocorrelation function (ACF) is zero for any non-zero lag, meaning that there is no linear dependence between white noise values at different times\n",
    "    - $\\text{Corr}(x_t, x_{t-s})=\\tau_s = 0 \\quad \\text{for any} \\; s \\neq 0$\n",
    "- Note: white noise is unpredictable using its past values since the future values are uncorrelated with past values  \n",
    "   \n",
    "\n",
    "<span style=\"background-color: yellow\">**Concept 4: Autoregressive Model of Order $p$ - AR$(p)$** </span>\n",
    "- White noise processes are used to create more complicated models\n",
    "- An AR$(p)$ model is a process where the current value $y_t$ depends on its own $p$ lags (past values)  \n",
    "- $y_t = c + \\phi_1 y_{t-1}+ \\phi_2 y_{t-2} + \\dots +  \\phi_p y_{t-p} + \\epsilon_t$ for some value $p$ \n",
    "    - Here $\\epsilon_t$ is assumed to be a white noise process\n",
    "- Some potential AR models (examples):\n",
    "    - AR(1): $y_{t} = 1 + 0.7y_{t-1} + \\epsilon_t$\n",
    "    - AR(2): $y_t = 0.5y_{t-1} - 0.3y_{t-2} + \\epsilon_t$\n",
    "    - AR(3): $y_t = 0.4 y_{t-1} - 0.2 y_{t-2} + 0.1 y_{t-3} + \\epsilon_t$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde2cfd-40ff-40b5-937a-cdfff24d2fe2",
   "metadata": {},
   "source": [
    "## Example: ACFs of White Noise and AR(1)\n",
    "\n",
    "### Generate Synthetic Data\n",
    "\n",
    "- Generate 500 synthetic observations of\n",
    "    1. While noise - $x_t$\n",
    "    2. AR(1) process $y_{t} = 1 + 0.7y_t + \\epsilon_t$\n",
    "- Plot their ACFs with 20 lags\n",
    "- Determine which variable exhibits time dependence (memory) suitable for making predictions based on their ACFs\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Generating white noise\n",
    "np.random.seed(42)  # for reproducibility\n",
    "x = np.random.normal(size=500) # white noise\n",
    "\n",
    "\n",
    "# Generating AR(1) process\n",
    "ar = np.array([1, -0.7])  # AR(1) with phi=0.7\n",
    "ma = np.array([1])\n",
    "AR1_process = ArmaProcess(ar, ma) \n",
    "ar1_data = AR1_process.generate_sample(nsample=500)\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "# Plot the white noise in the first subplot\n",
    "axs[0].plot(x)\n",
    "axs[0].set_title('White Noise')\n",
    "axs[0].set_ylabel('Value')\n",
    "\n",
    "# Plot the AR(1) process in the second subplot\n",
    "axs[1].plot(ar1_data, color='orange')\n",
    "axs[1].set_title('AR(1) Process')\n",
    "axs[1].set_ylabel('Value')\n",
    "axs[1].set_xlabel('Time')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22938094-3dbd-4d37-8aff-98eea62b6b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2af5bb-6b06-40aa-b376-046d03b6aa11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dbd065-c098-4c8f-acef-345ac2fa5c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c9a60c-2879-410c-acb0-1ebbfcb31a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d39cd-78c5-403a-a9c9-5a196483095c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "922fad4e-dfa8-4036-aadc-f73618e1e09c",
   "metadata": {},
   "source": [
    "### Plot ACFs\n",
    "\n",
    "```\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "plot_acf(x, ax=axes[0], lags=20, title='ACF of White Noise $(x_t)$')\n",
    "axes[0].set_ylim(-1.1, 1.1)  # Set y-axis limits\n",
    "axes[0].set_xticks(np.arange(0, 21, 1))  # Set x-ticks at increments of 1\n",
    "axes[0].grid(True, color='#D3D3D3')  \n",
    "\n",
    "plot_acf(ar1_data, ax=axes[1], lags=20, title='ACF of AR(1) Model $(y_t)$')\n",
    "axes[1].set_ylim(-1.1, 1.1)  # Set y-axis limits\n",
    "axes[1].set_xticks(np.arange(0, 21, 1))  # Set x-ticks at increments of 1\n",
    "axes[1].grid(True, color='#D3D3D3')  \n",
    "\n",
    "axes[0].set_ylabel(r'$\\tau_k$', rotation = 0)\n",
    "axes[1].set_ylabel(r'$\\tau_k$', rotation = 0)\n",
    "axes[1].set_xlabel('Lag $(k)$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5858f2-6116-4059-88a1-41bf1580a159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2ff6b5e-8d13-4a88-bdad-978e2e4e248a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Interpreting the ACF\n",
    "- The 95% confidence intervals are indicated by the blue shaded area\n",
    "- **White Noise**: The ACF plot for the white noise series $x_t$​ shows no signs of time series predictability\n",
    "    - No statistically significant (outside of the CI) ACFs for lags 1, 2, ..., 20\n",
    "- **AR(1)**: The ACF for the given AR(1) shows signs of about 7 significant autocorrelations indicating time series memory\n",
    "    - Why don't we have only one significant autocorrelation, i.e. $\\text{Corr}(x_t, x_{t-k})\\ne0$ for $k = 1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762cf3e4-d62b-46fb-8c1e-b69a7f4ef27b",
   "metadata": {},
   "source": [
    "### Repeated Substitution\n",
    "\n",
    "Consider the AR(1) process for 3 time periods, $t, t-1$ and $t-2$\n",
    "\n",
    " \\begin{align*}\n",
    "y_t &= c + \\phi y_{t-1} + \\epsilon_t \\\\\n",
    "y_{t-1} &= c + \\phi y_{t-2} + \\epsilon_{t-1} \\\\\n",
    "y_{t-2} &= c + \\phi y_{t-3} + \\epsilon_{t-2}\n",
    "\\end{align*}\n",
    "\n",
    "- From the first equation we see that $y_t$ depends on $y_{t-1}$\n",
    "    - This explains their correlation $\\text{Corr}(x_t, x_{t-1})$\n",
    "- If we substitute the equation for $y_{t-1}$ into the equation for $y_t$ we get\n",
    "    - $y_t = c + \\phi (c + \\phi y_{t-2} + \\epsilon_{t-1}) + \\epsilon_t= c + \\phi c + \\phi^2 y_{t-2} + \\phi \\epsilon_{t-1} + \\epsilon_t$\n",
    "    - Now we see that $y_t$ equivalently depends on $y_{t-2}$ but with the coefficient $\\phi^2$\n",
    "    - This explains the non-zero correlation $\\text{Corr}(x_t, x_{t-2})$\n",
    "- Such repeated substitutions show that $y_t$ depends on all its previous values but at decreasing magnitudes for $|\\phi|<1$\n",
    "    - This explains non-zero $\\text{Corr}(x_t, x_{t-k})$ for all $k=0, 1, 2,3, \\dots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fde15c5-ea51-4861-b10b-d9d273bbb14c",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"background-color: yellow\">**Concept 5: Partial Autocorrelation Function - PACF**</span>\n",
    "- Measure of the relationship between a time series variable and its own lagged values, after accounting for the effects of the intervening lags \n",
    "    - Correlation between $y_t$​ and $y_{t−k}$ after controlling for the effects of $y_{t−1},y_{t−2},\\dots,y_{t−(k−1)}$ \n",
    "    - From the equation for AR(1) we see that $y_t$ depends only on $y_{t-1}$ \n",
    "    - From the first equation for AR(2) the process $y_t$ does NOT depend on $y_{t−2}$ after we control for $y_{t-1}$\n",
    "    - Etc...\n",
    "\n",
    "### Using ACF and PACF to indentify an AR(p) model\n",
    "- We can use ACF and PACF to identify an AR model and determine it order $p$ \n",
    "- For an AR(p) model:\n",
    "    - PACF cuts off sharly to zero after $p$ lags\n",
    "        - E.g. if there are two statistically significant PACFs then its an AR(2)\n",
    "    - ACF: Gradual decay towards zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a626249-a05c-44de-a251-72f38c49eef8",
   "metadata": {},
   "source": [
    "### Plotting PACF \n",
    "\n",
    "We'll no plot the PACF for the white noise and AR(1) models we created above\n",
    "\n",
    "\n",
    "```\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "plot_pacf(x, ax=axes[0], lags=20, title='PACF of White Noise $(x_t)$')\n",
    "axes[0].set_ylim(-1.1, 1.1)  # Set y-axis limits\n",
    "axes[0].set_xticks(np.arange(0, 21, 1))  # Set x-ticks at increments of 1\n",
    "axes[0].grid(True, color='#D3D3D3')  \n",
    "\n",
    "plot_pacf(ar1_data, ax=axes[1], lags=20, title='PACF of AR(1) Model $(y_t)$')\n",
    "axes[1].set_ylim(-1.1, 1.1)  # Set y-axis limits\n",
    "axes[1].set_xticks(np.arange(0, 21, 1))  # Set x-ticks at increments of 1\n",
    "axes[1].grid(True, color='#D3D3D3')  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4cf41-e365-4b9d-82f2-4877cae25371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5af159e-7780-4e36-8a6e-acdb9baea2c8",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "### Interpretation of the PACF\n",
    "\n",
    "- We can see that for the white noise $x_t$ none of the PACF coefficients are statistically significant (for $k>0$)\n",
    "- For the AR(1) data PACF cuts for to zero after the first lag therefore we have an AR(1) model\n",
    "    - We now know that $p=1$ and we can estimate an AR(1) model and make predictions using it\n",
    "\n",
    "\n",
    " \n",
    "```\n",
    "\n",
    "\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "\n",
    "\n",
    "# Estimating the AR(1) model\n",
    "model = AutoReg(ar1_data, lags=1)  # lag=1 for AR(1)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Print the estimated parameters\n",
    "print(model_fit.summary())\n",
    "\n",
    "# Plot the original data and the fitted values\n",
    "plt.plot(ar1_data, label=\"Original Data\")\n",
    "plt.plot(model_fit.fittedvalues, label=\"Fitted/Predicted Values\", color='red')\n",
    "plt.title('AR(1) Model Fit')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb81be8-d505-4836-a6d0-76009c84654e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976cbc49-ebe0-4dad-aa19-8f1785eee353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f3dd4-1183-4b9a-81df-aa3017f13bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9800a3-2937-4ab5-883f-7ab6245178cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95692bd1-eeca-4f3c-8dd4-aa7f53dafe6c",
   "metadata": {},
   "source": [
    "<span style=\"background-color: yellow\">**Concept 6: Moving Average Models - MA(q)**</span>\n",
    "- Besides AR(p) models we also have MA(q) models\n",
    "- MA(q) model depends on $q$ values of the past error terms\n",
    "    - $y_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_q \\epsilon_{t-q}$\n",
    "- MA processes typically have shorter memory than AR models\n",
    "- Some MA examples\n",
    "    - MA(1): $y_t = 1.5 + \\epsilon_t + 0.7 \\epsilon_{t-1}$\n",
    "    - MA(2): $y_t = 2.0 + \\epsilon_t + 0.6 \\epsilon_{t-1} - 0.3 \\epsilon_{t-2}$\n",
    "    - MA(3): $y_t = 0.5 + \\epsilon_t + 0.8 \\epsilon_{t-1} + 0.4 \\epsilon_{t-2} - 0.2 \\epsilon_{t-3}$\n",
    "- Using ACF and PACF to identify an MA(q) model\n",
    "    - ACF: Cuts off sharply after lag q\n",
    "    - PACF: Gradually decays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f8204-8aba-42e9-ae2e-c8eb4dbd5633",
   "metadata": {},
   "source": [
    "<span style=\"background-color: yellow\">**Concept 7: Autoregressive Moving Average Models - ARMA(p,q)**</span>\n",
    "\n",
    "- The ARMA(p,q) model combines AR(p) and MA(q) components, making it a very flexible time series model\n",
    "    - $y_t = \\mu + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\dots + \\theta_q \\epsilon_{t-q}$\n",
    "- Examples:\n",
    "    - ARMA(1,1): $y_t = 1.0 + 0.5 y_{t-1} + \\epsilon_t + 0.3 \\epsilon_{t-1}$\n",
    "    - ARMA(2,1): $y_t = -0.5 + 0.6 y_{t-1} - 0.4 y_{t-2} + \\epsilon_t + 0.2 \\epsilon_{t-1}$\n",
    "    - ARMA(2, 2): $y_t = 0.8 + 0.7 y_{t-1} - 0.2 y_{t-2} + \\epsilon_t + 0.5 \\epsilon_{t-1} - 0.1 \\epsilon_{t-2}$\n",
    "- For and ARMA(p,q) model we have\n",
    "    - ACF: Gradual decay, influenced by both AR and MA terms\n",
    "    - PACF: Gradual decay, with no clear cut-off\n",
    "    - Difficult to identify from ACF and PACF only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1254e237-a320-4ded-bcab-8201496f8c5e",
   "metadata": {},
   "source": [
    "### Identification of MA(q), AR(p) and ARMA(p,q) Models From Data\n",
    "\n",
    "- We can use a combination of ACF, PACF and Information Criteria (not covered in our unit) to identify the type of model (AR, MA, or ARMA) and p and q\n",
    "    - Diagnostic: In a well chosen model the residuals should behave like white noise \n",
    "- Details are outside of scope of this unit\n",
    "- Below we will look at some Python liberaries that select the best ARMA type model this automatically\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0317c21b-28ea-4f0d-b073-b11cfbbd88bb",
   "metadata": {},
   "source": [
    "## Practical Forecasting of Time Series Data in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48104cf1-8eb2-4baa-9e50-161827bbd14f",
   "metadata": {},
   "source": [
    "There are a number of statistical Python libraries for doing Time Series analysis and forecasting. Some of them are as follows:\n",
    "\n",
    "\n",
    "- Nixtla\n",
    "    - [https://www.nixtla.io/](https://www.nixtla.io/)\n",
    "    - [https://github.com/nixtla](https://github.com/nixtla)\n",
    "- GluonTS\n",
    "    - [https://ts.gluon.ai/stable/index.html](https://ts.gluon.ai/stable/index.html)\n",
    "    - [https://github.com/awslabs/gluonts](https://github.com/awslabs/gluonts)\n",
    "- Merlion\n",
    "    - [https://opensource.salesforce.com/Merlion/v1.0.0/index.html](https://opensource.salesforce.com/Merlion/v1.0.0/index.html)\n",
    "    - [https://github.com/salesforce/Merlion]\n",
    "- pmdarima\n",
    "    - [http://alkaline-ml.com/pmdarima/](http://alkaline-ml.com/pmdarima/)\n",
    "    - [https://github.com/alkaline-ml/pmdarima](https://github.com/alkaline-ml/pmdarima)\n",
    "- statsmodels\n",
    "    - [https://www.statsmodels.org/stable/index.html](https://www.statsmodels.org/stable/index.html)\n",
    "    - [https://github.com/statsmodels/statsmodels/](https://github.com/statsmodels/statsmodels/)\n",
    "- sktime\n",
    "    - [https://www.sktime.net/en/stable/](https://www.sktime.net/en/stable/)\n",
    "    - [https://github.com/sktime/sktime](https://github.com/sktime/sktime)\n",
    "- Many other new libararies\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e5950-2f77-4aea-b1ec-b5d14332592c",
   "metadata": {},
   "source": [
    "## Using and Installing `sktime` Library\n",
    "\n",
    "Click on `File`-> `New` -> `Terminal` \n",
    "\n",
    "and run the following commands:\n",
    "\n",
    "```\n",
    "pip install sktime\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47fd0c-0fbc-40ef-b276-ce56092095a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efc1dd82-1b72-4464-ae51-95a693082857",
   "metadata": {},
   "source": [
    "```\n",
    "pip install pmdarima\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51092776-cda5-4807-aa73-223494d0f34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a300ed7-f980-4e78-a376-0f1e22f892fd",
   "metadata": {},
   "source": [
    "```\n",
    "pip install prophet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fef83f-4dc2-4893-8e7c-541f6ccb537e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "714e63b7-a815-4b97-a3e8-7a3b2e758a53",
   "metadata": {},
   "source": [
    "### Importing and Plotting Datasets from `sktime`\n",
    "\n",
    "- We are going to look at a famous time series dataset\n",
    "    - Number of monthly totals of international airline passengers from 1949 to 1960 (in thousands).\n",
    "\n",
    "\n",
    "```\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # hide warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sktime.utils.plotting import plot_series\n",
    "from sktime.datasets import load_airline\n",
    "\n",
    "\n",
    "y = load_airline()\n",
    "\n",
    "plot_series(y)\n",
    "plt.show()\n",
    "print(y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f4ba1-cfdd-45dd-9a3e-457c1b40ee69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24e8bf-455c-4ccb-8321-0b12631667c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de4f14-da71-40c1-ae9e-b0942fc56d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80907a0d-391b-4dab-a16f-aa482e754b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d347ca5c-d189-43a6-9ffd-6cc665f47279",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### \"Fixing\" `sktime`\n",
    "\n",
    "If the above code does NOT work we need to fix a little bug that in some versions of `sktime`    \n",
    "- If the code works ignore these instructions\n",
    "\n",
    "1. Open cmd terminal\n",
    "    - start -> type `cmd` \n",
    "2. In cmd terminal type `start C:\\Users\\USERNAME\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sktime\\datatypes\\_adapter\\`\n",
    "   - Replace USERNAME with your own user name\n",
    "3. Make a backup copy of \"dask_to_pd.py\" (just copy and paste it)\n",
    "4. Open \"dask_to_pd.py\" in notepad\n",
    "5. Search for ('ctrl'+f) 'import dask'\n",
    "6. add `import dask.dataframe` immediately below `import dask`\n",
    "   - make sure the indent is aligned\n",
    "7. It should look like this\n",
    "   <img src=\"images/fix-sktime.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "   \n",
    "9. restart kernel in Jupyer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d82bf9-57ca-48a7-ba70-78fc388b0ea3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- `sktime` relies on `pandas` `DataFrames` and the index here is set to `PeriodIndex` type from pandas\n",
    "    - If importing new data from a `pandas` `Dataframe` we must set the index to `PeriodIndex` in order for it to work with `sktime` \n",
    "\n",
    "```\n",
    "type(y.index)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0a253-86f8-4943-9ebf-9bc8c6e41b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9851ddb-ae38-4d5b-bf71-9ea73264e9f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Some Simple Univariate Time Series Models in `sktime`\n",
    "\n",
    "\n",
    "- Note that in time series analysis, a **season** refers to a repeating pattern or cycle in the data that occurs at regular intervals over time.\n",
    "    - This concept is crucial for analysing data that exhibits predictable fluctuations due to seasonal influences\n",
    "    - Examples: day-of-the-week seasonality, weekly, monthly, quarterly, annual seasonality, etc.\n",
    " \n",
    "      \n",
    "We will evaluate how several time series models perform on the airline passenger data\n",
    "\n",
    "- `NaiveForecaster` - makes forecasts using simple strategies\n",
    "    - https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.forecasting.naive.NaiveForecaster.html\n",
    "    - We can set `strategy` parameter to the following values\n",
    "        - 'last' - prediction will be the last value in the training series when sp is 1 ($\\hat{y}_{t+1}=y_t$).\n",
    "            - When sp is not 1, last value of each season in the last window will be forecasted for each season   \n",
    "        - 'mean' - forecast the mean of last window of training series when sp is 1 ($\\hat{y}_{t+1}=\\frac{1}{t}\\sum_{k=1}^{t} y_{t-k}$).\n",
    "            - When sp is not 1, mean of all values in a season from last window will be forecasted for each season\n",
    "        - 'drift' - forecast by fitting a line between the first and last point of the window and extrapolating it into the future.\n",
    "- `AutoETS` implements various types of exponential smoothing\n",
    "    - There are many types of exponential smoothing\n",
    "    - A simple type is where forecasts are calculated using weighted averages\n",
    "    - The weights are decreasing exponentially as observations go from further in the past\n",
    "        - Smallest weights are associated with the oldest observations\n",
    "    -  $\\hat{y}_{t+1}=\\alpha y_t +\\alpha (1-\\alpha) y_{t-1} + \\alpha(1-\\alpha)^2 y_{t-2} + \\dots \\quad$  where $0\\le \\alpha \\le 1$\n",
    "- `AutoARIMA` - AutoRegressive Integrated Moving Average models\n",
    "    - [https://otexts.com/fpp3/arima.html](https://otexts.com/fpp3/arima.html)\n",
    "    - [https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.forecasting.arima.AutoARIMA.html](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.forecasting.arima.AutoARIMA.html)\n",
    "- `Prophet`\n",
    "    - **Not Examinable** \n",
    "    - Prophet is open source software released by Facebook’s Core Data Science team\n",
    "    - [https://facebook.github.io/prophet/](https://facebook.github.io/prophet/)\n",
    "    - [https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.forecasting.fbprophet.Prophet.html](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.forecasting.fbprophet.Prophet.html)\n",
    "- `ThetaForecaster` - this method performed particularly well in many time series competition but it is actually equivalent to simple exponential smoothing with drift\n",
    "    - **Not Examinable** \n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f620b-fcdb-4e2f-9f1f-6d492e1e739f",
   "metadata": {},
   "source": [
    "## Model Comparisons on the Airline Passenger Dataset\n",
    "\n",
    "\n",
    "To compare model perfromances we will do the following:\n",
    "- Use the **holdout method** (week 7 lecture material) to fit and evaluate the performance of alternative models\n",
    "    - We will fit the models to the Training dataset\n",
    "    - Select the best model on the Validation set\n",
    "    - Evaluate performance and the extent of overfitting on the Test dataset\n",
    "- Performance Criteria\n",
    "    1. Mean Square Error $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$\n",
    "    2. Mean Absolute Errors $\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"images/week11_3.png\" alt=\"Drawing\" style=\"width: 400px;\"/>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4b524-2470-4ee1-b227-3eb0605e6ab5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 1: Split the data into Training, Validation and Test Datasets\n",
    "\n",
    "- Create Training, Test and Validation Datasets\n",
    "- Use `temporal_train_test_split` which splits the data into sequential train and test subsets\n",
    "    - [https://www.sktime.net/en/v0.11.4/api_reference/auto_generated/sktime.forecasting.model_selection.temporal_train_test_split.html](https://www.sktime.net/en/v0.11.4/api_reference/auto_generated/sktime.forecasting.model_selection.temporal_train_test_split.html)\n",
    "- `test_size` parameter can be either\n",
    "    - a float (fraction of sample)\n",
    "    - an integer (number of observations)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "\n",
    "\n",
    "# ------------- create test dataset ------------------\n",
    "y_train_refit, y_test = temporal_train_test_split(y, test_size=12)  # test set will have 12 observations\n",
    "fh_test = ForecastingHorizon(y_test.index, is_relative=False)\n",
    "print('train_refit shape', y_train_refit.shape)\n",
    "\n",
    "print('test shape', y_test.shape)\n",
    "print('test index', fh_test)\n",
    "\n",
    "\n",
    "# ------------- create training and validation datasets ------------------\n",
    "y_train, y_validate = temporal_train_test_split(y_train_refit, test_size=12)\n",
    "fh_validate = ForecastingHorizon(y_validate.index, is_relative=False)\n",
    "print('train shape', y_train.shape)\n",
    "\n",
    "print('validate shape', y_validate.shape)\n",
    "print('validate index', fh_validate)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa529583-84fe-4239-894b-03bee04006b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26079bde-2dea-4774-b099-3d23e8a9e348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6169b-e05e-4abc-aa84-b896c048ad0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5bb22d8-3abb-4202-82ba-1e1e52458059",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: List Forecasting Models in a Dictionary\n",
    "\n",
    "```\n",
    "\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.ets import AutoETS\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "from sktime.forecasting.fbprophet import Prophet\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "\n",
    "forecasters = {}   # a dictionary of forecasters\n",
    "\n",
    "# ----------------------------- MODELS WHERE WE DON'T USE SEASONALITY -----------------------\n",
    "forecasters['f_naive_last_no_sp'] = NaiveForecaster(strategy = 'last', sp = 1)  # last value without seasonal periodicity\n",
    "forecasters['f_naive_mean_no_sp'] = NaiveForecaster(strategy = 'mean', sp = 1)  # forecast the mean value\n",
    "forecasters['f_naive_drift_no_sp'] = NaiveForecaster(strategy = 'drift', sp = 1)  # forecast line trend\n",
    "forecasters['f_auto_ets_no_sp'] = AutoETS(auto=True, sp=1, n_jobs=-1)  # automatic exponential smoothing no seasonality\n",
    "forecasters['f_auto_arima_no_sp'] = AutoARIMA(sp=1)   # automatic ARIMA no seasonality\n",
    "forecasters['f_theta_no_sp'] = ThetaForecaster(sp=1)  \n",
    "\n",
    "\n",
    "# ----------------------------- MODELS WHERE WE USE SEASONALITY -----------------------\n",
    "forecasters['f_naive_last_with_sp'] = NaiveForecaster(strategy = 'last', sp = 12)  # last value in each season\n",
    "forecasters['f_naive_mean_with_sp'] = NaiveForecaster(strategy = 'mean', sp = 12)  # forecast the mean value of seasons\n",
    "forecasters['f_drift_drift_with_sp'] = NaiveForecaster(strategy = 'drift', sp = 12)  # forecast line trend\n",
    "forecasters['f_auto_ets_with_sp'] = AutoETS(auto=True, sp=12, n_jobs=-1)\n",
    "forecasters['f_auto_arima_with_sp'] = AutoARIMA(sp=12)   # automatic ARIMA with seasonality\n",
    "forecasters['f_theta_with_sp'] = ThetaForecaster(sp=12)    \n",
    "forecasters['f_prophet_sp_multiplicative']= Prophet(seasonality_mode='multiplicative')   \n",
    "forecasters['f_prophet_sp_additive']= Prophet(seasonality_mode='additive')   \n",
    "\n",
    "forecasters\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5be2f-b3b5-4e38-b819-8115b5b64c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc53a6-e5ff-4298-b2de-855cb8fa2159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac7c4460-fbba-48c1-9123-e136d407399a",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Fit the Models, Make Predictions of the Validation Dataset, Plot Forecasts\n",
    "\n",
    "- Fit all the Models\n",
    "- Predict Validation Dataset\n",
    "- Plot Forecasts\n",
    "- Compare Forecasts\n",
    "- Select Best Model\n",
    "\n",
    "  \n",
    "```\n",
    "predictions_validate = {}\n",
    "\n",
    "for forecaster in forecasters:\n",
    "    forecasters[forecaster].fit(y_train)\n",
    "    predictions_validate[forecaster] = forecasters[forecaster].predict(fh_validate) \n",
    "\n",
    "df_validate = pd.DataFrame(predictions_validate)\n",
    "\n",
    "plot_output = plot_series(y, *[df_validate[col] for col in df_validate.columns], labels = ['y'] + list(df_validate.columns))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_validate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2b2a6-1f9b-4306-8d4a-8779979966f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da7a02-bf65-4560-a0bb-d05eb93a3697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1836439d-4aee-4865-8cdd-f20ddfa87dc1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4: Compute Performance Criteria and Compare Models\n",
    "\n",
    "\n",
    "- Compute MSE and MAE\n",
    "- Rank and select best model\n",
    "\n",
    "```\n",
    "from sktime.performance_metrics.forecasting import MeanSquaredError, MeanAbsoluteError\n",
    "\n",
    "mse = MeanSquaredError()\n",
    "mae = MeanAbsoluteError()\n",
    "\n",
    "dict_mse = {}\n",
    "dict_errors = {}\n",
    "\n",
    "for forecaster in predictions_validate:\n",
    "    # Calculate error metric for each forecaster\n",
    "    forecaster_errors = {\n",
    "        'mse': mse(y_validate, predictions_validate[forecaster]),\n",
    "        'mae': mae(y_validate, predictions_validate[forecaster])\n",
    "    }\n",
    "    \n",
    "    # Store the errors in the dictionary\n",
    "    dict_errors[forecaster] = forecaster_errors\n",
    "\n",
    "df_errors_validation = pd.DataFrame.from_dict(dict_errors, orient='index') # convert to a DF\n",
    "df_errors_sorted_validation = df_errors_validation.sort_values('mse', ascending=True) # Sort errors by MSE\n",
    "df_errors_sorted_validation.insert(0, 'Rank', np.arange(1, len(df_errors_sorted_validation)+1)) # add rankings\n",
    "\n",
    "# Displaying the sorted DataFrame\n",
    "\n",
    "print('\\n Forecast Rankings on Validation Dataset According to MSE \\n')\n",
    "print(df_errors_sorted_validation)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e908c8e8-a092-46ce-bc6d-d2281bb86fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca325131-f502-4e5e-ac81-7f1830b65568",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 5: Refit the models to the entire (expanded) Training dataset & evaluate on Test dataset\n",
    "\n",
    "- Refit the models to the entire training dataset\n",
    "- Re-evaluate performance on the test dataset\n",
    "- Assess the consistency of ranking and the extent of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f09d03d-1295-47d9-b56f-7051f054d633",
   "metadata": {},
   "source": [
    "1. Refit the models\n",
    "\n",
    "```\n",
    "predictions_test = {}\n",
    "\n",
    "for forecaster in forecasters:\n",
    "    forecasters[forecaster].fit(y_train_refit)\n",
    "    predictions_test[forecaster] = forecasters[forecaster].predict(fh_test) \n",
    "\n",
    "df_test = pd.DataFrame(predictions_test)\n",
    "\n",
    "plot_output = plot_series(y, *[df_test[col] for col in df_test.columns], labels = ['y'] + list(df_test.columns))\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_test\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a4d55-9415-4ec7-8fd1-53ee57e94e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "181052f8-c166-407d-9698-00b1b24772d4",
   "metadata": {},
   "source": [
    "2. Compare Predictions\n",
    "\n",
    "```\n",
    "dict_mse = {}\n",
    "dict_errors = {}\n",
    "\n",
    "for forecaster in predictions_test:\n",
    "    # Calculate error metric for each forecaster\n",
    "    forecaster_errors = {\n",
    "        'mse': mse(y_test, predictions_test[forecaster]),\n",
    "        'mae': mae(y_test, predictions_test[forecaster])\n",
    "    }\n",
    "    \n",
    "    # Store the errors in the dictionary\n",
    "    dict_errors[forecaster] = forecaster_errors\n",
    "\n",
    "df_errors_test = pd.DataFrame.from_dict(dict_errors, orient='index') # convert to a DF\n",
    "df_errors_sorted_test = df_errors_test.sort_values('mse', ascending=True) # Sort errors by MSE\n",
    "df_errors_sorted_test.insert(0, 'Rank', np.arange(1, len(df_errors_sorted_test)+1)) # add rankings\n",
    "\n",
    "# Displaying the sorted DataFrame\n",
    "\n",
    "print('\\n Forecast Rankings on Test Dataset According to MSE \\n')\n",
    "print(df_errors_sorted_test)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da926aa-4f2a-444a-8865-4aba38ebd924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50207612-0e84-402c-9de3-7b026daf2a31",
   "metadata": {},
   "source": [
    "\n",
    "- Print again validation results\n",
    "\n",
    "```\n",
    "print('\\n Forecast Rankings on Validation Dataset According to MSE \\n')\n",
    "print(df_errors_sorted_validation)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368804a-3fa4-4090-8964-0d69e57f1336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f181658-7968-46d4-a38a-5706ad1ecdbe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "- Model rankings change but the best models remain on the top of the list\n",
    "    - Prophet, ARIMA, ETS (all with seasonality) rank high on the list\n",
    "- Holdout method is not the best way to evaluate performance (as discussed in week 7)\n",
    "    - Cross-validation, Rolling and Expanding Forecast Windows are better at selecting optimal models\n",
    "    - These are more complicated techniques and left for more advanced units on time series forecasting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
