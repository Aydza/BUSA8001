{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Weeks 10 & 11 - Predicting Continuous Target Variables with Regression Analysis {-}\n",
    "\n",
    "\n",
    "### Unit Convenor & Lecturer {-}\n",
    "[George Milunovich](https://www.georgemilunovich.com)  \n",
    "[george.milunovich@mq.edu.au](mailto:george.milunovich@mq.edu.au)\n",
    "\n",
    "### References {-}\n",
    "\n",
    "1. Python Machine Learning 3rd Edition by Raschka & Mirjalili - Chapter 10\n",
    "2. Various open-source material\n",
    "\n",
    "## Overview {-}\n",
    "\n",
    "- Introducing Regression  \n",
    "  - Simple Linear Regression  \n",
    "  - Multiple Linear Regression  \n",
    "- Exploring the Housing Dataset  \n",
    "  - Visualizing the Important Characteristics of a Dataset  \n",
    "- Implementing an Ordinary Least Squares Linear Regression Model  \n",
    "  - Estimating Regression Parameters with Gradient Descent  \n",
    "  - Estimating the coefficient of a regression model via scikit-learn  \n",
    "- Fitting a Robust Regression Model using RANSAC  \n",
    "- Evaluating the Performance of Linear Regression Models  \n",
    "- Using Regularized Methods for Regression  \n",
    "- Turning a Linear Regression Model Into a Curve - Polynomial Regression  \n",
    "  - Modeling Nonlinear Relationships in the Housing Dataset  \n",
    "  - Dealing with Nonlinear Relationships using Random Forests  \n",
    "    - Decision Tree Regression  \n",
    "    - Random Forest Regression  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Revision: Supervised Learning**  \n",
    "- *Classification*: a prediction problem where a class label is predicted for a given example of input data.   \n",
    "    - E.g. predict if a mortgage customer will default on a mortgage payment or not \n",
    "    - Classify if an email is spam or not.  \n",
    "- *Regression Analysis*: a prediction problem where the predicted target variable is continuous  \n",
    "    - E.g. predict house prices on the basis of the house characteristics such as the number of bedrooms, bathrooms, etc.   \n",
    "    \n",
    "## Introducing Linear Regression {-}  \n",
    "\n",
    "<br>\n",
    "\n",
    "**Linear Regression**  \n",
    "- Model the linear relationship between one or multiple features and a **numeric target variable**  \n",
    "- Predict outputs on a continuous scale rather than categorical class labels  \n",
    "- Model Training: learn regression parameters (weights) from the training dataset which will be used to make a prediction $\\hat{y}$ \n",
    "\n",
    "<br>\n",
    "\n",
    "**Simple (Univariate) Linear Regression**  \n",
    "\n",
    "\n",
    "Model the relationship between a single feature (explanatory variable $x$) and a continuous target $y$  \n",
    "\n",
    "True values of the target variable: $y= w_0 + w_1x + u$   \n",
    "Predictions of the target variable: $\\hat{y}=w_0 + w_1x$    \n",
    "Prediction Errors (residuals) are computed as $u = y - \\hat{y}$     \n",
    "\n",
    "- $w_0$ - intercept  \n",
    "- $w_1$ - regression slope / weight coefficient of the explanatory variable   \n",
    "- $u$ = residual  \n",
    "\n",
    "<img src=\"images/10_01.png\" alt=\"Drawing\" style=\"width: 400px;\"/>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression {-}\n",
    "\n",
    "- Multiple Linear Regression: A linear regression with multiple explanatory variables  \n",
    "- E.g Multiple linear regression with two exlanatory variables, $k=2$  \n",
    "\n",
    "$y = w_0 x_0 + w_1x_1 + \\dots + w_kx_k \\quad$        where we set $x_0=1$  \n",
    "\n",
    "<img src=\"images/10_16.png\" alt=\"Drawing\" style=\"width: 400px;\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exploring the Housing Dataset {-}  \n",
    "\n",
    "- House prices and 13 house attributes from the 1970s \n",
    "\n",
    "\n",
    "\n",
    "- Target: median value of owner-occupied homes in $1000s  \n",
    "\n",
    "- Columns  \n",
    "\n",
    "1. CRIM      per capita crime rate by town  \n",
    "2. ZN        proportion of residential land zoned for lots over 25,000 sq.ft.  \n",
    "3. INDUS     proportion of non-retail business acres per town  \n",
    "4. CHAS      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)  \n",
    "5. NOX       nitric oxides concentration (parts per 10 million)  \n",
    "6. RM        average number of rooms per dwelling  \n",
    "7. AGE       proportion of owner-occupied units built prior to 1940  \n",
    "8. DIS       weighted distances to five Boston employment centres  \n",
    "9. RAD       index of accessibility to radial highways  \n",
    "10. TAX      full-value property-tax rate per \\$10,000  \n",
    "11. PTRATIO  pupil-teacher ratio by town  \n",
    "12. LSTAT    Percentage of low income residents  \n",
    "13. MEDV     Median value of owner-occupied homes in \\$1000s  \n",
    "\n",
    "\n",
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/Housing.csv')\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.info()\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "## EDA - Exploring the Important Characteristics of a Dataset {-} \n",
    "\n",
    "- **Exploratory Data Analysis (EDA)**\n",
    "    - EDA refers to analysing data to summarise their main characteristics, often using visual methods.\n",
    "    - EDA helps to uncover patterns, relationships, anomalies, and other insights within the data.\n",
    "    - It involves techniques like data visualisation, descriptive statistics, and sometimes simple modelling.\n",
    "    - **EDA is often a preliminary step before more formal statistical techniques / analytics are applied and can be crucial for understanding the underlying structure of data sets.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Some Common EDA Techniques**\n",
    "    - Descriptive Statistics Tables - provide basic summary statistics, e.g. mean, variance, min, max, etc.\n",
    "    - Histograms (numeric data) / Pie Charts (categorical data) - a histogram is a graphical representation of the distribution of numerical data. \n",
    "    - Scatter Plots - display the relationship between two variables.\n",
    "    - Correlation plots/tables/matrices - are a tabular representation of the correlation coefficients between pairs of variables in a data set.|\n",
    "- Lets consider the basic summary statistics and some histograms first\n",
    "\n",
    "\n",
    "1. Summary Statistics\n",
    "```\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "df.describe()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Histogram\n",
    "\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 4))  \n",
    "\n",
    "sns.histplot(data=df, x='MEDV', stat='percent', ax=axes[0])  \n",
    "axes[0].set_title('Histogram for MEDV')\n",
    "\n",
    "sns.histplot(data=df, x='RM', stat='percent',ax=axes[1])  \n",
    "axes[1].set_title('Histogram for RM')\n",
    "\n",
    "sns.histplot(data=df, x='LSTAT', stat='percent', ax=axes[2])  \n",
    "axes[2].set_title('Histogram for LSTAT')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Next we'll consider scatter plots between all pairs of variables in the dataset\n",
    "    - A scatter plot is a type of data visualisation that displays individual data points as dots on a two-dimensional coordinate system\n",
    "    - Each dot represents the value of two variables‚Äîone plotted along the horizontal axis (x-axis) and the other plotted along the vertical axis (y-axis)\n",
    " \n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(df)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "4. **Correlation**\n",
    "- Linear correlation, also known as Pearson correlation, measures the strength and direction of the linear relationship between two continuous variables.\n",
    "- The key concept here is that of **linear** relationship -> it may not capture nonlinear relationships\n",
    "- $r=\\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}\\in[-1,1]$\n",
    "- The closer the correlation coefficient is to 1 or -1, the stronger the linear relationship between the variables.\n",
    "- A correlation coefficient closer to 0 indicates a weaker linear relationship.\n",
    "- Direction of Correlation: The sign of the correlation coefficient indicates the direction of the relationship:\n",
    "    - Positive $r$ indicates a positive correlation: as one variable increases, the other variable tends to increase.\n",
    "    - Negative $r$ indicates a negative correlation: as one variable increases, the other variable tends to decrease.\n",
    "    - $r=-1$ -> perfect negative correlation  \n",
    "    - $r=0$ -> no linear relationship  \n",
    "    - $r=1$ -> perfect positive correlation  \n",
    "    - Note: the covariance between standardized features is actually equal to their correlation coefficient  \n",
    "\n",
    "5. **Correlation Matrix**  \n",
    "    - Plot all correlations in one graph  \n",
    "    - Look for high (both positive and negative) correlations  \n",
    "    - From last row we see  \n",
    "        - High positive (r=0.7) correlation between RM and MEDV and\n",
    "        - High negative (r=-0.74) correlation between LSTAT and MEDV (but from scatterplot this relationship looks nonlinear)   \n",
    "    - Use `corr()` from `pandas` to get correlation matrix, plot it using `heatmap` from `seaborn`  \n",
    "    \n",
    "```\n",
    "df.corr()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also plot the correlation matrix inside a heat map\n",
    "    - Very light colours represent large positive correlations\n",
    "    - Very dark colours represent large negative correlations\n",
    "- Both large positive and large negative can be used in predictive models\n",
    "\n",
    "```\n",
    "import seaborn as sns\n",
    "\n",
    "corrmat = df.corr()\n",
    "# print(corrmat.round(3).to_string())\n",
    "# corrmat\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.heatmap(corrmat, annot=True, vmax=.8, square=True)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "## Implementing an Ordinary Least Squares Linear Regression Model {-}  \n",
    "\n",
    "- Linear regression computes the equation of the best-fitting straight line through the examples of the dataset  \n",
    "- How is 'best-fitting' defined?  \n",
    "    - Ordinary Least Squares (OLS) - estimate parameters of the linear regression line which *minimises* the sum of squared verticial distances (prediction errors/residuals) from the estimated line to the training examples   \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OLS implements the same cost function (of predictions errors) that Adaline employs to estimate its weights  \n",
    "    - Sum of Squared Errors (SSE) cost function: $J(w)=\\frac{1}{2}\\sum_{i=1}^n\\left(y_{i} - \\hat{y}_{i}\\right)^2$  \n",
    "    - Given that that cost function is **exactly the same as in Adaline** we can optimize regression weights (betas) using gradient descent (GD) or stochastic gradient descent (SGD) as in Adaline  \n",
    "        - A gradient descent implementation of OLS regression is provided in the textbook  \n",
    "    - Besides GD and similar iterative optimisation algorithms there is a closed form solution for solving OLS parameters  \n",
    "        - $w=(X^{'}X)^{-1}X^{'}y$ making sure that the matrix $X$ has a column of 'ones' (usually first column)  \n",
    "        - This solution for $w$ does not require standardising the data  \n",
    "    - Regression predictions are formed as $\\hat{y}=w_0x_0 + w_1x_1 + w_2x_2 + \\dots + w_kx_k = \\sum_{i=1}^k w_ix_i$  \n",
    "        - OLS regression is essentially equal to Adaline but without the unit step function so that we obtain continuous predictions of the target variable (rather than class labels -1 and 1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "<span style='background:orange'>  **Example: Closed-form solution for simple linear regression**    \n",
    "\n",
    "1. Add a column of 'ones' to df  \n",
    "2. Set 'MEDV' - median house price value - as $y$, ['ones', 'RM'], where RM is the number of rooms as $X$  \n",
    "3. Compute the weights (betas) of the linear regression $\\hat{y}=w_0 + w_1x$ using the following formula $w=(X^{'}X)^{-1}X^{'}y$  \n",
    "4. Compute the fitted values (predictions) of $y$ given $X$ using the formula $\\hat{y}=Xw$ and plot the prediction together with the scatterplot of RM and MEDV  \n",
    "5. Predict house price for a house with 5 rooms using the formula $ùë¶ÃÇ=ùë§‚Ä≤ùë•$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "1. Add a column of 'ones' to df\n",
    "\n",
    "```\n",
    "df['ones'] = 1\n",
    "df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "2. Set 'MEDV' - median house price value - as $y$, ['ones', 'RM'], where RM is the number of rooms as $X$\n",
    "\n",
    "```\n",
    "X = df[['ones', 'RM']].values\n",
    "y = df['MEDV'].values\n",
    "\n",
    "# X\n",
    "# y\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "3. Compute the weights (betas) of the linear regression $\\hat{y}=w_0 + w_1x$ using the following formula $w=(X^{'}X)^{-1}(X^{'}y)$\n",
    "\n",
    "```\n",
    "w = np.linalg.inv(X.T@X)@(X.T@y)\n",
    "\n",
    "print(w)\n",
    "\n",
    "print(f'Intercept: {w[0]:.3f}')\n",
    "print(f'Slope: {w[1]:.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "4. Compute the fitted values (predictions) of $y$ given $X$ using the formula $\\hat{y}=w_0x_0+w_1x_1=Xw$ and plot the prediction together with the scatterplot of RM and MEDV\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_hat = X@w\n",
    "\n",
    "plt.scatter(X[:,1], y, c = 'steelblue', edgecolor='white', s=70)\n",
    "plt.plot(X[:,1], y_hat, color = 'black', lw = 2)\n",
    "\n",
    "plt.xlabel('Average number of rooms [RM]')\n",
    "plt.ylabel('Price in $1,000 [MEDV]')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "5. Predict house price for a house with 5 rooms using the formula $\\hat{y}=w_0x_0+w_1x_1= w^{'}x$\n",
    "\n",
    "```\n",
    "x_for_prediction = [1, 5]\n",
    "\n",
    "y_hat_x_5 = w.T@x_for_prediction\n",
    "print(f'Predicted price (in $ thousands) for X = 5 rooms: {y_hat_x_5:.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "<span style='background:orange'>  **Example: Simple linear regression with scikit-learn**   \n",
    "    \n",
    "1. Compute the weights (betas) of the linear regression $\\hat{y}=w_0 + w_1x$ using the `fit` method of `LinearRegression`  \n",
    "2. Compute the fitted values (predictions) of $y$ given $X$ using the `predict` method and plot the prediction together with the scatterplot of RM and MEDV   \n",
    "3. Predict house price for a house with 5 rooms using `predict` method  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "1. Compute the weights (betas) of the linear regression $\\hat{y}=w_0 + w_1x$ using the `fit` method of `LinearRegression`\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "slr = LinearRegression()\n",
    "\n",
    "slr.fit(X=df[['RM']], y=df['MEDV'])\n",
    "\n",
    "print(f'Intercept: {slr.intercept_:.3f}')\n",
    "print(f'Slope: {slr.coef_[0]:.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "2. Compute the fitted values (predictions) of $y$ given $X$ using the `predict` method and plot the prediction together with the scatterplot of RM and MEDV\n",
    "\n",
    "```\n",
    "y_predict = slr.predict(df[['RM']])\n",
    "# y = df['MEDV']\n",
    "\n",
    "plt.scatter(df[['RM']], df['MEDV'], c = 'steelblue', edgecolor='white', s=70)\n",
    "plt.plot(df[['RM']], y_predict, color = 'black', lw = 2)\n",
    "\n",
    "# plt.scatter(X[:,1], y, c = 'steelblue', edgecolor='white', s=70)\n",
    "# plt.plot(X[:,1], y_predict, color = 'black', lw = 2)\n",
    "\n",
    "plt.xlabel('Average number of rooms [RM]')\n",
    "plt.ylabel('Price in $1,000 [MEDV]')\n",
    "plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "3. Predict house price for a house with 5 rooms using `predict` method\n",
    "\n",
    "```\n",
    "# X_predict = np.array([5]) # 1-d array doesn't work with sklearn\n",
    "# X_predict = np.array([5]).reshape(-1,1)  # 1st way of reshaping into a 2-d array\n",
    "# X_predict = np.array([[5]]) # 2nd way of reshaping\n",
    "\n",
    "X_predict = pd.DataFrame({'RM': [5]})\n",
    "\n",
    "y_hat_x_5_sklearn = slr.predict(X_predict)\n",
    "print(f'Predicted price (in $ thousands) for X = 5 rooms: {y_hat_x_5_sklearn[0]:.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Fitting a Robust Regression Model using RANSAC {-}  \n",
    "\n",
    "**Outliers**: data points that differ significantly from other observations.   \n",
    "- An **outlier** may represent a true low probability event, or it may indicate a measurement error  \n",
    "- Regression models can be heavily impacted by the presence of outliers  \n",
    "    - This is a problem since regression models are meant to capture relationships which hold on average, and outliers, by definition, are far away from the average\n",
    "    - Thus, outliers can affect the results and statistical analyses significantly, often skewing the data disproportionately     \n",
    "- There are many statistical tests to detect outliers  \n",
    "    - Once detected outliers are sometimes removed from the dataset  \n",
    "- Instead of directly removing outliers we will implement the RANdom SAmple Consensus (RANSAC) algorithm  \n",
    "    - Fit a regression model to a subset of data, the so-called **Inliers**  \n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "**RANSAC Algorithm**  \n",
    "- `from sklearn.linear_model import RANSACRegressor`  \n",
    "    - Set the smallest number of examples to be used in estimation\n",
    "    - Select allowed distance from the fitted line, and \n",
    "    - Expand the initial set with consistent data points  \n",
    "\n",
    "\n",
    "1. Select a minimum number of randomly chosen examples to be treated as inliers and fit the model  \n",
    "    - E.g. set `min_samples = 50`  \n",
    "2. Test all other data points against the fitted model and add those points which are within a user-given distance from the fitted line  \n",
    "    - E.g. set the distance to 5  (`residual_threshold=5`) - Maximum residual for a data sample to be classified as an inlier  \n",
    "    - This value is problem specific and while 5 works in our problem it may not always work. This is a disadvantage of RANSAC algorithm  \n",
    "3. Refit the model using all inliers  \n",
    "4. Terminate the algorithm if the fraction of the number of inliers over the sample size exceeds a predefined threshold or if a fixed number of iterations are reached. Go to step 1 otherwise.  \n",
    "    - max_trials: after all trials, the model with the highest number of inliers is selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "<span style='background:orange'>  **Example: Fit linear regression model with the RANSAC algorithm**   \n",
    "\n",
    "1. Fit a linear regression via RANSAC with 'MEDV' - median house price value - as $ùë¶$, 'RM' as $x$  \n",
    "2. Plot the fitted line and label datapoints as inliers (use `inliear_mask_`) or outliers  \n",
    "3. Print the slope and intercept of the estimated (robust) regression line  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "1. Fit a linear regression via RANSAC with 'MEDV' - median house price value - as $ùë¶$, 'RM' as $x$\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "X = df[['RM']]\n",
    "y = df['MEDV']\n",
    "\n",
    "ransac = RANSACRegressor(LinearRegression(), \n",
    "                         max_trials=100, \n",
    "                         min_samples=50, \n",
    "                         residual_threshold=5.0,   # example an inlier if distance from the fitted line within 5 units\n",
    "                         random_state=0)\n",
    "\n",
    "ransac.fit(X, y)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "2. Plot the fitted line and label datapoints as inliers (use `inliear_mask_`) or outliers\n",
    "\n",
    "```\n",
    "inlier_mask = ransac.inlier_mask_\n",
    "outlier_mask = np.logical_not(inlier_mask)\n",
    "\n",
    "# print(inlier_mask[inlier_mask == True].shape)\n",
    "# print(pd.DataFrame(np.vstack((inlier_mask, outlier_mask)).T))\n",
    "# print(pd.DataFrame(np.vstack((inlier_mask, outlier_mask)).T).value_counts())\n",
    "\n",
    "line_X = np.arange(3, 10, 1)\n",
    "\n",
    "line_y_ransac = ransac.predict(line_X.reshape(-1,1))\n",
    "\n",
    "plt.scatter(X[inlier_mask], y[inlier_mask], c='steelblue', edgecolor='white', marker='o', label='Inliers')\n",
    "plt.scatter(X[outlier_mask], y[outlier_mask], c='limegreen', edgecolor='white', marker='s', label='Outliers')\n",
    "plt.plot(line_X, line_y_ransac, color='black', lw=2)   \n",
    "\n",
    "plt.xlabel('Average number of rooms [RM]')\n",
    "plt.ylabel('Price in $1000s [MEDV]')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "#plt.savefig('images/10_08.png', dpi=300)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "3. Print the slope and intercept of the estimated (robust) regression line\n",
    "\n",
    "```\n",
    "print(f'Slope: {ransac.estimator_.coef_[0]:.3f}')\n",
    "print(f'Intercept: {ransac.estimator_.intercept_:.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "for the entire dataset\n",
    "\n",
    "Slope: 9.102\n",
    "Intercept: -34.671"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluating the Performance of Linear Regression Models {-}  \n",
    "\n",
    "  \n",
    "1. **Mean Squared Error (MSE)**  \n",
    "- $MSE=\\frac{1}{n}\\sum_{i=1}^n\\left(y_{i} - \\hat{y}_{i}\\right)^2$   \n",
    "- MSE = average of squared prediction errors\n",
    "- Objective: **Minimise MSE** (build a model to make it as small as possible)\n",
    "- Interpretation depends on feature scaling/units of measurement  \n",
    "- Can be computed both on:  \n",
    "    - Training dataset -> in order to tune hyperparameters and compare/rank different models \n",
    "    - Test dataset -> evaluate forecasting performance, see if the model generalises well, and compare/rank different models  \n",
    "    - If the MSE computed on the training dataset is much smaller than the MSE computed from test examples -> sign of overfitting  \n",
    "  \n",
    "2. **Coefficient of Determination $(R^2)$**  \n",
    "- $R^2 = 1 - \\frac{\\sigma_u^2}{\\sigma_y^2}=r_{y,\\hat{y}}^2\\in[0,1]$  \n",
    "- If $R^2=1\\Rightarrow MSE=0\\Rightarrow$Perfect Fit\n",
    "\n",
    "3. **Residual Plot** - plot the **Residuals** (y-axis) vs. **Predicted Values** (x-axis)  \n",
    "- If a model provides a good fit then Residuals (errors) should be:  \n",
    "    - Randomly distributed around the centre line  \n",
    "    - No outliers with large deviations from the centre line  \n",
    "- Indicators of potential problems  \n",
    "    - Patterns in residual plots -> model unable to capture some explanatory features (potential missing variables or nonlinearity) \n",
    "    - Large outliers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "\n",
    "<span style='background:orange'>  **Example: Evaluating Regression Performance**   \n",
    "    \n",
    "1. Use all house features as X and set y = MEDV. Split data into train and test datasets with test size being 30% of the total sample size.  \n",
    "2. Fit a linear regression to training data, and predict house prices for both training and test datasets  \n",
    "3. Compute MSE and $R^2$ for the train and test datasets  \n",
    "4. Graph the residual plot for both training and test datasets  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "1. Use all house features as X and set y = MEDV. Split data into train and test datasets with test size being 30% of the total sample size.\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['MEDV']\n",
    "X = df.drop(columns = ['MEDV', 'ones'])\n",
    "\n",
    "X\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "2. Fit a linear regression to training data, and predict house prices for both training and test datasets\n",
    "\n",
    "```\n",
    "slr = LinearRegression()\n",
    "\n",
    "slr.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = slr.predict(X_train)\n",
    "y_test_pred = slr.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "3. Compute MSE and $R^2$ for the training and test datasets\n",
    "\n",
    "```\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(f'MSE train: {mean_squared_error(y_train, y_train_pred):.3f}, test: {mean_squared_error(y_test, y_test_pred):.3f}')\n",
    "print(f'R^2 train: {r2_score(y_train, y_train_pred):.3f}, test: {r2_score(y_test, y_test_pred):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "3. Graph the residual plot for both training and test datasets\n",
    "\n",
    "```\n",
    "plt.scatter(y_train_pred,  y_train_pred - y_train, c='steelblue', marker='o', edgecolor='white', label='Training data')\n",
    "plt.scatter(y_test_pred,  y_test_pred - y_test, c='limegreen', marker='s', edgecolor='white', label='Test data')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper left')\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)\n",
    "plt.xlim([-10, 50])\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('images/10_09.png', dpi=300)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Using Regularized Methods for Regression {-}  \n",
    "\n",
    "- In regression analysis, like in classification, we often face the problem of overfitting  \n",
    "    - Regularisation: introduce a penalty against complexity by shrinking parameter values  \n",
    "    - Regularisation methods in regression analysis:  \n",
    "        - L2 regularisation: Ridge Regression  \n",
    "        - L1 regularisation: LASSO (Least Absolute Shrinkage and Selection Operator)  \n",
    "        - L1 + L2 regularisation: Elastic Net  \n",
    "         \n",
    "**Ridge Regression**  \n",
    "- Ridge Regression is just an L2 penalised model that we have seen before  \n",
    "- We modify the least-squares cost function as follows $J(w)=\\sum_{i=1}^n\\left(y_{i} - \\hat{y}_{i}\\right)^2 + \\lambda||w||_2^2$   \n",
    "    - As before $\\lambda||w||_2^2=\\lambda\\sum_{j=1}^mw_j^2$  \n",
    "    - Hyperparameter $\\lambda$ - strength of regularization $(\\lambda\\uparrow\\Rightarrow$Regularization$\\uparrow)$   \n",
    "    - Note: do not regularize the intercept $w_0$  \n",
    "    \n",
    "**LASSO (Least Absolute Shrinkage and Selection Operator)**  \n",
    "- L1 regularization  \n",
    "- $J(w)=\\sum_{i=1}^n\\left(y_{i} - \\hat{y}_{i}\\right)^2 + \\lambda||w||_1$   \n",
    "    - $\\lambda||w||_1=\\lambda\\sum_{j=1}^m|w_j|$  \n",
    "- When regularization strength is high-> certain weights can become zero -> LASSO = supervised feature-selection technique  \n",
    "- Limitation: selects at most $n$ features when $m>n$   \n",
    "    - $m=$ number of features, $n=$ is the number of observations\n",
    "    - However this also avoids saturation (when n = m -> saturated model -> fits data perfectly but does not generalise well)  \n",
    "    \n",
    "**Elastic Net**  \n",
    "- Both L1 and L2 penalties  \n",
    "- $J(w)=\\sum_{i=1}^n\\left(y_{i} - \\hat{y}_{i}\\right)^2 + \\lambda_1||w||_2^2 + \\lambda_2||w||_1$   \n",
    "    - L1 penalty generates sparsity (feature selection)  \n",
    "    - L2 penalty allows for selecting more than $n$ features when $m>n$  \n",
    "    \n",
    "    \n",
    "**Implementations in scikit-learn**  \n",
    "- Use similarly to `LinearRegression`  \n",
    "    - `from sklearn.linear_model import Ridge`  \n",
    "    - `from sklearn.linear_model import Lasso`  \n",
    "    - `from sklearn.linear_model import ElasticNet`  \n",
    "- Must specify $\\lambda_1$ and/or $\\lambda_2$  \n",
    "    - These are most often optimised via k-fold cross-validation   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "<span style='background:orange'>  **Example: Regularized Regression Models**   \n",
    "    \n",
    "1. Using the same data as in the last exercise above fit a LASSO regression to training data, and predict house prices for both training and test datasets  \n",
    "2. Compute $R^2$ and MSE for both train and test datasets and print estimated LASSO coefficients  \n",
    "3. Repeat 1. and 2 using a Ridge regression  \n",
    "4. Repeat 1. and 2 using an Elastic Net regression  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "1. Using the same data as in the last exercise above fit a LASSO regression to training data, and predict house prices for both training and test datasets\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import Lasso\n",
    "    \n",
    "lasso = Lasso(alpha=0.1) # alpha = lambda (above)\n",
    "\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = lasso.predict(X_train)\n",
    "y_test_pred = lasso.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "2. Compute $R^2$ and MSE for both train and test datasets and print estimated LASSO coefficients\n",
    "\n",
    "```\n",
    "np.set_printoptions(precision=3, suppress = True)       # format printing to 3 decimal places in numpy\n",
    "\n",
    "\n",
    "print(f'LASSO MSE train: {mean_squared_error(y_train, y_train_pred):.3f}, test: {mean_squared_error(y_test, y_test_pred):.3f}')\n",
    "print(f'LASSO R^2 train: {r2_score(y_train, y_train_pred):.3f}, test: {r2_score(y_test, y_test_pred):.3f}\\n')\n",
    "\n",
    "print(f'LASSO coefficients:\\n {lasso.coef_}')\n",
    "print(f'LASSO intercept:\\n {lasso.intercept_}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "3. Repeat 1. and 2 using a Ridge regression\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=0.1)\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_ridge = ridge.predict(X_train)\n",
    "y_test_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "print(f'Ridge MSE train: {mean_squared_error(y_train, y_train_pred_ridge):.3f}, test: {mean_squared_error(y_test, y_test_pred_ridge):.3f}')\n",
    "print(f'Ridge R^2 train: {r2_score(y_train, y_train_pred_ridge):.3f}, test: {r2_score(y_test, y_test_pred_ridge):.3f}\\n')\n",
    "\n",
    "print(f'Ridge coefficients:\\n {ridge.coef_}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "4. Repeat 1. and 2 using an Elastic Net regression\n",
    "\n",
    "```\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elasticnet = ElasticNet(alpha=0.001, l1_ratio=0.5)\n",
    "\n",
    "elasticnet.fit(X_train, y_train)\n",
    "y_train_pred_elasticnet = elasticnet.predict(X_train)\n",
    "y_test_pred_elasticnet = elasticnet.predict(X_test)\n",
    "\n",
    "print(f'Elastic Net MSE train: {mean_squared_error(y_train, y_train_pred_elasticnet):.3f}, test: {mean_squared_error(y_test, y_test_pred_elasticnet):.3f}')\n",
    "print(f'Elastic Net R^2 train: {r2_score(y_train, y_train_pred_elasticnet):.3f}, test: {r2_score(y_test, y_test_pred_elasticnet):.3f}\\n')\n",
    "\n",
    "print(f'Elastic Net coefficients:\\n {elasticnet.coef_}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Turning a Linear Regression Model into a Curve - Polynomial Regression {-}  \n",
    "\n",
    "- So far we have assumed a linear relations between the target $y$ and features $x$  \n",
    "- One way to account for nonlinear patterns is to use a polynomial regression  \n",
    "    - $y=w_0 + w_1x + w_2x^2 + \\dots + w_dx^d$  \n",
    "        - Here $d$ denotes the degree of the polynomial  \n",
    "        - Note: adding polynomial features increases the complexity of the model which raises the chance of overfitting, and is the reason why we use kernel methods in the first place  \n",
    "    - `from sklearn.preprocessing import PolynomialFeatures`   \n",
    "- An alternative to polynomial features is a log-transformation  \n",
    "    - Either just transform $x$, or both $x$ and $y$  \n",
    "        - E.g. $y=w_0 + w_1 log(x)$ or  \n",
    "        - E.g. $log(y) = w_0 + w_1 log(x)$  -> must convert back to $y$ after making the predictions using $\\hat{y}=exp(log(\\hat{y}))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "<span style='background:orange'>  **Example: Regressions with Polynomial Features**   \n",
    "\n",
    "1. Generate nonlinear data and plot it \n",
    "2. Create a quadratic term of $x$ using `PolynomialFeatures`  \n",
    "3. Fit a linear regression and a polynomial regression & plot fitted values vs true data  \n",
    "4. Predict $y$ using training dataset for X, compare MSE and $R^2$ across the linear and quadratic regressions  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "1. Generate nonlinear data and plot it \n",
    "\n",
    "```\n",
    "import seaborn as sns\n",
    "\n",
    "X = np.array([258.0, 270.0, 294.0, 320.0, 342.0, 368.0, 396.0, 446.0, 480.0, 586.0]).reshape(-1,1)\n",
    "y = np.array([236.4, 234.4, 252.8, 298.6, 314.2, 342.2, 360.8, 368.0, 391.2, 390.8])\n",
    "\n",
    "df2 = pd.DataFrame(np.hstack((y.reshape(-1,1), X)), columns = ['y', 'X'])\n",
    "print(df2)\n",
    "\n",
    "sns.regplot(data = df2, x='X', y='y', ci = None) \n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "2. Create a quadratic term of $x$ using `PolynomialFeatures`\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "quadratic = PolynomialFeatures(degree=2)\n",
    "X_quad = quadratic.fit_transform(X)\n",
    "\n",
    "X_quad\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "3. Fit a linear regression and a polynomial regression & plot fitted values vs true data\n",
    "\n",
    "```\n",
    "lr_1 = LinearRegression()\n",
    "lr_2 = LinearRegression()\n",
    "\n",
    "# ----- fit linear features\n",
    "lr_1.fit(X, y)\n",
    "y_lin_fit = lr_1.predict(X)\n",
    "\n",
    "# ----- fit quadratic features\n",
    "lr_2.fit(X_quad, y)\n",
    "y_quad_fit = lr_2.predict(X_quad)\n",
    "\n",
    "# plot fitted values\n",
    "plt.scatter(X, y, label='Training points')\n",
    "plt.plot(X, y_lin_fit, label='Linear fit', linestyle='--')\n",
    "plt.plot(X, y_quad_fit, label='Quadratic fit')\n",
    "\n",
    "plt.xlabel('Explanatory variable')\n",
    "plt.ylabel('Predicted or known target values')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/10_11.png', dpi=300)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "4. Predict $y$ using training dataset for X, compare MSE and $R^2$ across the linear and quadratic regressions\n",
    "\n",
    "```\n",
    "y_lin_pred = lr_1.predict(X)\n",
    "y_quad_pred = lr_2.predict(X_quad)\n",
    "\n",
    "print(f'Training MSE linear: {mean_squared_error(y, y_lin_pred):.3f}, quadratic: {mean_squared_error(y, y_quad_pred):.3f}')\n",
    "print(f'Training R^2 linear: {r2_score(y, y_lin_pred):.3f}, quadratic: {r2_score(y, y_quad_pred):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "### Modeling Nonlinear Relationships in the Housing Dataset\n",
    "\n",
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "<span style='background:orange'>  **Example: Nonlinear Relationships in the Housing Dataset** \n",
    "1. Model the relationship between MEDV (medium house price value) and LSTAT (% of low income population) using 2nd (quadratic) and 3rd (cubic) degree polynomials\n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "X = df[['LSTAT']].values\n",
    "y = df['MEDV'].values\n",
    "\n",
    "regr = LinearRegression()\n",
    "\n",
    "# ---- create quadratic features\n",
    "quadratic = PolynomialFeatures(degree=2)\n",
    "X_quad = quadratic.fit_transform(X)\n",
    "# print(X_quad)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# ---- create cubic features\n",
    "cubic = PolynomialFeatures(degree=3)\n",
    "X_cubic = cubic.fit_transform(X)\n",
    "# print(X_cubic)\n",
    "\n",
    "# create X for plotting\n",
    "X_fit = np.arange(X.min(), X.max(), 1).reshape(-1, 1) # [:, np.newaxis]\n",
    "# X_fit = np.arange(X.min(), X.max(), 1)[:, np.newaxis]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# fit learn regression\n",
    "regr = regr.fit(X, y)\n",
    "y_lin_fit = regr.predict(X_fit)\n",
    "linear_r2 = r2_score(y, regr.predict(X))\n",
    "\n",
    "# fit quadradit regression\n",
    "regr = regr.fit(X_quad, y)\n",
    "y_quad_fit = regr.predict(quadratic.fit_transform(X_fit))\n",
    "quadratic_r2 = r2_score(y, regr.predict(X_quad))\n",
    "\n",
    "# fit cubic regression\n",
    "regr = regr.fit(X_cubic, y)\n",
    "y_cubic_fit = regr.predict(cubic.fit_transform(X_fit))\n",
    "cubic_r2 = r2_score(y, regr.predict(X_cubic))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# plot results\n",
    "plt.scatter(X, y, label='Training points', color='lightgray')\n",
    "\n",
    "plt.plot(X_fit, y_lin_fit, label=f'Linear (d=1), $R^2={linear_r2:.2f}$', color='blue', lw=2, linestyle=':')\n",
    "plt.plot(X_fit, y_quad_fit, label=f'Quadratic (d=2), $R^2={quadratic_r2:.2f}$', color='red', lw=2, linestyle='-')\n",
    "plt.plot(X_fit, y_cubic_fit, label=f'Cubic (d=3), $R^2={cubic_r2:.2f}$', color='green', lw=2, linestyle='--')\n",
    "\n",
    "plt.xlabel('% lower status of the population [LSTAT]')\n",
    "plt.ylabel('Price in $1000s [MEDV]')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# #plt.savefig('images/10_12.png', dpi=300)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dealing with Nonlinear Relationships using Decision Trees and Random Forests {-}\n",
    "\n",
    "- Decision Tree *classifiers* we considered previously can be modified to form decision tree *regressions*  \n",
    "\n",
    "  \n",
    "   \n",
    "- **Decision Tree Regression**  \n",
    "    - Can fit nonlinear features without feature transformation   \n",
    "    - Decision trees analyse one feature at a time  \n",
    "    - Iteratively split nodes until the leaves are pure or a stopping criterion is satisfied   \n",
    "    - In order to measure impurity at node $t$ need an impurity metric that is suitable for continuous variables  \n",
    "        - Use MSE - also known as **within-node variance**  \n",
    "        - When using MSE as an impurity measure the splitting criterion is known as **variance reduction**  \n",
    "    - $I(t)=MSE(t)=\\frac{1}{N_t}\\sum_{i\\in D_t}(y_{i}-\\hat{y}_t)^2$  \n",
    "        - $N_t$ - number of training examples at node $t$  \n",
    "        - $D_t$ - training subset at node $t$  \n",
    "        - $\\hat{y}_t=\\frac{1}{N_t}\\sum_{i\\in D_t}y_{i}$  \n",
    "    - Need to choose an appropriate depth of the tree so as not to overfit or underfit  \n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Random Forest Regression**  \n",
    "    - Ensemble technique combining multiple decision trees  \n",
    "        - Use MSE criterion to grow individual trees  \n",
    "        - Predicted target variable - average prediction over all decision trees  \n",
    "    - Advantages over decision trees  \n",
    "        - Usually better generalization performance (test sample predictions) than an individual decision trees  \n",
    "        - Don't require much parameter tuning  \n",
    "            - Still need to tune number of decision trees  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "<span style='background:orange'>  **Example: Nonlinear Relationships with Decision Trees and Random Forests**   \n",
    "1. Fit a decision tree regression on MEDV (medium house price value) and LSTAT (% of lower status population) and plot fitted values  \n",
    "2. Split the data into 60% train and 40% test datasets  \n",
    "3. Fit a random forest regression on on MEDV (medium house price value) and all available features in the dataset. Print train and test $R^2$ and MSEs.  \n",
    "4. Plot train and test set residuals      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "1. Fit a decision tree regression on MEDV (medium house price value) and LSTAT (% of lower status population) and plot fitted values\n",
    "\n",
    "```\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X = df[['LSTAT']].values\n",
    "y = df['MEDV'].values\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "tree.fit(X, y)\n",
    "\n",
    "\n",
    "plt.scatter(X, y, c='steelblue', edgecolor='white', s=70)\n",
    "\n",
    "sort_idx = X.flatten().argsort()\n",
    "plt.plot(X[sort_idx], tree.predict(X[sort_idx]), c='black', lw=2)\n",
    "\n",
    "plt.xlabel('% lower status of the population [LSTAT]')\n",
    "plt.ylabel('Price in $1000s [MEDV]')\n",
    "#plt.savefig('images/10_14.png', dpi=300)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "2. Split the data into 60% train and 40% test datasets\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "X = df.drop(columns = ['MEDV', 'ones'])\n",
    "y = df['MEDV']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "\n",
    "X_train\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "3. Fit a random forest regression on on MEDV (medium house price value) and all available features in the dataset. Print train and test $R^2$ and MSEs.\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=1000, \n",
    "                               criterion='squared_error', \n",
    "                               random_state=1, \n",
    "                               n_jobs=-1)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = forest.predict(X_train)\n",
    "y_test_pred = forest.predict(X_test)\n",
    "\n",
    "print(f'MSE train: {mean_squared_error(y_train, y_train_pred):.3f}, test: {mean_squared_error(y_test, y_test_pred):.3f}')\n",
    "print(f'R^2 train: {r2_score(y_train, y_train_pred):.3f}, test: {r2_score(y_test, y_test_pred):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "4. Plot train and test set residuals\n",
    "\n",
    "```\n",
    "plt.scatter(y_train_pred, y_train_pred - y_train, c='steelblue', edgecolor='white', marker='o', s=35, alpha=0.9, label='Training data')\n",
    "plt.scatter(y_test_pred, y_test_pred - y_test, c='limegreen', edgecolor='white', marker='s', s=35, alpha=0.9, label='Test data')\n",
    "\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper left')\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color='black')\n",
    "plt.xlim([-10, 50])\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('images/10_15.png', dpi=300)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Other Nonlinear Methods {-}\n",
    "\n",
    "Most classifiers that we have considered so far have their regression counterparts that you can experiment with on your own.\n",
    "\n",
    "- Use Kernel PCA to extract nonlinear factors to be used in a linear regression  \n",
    "- `from sklearn.ensemble import BaggingRegressor` - Bagging Regressor  \n",
    "- `from sklearn.ensemble import AdaBoostRegressor` - AdaBoost Regressor  \n",
    "- `from sklearn.svm import SVR` - Support Vector Regressor  \n",
    "- `from sklearn.neural_network import MLPRegressor` - Multi-layer Perceptron (Neural Network) Regressor  \n",
    "- Etc.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast Combinations - Ensemble Methods\n",
    "\n",
    "In addition to the algorithms metioned above we should also explore combining the forecasts from several regressions via\n",
    "\n",
    "- Voting \n",
    "    - A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. \n",
    "    - Then it averages the individual predictions to form a final prediction.\n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html)\n",
    "- Stacking\n",
    "    - Stacking combines forecasts from different regressions by using the predictions from each base models as a new feature\n",
    "    - The architecture of a stacking model involves two or more base models, often referred to as level-0 models, and a meta-model that combines the predictions of the base models, referred to as a level-1 model.\n",
    "    - Level-0 Models (Base-Models): Models fit on the training data and whose predictions are compiled.\n",
    "    - Level-1 Model (Meta-Model): Model that learns how to best combine the predictions of the base models.\n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html)\n",
    "    \n",
    "    \n",
    "```\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "estimators = [('ridge', RidgeCV()), ('svr', LinearSVR())]\n",
    "               # , \n",
    "               # ('linear_reg', LinearRegression(random_state=1))]\n",
    "\n",
    "stacking_reg = StackingRegressor(estimators=estimators, final_estimator = DecisionTreeRegressor(random_state = 1))\n",
    "\n",
    "stacking_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "y_train_pred_stacking = stacking_reg.predict(X_train_scaled)\n",
    "y_test_pred_stacking = stacking_reg.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "print(f'MSE train: {mean_squared_error(y_train, y_train_pred_stacking):.3f}, test: {mean_squared_error(y_test, y_test_pred_stacking):.3f}')\n",
    "print(f'R^2 train: {r2_score(y_train, y_train_pred):.3f}, test: {r2_score(y_test, y_test_pred):.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Serializing Fitted Scikit-Learn Estimators {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training a machine learning model can be computationally expensive especially if we are doing a cross-validation grid search of hyperparameters\n",
    "    - Don't want to re-train our model every time we close Python interpreter and want to make a new prediction or reload web application\n",
    "- Use built-in `pickle` to *serialize* and *deserialize* Python objects so we can save our trained classifier (don't need to retrain it)\n",
    "    - Serialization is the process of translating a data structure or object state from computer memory into a format that can be stored or transmitted and reconstructed later\n",
    "    - `pickle.dump` - save an object to a file object\n",
    "    - `pickle.load` - load an object from a file object\n",
    "    - Warning: `pickle` can be a **security risk** as it executes code that has been stored in a pickle file. Do not unpickle files from unknown sources\n",
    "\n",
    "\n",
    "\n",
    "- Refit the model as above\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=1000, \n",
    "                               criterion='squared_error', \n",
    "                               random_state=1, \n",
    "                               n_jobs=-1)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = forest.predict(X_train)\n",
    "y_test_pred = forest.predict(X_test)\n",
    "\n",
    "print(f'MSE train: {mean_squared_error(y_train, y_train_pred):.3f}, test: {mean_squared_error(y_test, y_test_pred):.3f}')\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a trained model\n",
    "\n",
    "```\n",
    "import pickle\n",
    "pickle.dump(forest, open(\"forest.pkl\", \"wb\"))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Import a saved models\n",
    "\n",
    "```\n",
    "saved_forest = pickle.load(open(\"forest.pkl\", 'rb'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test the imported model\n",
    "\n",
    "```\n",
    "y_train_pred_saved = saved_forest.predict(X_train)\n",
    "y_test_pred_saved = saved_forest.predict(X_test)\n",
    "\n",
    "print(f'MSE train: {mean_squared_error(y_train, y_train_pred_saved):.3f}, test: {mean_squared_error(y_test, y_test_pred_saved):.3f}')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
