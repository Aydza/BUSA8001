{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Week 8: Combining Different Models for Ensemble Learning   \n",
    "\n",
    "\n",
    "### Unit Convenor & Lecturer    \n",
    "[George Milunovich](https://www.georgemilunovich.com)  \n",
    "[george.milunovich@mq.edu.au](mailto:george.milunovich@mq.edu.au)\n",
    "\n",
    "### References     \n",
    "\n",
    "1. Python Machine Learning 3rd Edition by Raschka & Mirjalili - Chapter 7\n",
    "2. Various open-source material\n",
    "\n",
    "### Overview    \n",
    "\n",
    "\n",
    "- Learning with Ensembles\n",
    "    - Using the Majority Voting Principle to Make Predictions\n",
    "- Bagging â€“ Building an Ensemble of Classifiers from Bootstrap Samples\n",
    "    - Applying bagging to classify examples in the Wine dataset\n",
    "- Adaptive Boosting (AdaBoost) - Leveraging Weak Learners\n",
    "    - Applying AdaBoost to Classify Examples in the Wine Dataset\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with Ensembles   \n",
    "\n",
    "- In predictive analytics, **ensembles** refer to methods that combine multiple predictive models to improve accuracy and reduce the likelihood of an incorrect prediction.\n",
    "\n",
    "<hr style=\"width:35%;margin-left:0;\">\n",
    " \n",
    "\n",
    "<img src=\"images/07_02.png\" alt=\"Drawing\" style=\"width: 400px;\"/>  \n",
    "\n",
    "Ensemble Methods\n",
    "- Combine different classifiers into a meta-classifier that has a better predictive performance than any of the individual classifiers  \n",
    "- **Majority voting** principle  \n",
    "    - Select class label that has been predicted by the majority of classifiers (received at least 50% of votes)  \n",
    "- **Plurality voting** - multi-class settings: select the class that has received the most votes  \n",
    "- Ensemble methods have the ability to improve **bias** and/or **variance** depending on the method used\n",
    "    - Some methods improve bias, some variance, and some both\n",
    "    - These results hold in general and do not guarantee that ensemble methods will improve classification ability in every application\n",
    "\n",
    "\n",
    "<img src=\"images/07_01.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "<hr style=\"width:35%;margin-left:0;\">\n",
    "\n",
    "### Ensemble Learning    \n",
    "\n",
    "1. Start with $m$ classifiers $(C_1, C_2, \\dots,C_m)$  \n",
    "    - E.g. $C_1$ = Decision Tree, $C_2$ = Support Vector Machine, etc.  \n",
    "2. To predict a class label $(\\hat{y})$ via majority (plurality) get predictions from each classifier $C_j$ and combine them  \n",
    "    - $\\hat{y}=\\text{mode}\\left(C_1(x), C_2(x),\\dots,C_m(x)\\right)$  \n",
    "    - Remember: **mode** is the most frequent observation  \n",
    "\n",
    "<hr style=\"width:35%;margin-left:0;\">\n",
    "\n",
    "### Majority Voting in scikit-learn    \n",
    "\n",
    "- In scikit-learn we use `from sklearn.ensemble import VotingClassifier`  \n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)  \n",
    "\n",
    "<hr style=\"width:35%;margin-left:0;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learning with Weights  \n",
    "\n",
    "**Learning with Weights**\n",
    "\n",
    "Sometimes different classification algorithms can be given individual weights for confidence  \n",
    "- Here the weights will be denoted $w_j$ ($j=1,2,\\dots,m$)\n",
    "- The weights should sum up to 1\n",
    "- E.g. $w_1 = 0.3, w_2 = 0.4, w_3 = 0.3 \\rightarrow \\sum_i w_i = 1$\n",
    "- Some algorithms have greater importance in voting than others \n",
    "- The weights could be allocated based on previous evidence, experience, etc\n",
    "\n",
    "**Hard Voting**  \n",
    "- If `voting=hard` (voting parameter set to hard value) use predicted class labels for majority rule voting   \n",
    "- Prediction $\\hat{y}=$ class which has the highest sum of weights across all different classifiers $(C_1, C_2, \\dots,C_m)$    \n",
    "- Example: 3 Classifiers\n",
    "    - Weights given to each classifier: $w_1=0.2, w_2=0.2, w_3=0.6$,   \n",
    "    - Two classes 0 & 1: $C_1\\rightarrow0,C_2\\rightarrow0, C_3\\rightarrow1$\n",
    "    - Sum the weights of the classifiers voting for each class\n",
    "    - $\\sum w_j = 0.4$ (for Class 0)  \n",
    "    - $\\sum w_j = 0.6$ (for Class 1)  \n",
    "    - Sum of the weights of classifiers voting for Class 1 is greater -> $\\hat{y}=1$  \n",
    "  \n",
    "**Soft Voting**  \n",
    "- If `voting=soft` predict the class label which has the greatest weighted average probability - weighted sum of predicted probabilities is greatest  \n",
    "- $\\hat{y}=\\text{arg max}_{i\\in 0,1}\\sum_{j=1}^mw_jp_{ij}$ class for which the sum of $w_jp_{ij}$ across all different classifiers $(C_1, C_2, \\dots,C_m)$ is greatest  \n",
    "- Example: 3 Classifiers\n",
    "    - $w_1=0.2, w_2=0.2, w_3=0.6$,   \n",
    "    - Classes 0 & 1 for which the predicted probabilities are as follows: $C_1\\rightarrow[0.9, 0.1],C_2\\rightarrow[0.8,0.2], C_3\\rightarrow[0.4,0.6]$  \n",
    "    - weighted average $p(\\text{class 0}|x)=0.2\\times0.9 + 0.2\\times0.8 + 0.6\\times0.4=0.58$  \n",
    "    - weighted average $p(\\text{class 1}|x)=0.2\\times0.1 + 0.2\\times0.2 + 0.6\\times0.6=0.42$  \n",
    "    - Prediction $\\hat{y}=\\text{arg max}_{i\\in 0,1}[p(\\text{class 0}|x),p(\\text{class 1}|x)] =0$  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "## Using the Majority Voting Principle to Make Predictions {-}\n",
    "\n",
    "<span style='background:orange'>  **------------------ Exercise 1 ------------------**  \n",
    "1. Import Iris dataset from sklearn's datasets\n",
    "2. Compute accuracy for Perceptron, SVClassifier and DecisionTreeClassifier using a 10-fold cross-validation\n",
    "3. Combine the three classifiers into a VotingClassifier with hard voting and compute its accuracy via cross-validation on training data\n",
    "4. Plot decision regions using training data  \n",
    "5. Print all parameters of voting_classifier\n",
    "6. Fine tune some parameters of the VotingClassifier  \n",
    "7. Print best_params_ and best_score_ of VotingClassifier\n",
    "8. Export the optimized VotingClassifier as final_mv_classifier using best_estimator_\n",
    "9. Export the optimized VotingClassifier as final_mv_classifier using `best_estimator_`  \n",
    "- Make a prediction of y_test\n",
    "- Compute accuracy for y_test\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "1. Import Iris dataset from sklearn's datasets\n",
    "    - Choose only iris-versicolor & iris-virginica labals -> rows 50 - end\n",
    "    - Set X as sepal width and petal length -> columns 1 & 2\n",
    "    - Generate training and test (30%) datasets\n",
    "\n",
    "\n",
    "```\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X, y = iris.data[50:, [1, 2]], iris.target[50:]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "       train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "\n",
    "y_train\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "2. Compute accuracy for Perceptron, SVClassifier and DecisionTreeClassifier using a 10-fold cross-validation\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "clf1 = Perceptron()\n",
    "pipe1 = Pipeline([['sc', StandardScaler()],\n",
    "                  ['clf', clf1]])\n",
    "\n",
    "clf2 = SVC(kernel='poly', probability=True)\n",
    "pipe2 = Pipeline([['sc', StandardScaler()],\n",
    "                  ['clf', clf2]])\n",
    "\n",
    "clf3 = DecisionTreeClassifier()\n",
    "\n",
    "clf_labels = ['Perceptron', 'Support Vector Classifier', 'Decision Tree']\n",
    "\n",
    "print('10-fold cross validation:\\n')\n",
    "for clf, label in zip([pipe1, pipe2, clf3], clf_labels):\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=10,\n",
    "                             scoring='accuracy')\n",
    "    print(f'Accuracy:{scores.mean():.3f} (+/- {scores.std():.3f})', label)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "3. Combine the three classifiers into a VotingClassifier with hard voting and compute its accuracy via cross-validation on training data\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('perceptron_pipe', pipe1), ('supportvector_pipe', pipe2), ('Decision Tree', clf3)], voting='hard')\n",
    "\n",
    "\n",
    "clf_labels += ['Majority voting']\n",
    "\n",
    "# print(clf_labels)\n",
    "\n",
    "all_clf = [pipe1, pipe2, clf3, voting_clf]\n",
    "\n",
    "for clf, label in zip(all_clf, clf_labels):\n",
    "\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=10,\n",
    "                             scoring='accuracy')\n",
    "                             # scoring='roc_auc')\n",
    "\n",
    "\n",
    "    print(f'Accuracy: {scores.mean():.3f} +/- {scores.std():.3f}, {label}')\n",
    "    \n",
    "# print(clf, label)\n",
    "# print(clf.get_params())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "4. Plot decision regions using training data\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "x_min = X_train_std[:, 0].min() - 1\n",
    "x_max = X_train_std[:, 0].max() + 1\n",
    "y_min = X_train_std[:, 1].min() - 1\n",
    "y_max = X_train_std[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(nrows=2, ncols=2, \n",
    "                        sharex='col', \n",
    "                        sharey='row', \n",
    "                        figsize=(7, 5))\n",
    "\n",
    "for idx, clf, tt in zip(product([0, 1], [0, 1]),\n",
    "                        all_clf, clf_labels):\n",
    "    clf.fit(X_train_std, y_train)\n",
    "    \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.3)\n",
    "    \n",
    "    axarr[idx[0], idx[1]].scatter(X_train_std[y_train==0, 0], \n",
    "                                  X_train_std[y_train==0, 1], \n",
    "                                  c='blue', \n",
    "                                  marker='^',\n",
    "                                  s=50)\n",
    "    \n",
    "    axarr[idx[0], idx[1]].scatter(X_train_std[y_train==1, 0], \n",
    "                                  X_train_std[y_train==1, 1], \n",
    "                                  c='green', \n",
    "                                  marker='o',\n",
    "                                  s=50)\n",
    "    \n",
    "    axarr[idx[0], idx[1]].set_title(tt)\n",
    "\n",
    "plt.text(-3.5, -5., \n",
    "         s='Sepal width [standardized]', \n",
    "         ha='center', va='center', fontsize=12)\n",
    "plt.text(-12.5, 4.5, \n",
    "         s='Petal length [standardized]', \n",
    "         ha='center', va='center', \n",
    "         fontsize=12, rotation=90)\n",
    "\n",
    "#plt.savefig('images/07_05', dpi=300)\n",
    "plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Print all parameters of `voting_classifier`\n",
    "\n",
    "```\n",
    "voting_clf.get_params()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">  \n",
    "\n",
    "6. **Fine tune the following hyperparameters of the VotingClassifier**\n",
    "- `Decision_Tree2__max_depth` \n",
    "- `supportvector_pipe__clf__C`  \n",
    "- `perceptron_pipe__clf__l1_ratio` \n",
    "\n",
    "**Note**  \n",
    "By default, the default setting for `refit` in `GridSearchCV` is `True` (i.e., `GridSeachCV(..., refit=True)`), which means that we can use the fitted `GridSearchCV` estimator to make predictions via the `predict` method.\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'Decision Tree__max_depth': [1, 2, 3, 5, 10],\n",
    "          'supportvector_pipe__clf__C': [0.001, 0.1, 10, 100.0],\n",
    "          'perceptron_pipe__clf__l1_ratio': [0.01, 0.15, 0.5, 0.75, 0.99]}\n",
    "\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=voting_clf,\n",
    "                    param_grid=params,\n",
    "                    cv=10,\n",
    "                    scoring='accuracy')\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# for each combination of parameters do cross validation across 10 folds and print result\n",
    "for i in range(len(grid.cv_results_['mean_test_score'])):\n",
    "    print(f\"{grid.cv_results_['mean_test_score'][i]:.4} +/- {grid.cv_results_['std_test_score'][i]:.2f}, {grid.cv_results_['params'][i]}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "7. Print `best_params_` and `best_score_` of VotingClassifier\n",
    "\n",
    "```\n",
    "print(f'Best parameters: {grid.best_params_}')\n",
    "print(f'Accuracy: {grid.best_score_:.3f}')\n",
    "```\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "8. Export the optimized VotingClassifier as final_mv_classifier using `best_estimator_`\n",
    "- Make a prediction of `y_test`\n",
    "- Compute accuracy for `y_test`\n",
    "\n",
    "```\n",
    "final_voting_classifier = grid.best_estimator_\n",
    "\n",
    "print('Predictions of y_test:', final_voting_classifier.predict(X_test))\n",
    "print('Accuracy on test data:', final_voting_classifier.score(X_test, y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bagging - Building an Ensemble of Classifiers from Bootstrap Samples {-}\n",
    "\n",
    "- In Bagging (*bootstrap aggregating*) we still create ensembles of classifiers but **do not use the same training dataset** for each classifier \n",
    "- Create bootstrap samples (random samples with replacement) from the initial dataset\n",
    "- For each bootstrap sample, a separate model is trained.\n",
    "    - These models are of the **same type** but are trained independently of each other.\n",
    "    - Typically use **unpruned decision trees** as base classifiers\n",
    "    - Because the data in each bootstrap sample varies, the resulting models are different, capturing different patterns from the training data.\n",
    "- Reduces Overfitting: Bagging can reduce the risk of overfitting to the training data without significantly increasing bias.\n",
    "\n",
    "\n",
    "<img src=\"images/07_06.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "## Bagging Example {-}\n",
    "\n",
    "- Say we have only 7 training examples\n",
    "    - Each round of bagging will sample randomly **with replacement** from the 7 instances\n",
    "        - Random samples are denoted as **Bagging rounds**\n",
    "    - Each bootstrap sample is used to fit a classifier $C_j$ which is typically **an unpruned decision tree**\n",
    "    - Once the individual classifiers are fitted to the bootstrap samples, the predictions are combined using majority voting\n",
    "    \n",
    "\n",
    "<img src=\"images/07_07.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "### Bagging in scikit-learn {-}\n",
    "- In scikit-learn use `from sklearn.ensemble import BaggingClassifier`\n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\n",
    "    - Will need to specify `base_estimator` parameter which is usually a DecisionTree as well as `n_estimators`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "## Applying Bagging to Classify Examples in the Wine Dataset {-}\n",
    "\n",
    "<span style='background:orange'>  **------------------ Exercise 2 ------------------**  \n",
    " \n",
    "1. Import Wine dataset\n",
    "    - Set y = 'Class label' column\n",
    "    - Set X = 'Alcohol' and 'OD280/OD315 of diluted wines' columns\n",
    "2. Encode class labels into binary format & split data into training and test (20%) datasets\n",
    "3. Initialize `BaggingClassifier` with 500 Decision Trees, use `entropy` as criterion\n",
    "4. Compute `accuracy_score` on the training and test datasets for both single DecisionTree and BaggingClassifier \n",
    "5. Plot decision regions and the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "1. Import Wine dataset\n",
    "    - Set y = 'Class label' column\n",
    "    - Set X = 'Alcohol' and 'OD280/OD315 of diluted wines' columns\n",
    "    \n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data')\n",
    "\n",
    "df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n",
    "                   'Proline']\n",
    "\n",
    "\n",
    "# if the Wine dataset is temporarily unavailable from the\n",
    "# UCI machine learning repository, un-comment the following line\n",
    "# of code to load the dataset from a local path:\n",
    "\n",
    "# df_wine = pd.read_csv('wine.data', header=None)\n",
    "\n",
    "# ---- drop 1 class ------\n",
    "\n",
    "df_wine = df_wine[df_wine['Class label'] != 1]  # currently 3 labels: 1, 2, 3. Drop label 1\n",
    "\n",
    "y = df_wine['Class label'].values\n",
    "X = df_wine[['Alcohol', 'OD280/OD315 of diluted wines']].values\n",
    "\n",
    "y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "2. Encode class labels into binary format & split data into training and test (20%) datasets\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "3. Initialize `BaggingClassifier` with 500 Decision Trees, use `entropy` as criterion\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', \n",
    "                              max_depth=None,\n",
    "                              random_state=1)\n",
    "\n",
    "bag = BaggingClassifier(base_estimator=tree,\n",
    "                        n_estimators=500, \n",
    "                        n_jobs=1, \n",
    "                        random_state=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "4. Compute `accuracy_score` on the training and test datasets for both single DecisionTree and BaggingClassifier \n",
    "\n",
    "```\n",
    "tree = tree.fit(X_train, y_train)\n",
    "\n",
    "tree_train = tree.score(X_train, y_train) \n",
    "tree_test = tree.score(X_test, y_test)\n",
    "print(f'Decision tree train/test accuracies {tree_train:.3f}/{tree_test:.3f}')\n",
    "\n",
    "bag = bag.fit(X_train, y_train)\n",
    "bag_train = bag.score(X_train, y_train) \n",
    "bag_test = bag.score(X_test, y_test) \n",
    "print(f'Bagging train/test accuracies {bag_train:.3f}/{bag_test:.3f}')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "5. Plot decision regions and the training dataset\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_min = X_train[:, 0].min() - 1\n",
    "x_max = X_train[:, 0].max() + 1\n",
    "y_min = X_train[:, 1].min() - 1\n",
    "y_max = X_train[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(nrows=1, ncols=2, \n",
    "                        sharex='col', \n",
    "                        sharey='row', \n",
    "                        figsize=(8, 3))\n",
    "\n",
    "\n",
    "for idx, clf, tt in zip([0, 1],\n",
    "                        [tree, bag],\n",
    "                        ['Decision tree', 'Bagging']):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n",
    "    axarr[idx].scatter(X_train[y_train == 0, 0],\n",
    "                       X_train[y_train == 0, 1],\n",
    "                       c='blue', marker='^')\n",
    "\n",
    "    axarr[idx].scatter(X_train[y_train == 1, 0],\n",
    "                       X_train[y_train == 1, 1],\n",
    "                       c='green', marker='o')\n",
    "\n",
    "    axarr[idx].set_title(tt)\n",
    "\n",
    "axarr[0].set_ylabel('Alcohol', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.text(0, -0.2,\n",
    "         s='OD280/OD315 of diluted wines',\n",
    "         ha='center',\n",
    "         va='center',\n",
    "         fontsize=12,\n",
    "         transform=axarr[1].transAxes)\n",
    "\n",
    "#plt.savefig('images/07_08.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to see what's inside an object**\n",
    "\n",
    "- `dir()` function\n",
    "    - Without arguments, returns the list of names in the current local scope. \n",
    "    - With an argument, attempt to return a list of valid attributes for that object.\n",
    "\n",
    "```\n",
    "dir(tree)\n",
    "\n",
    "tree.get_depth()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "# Adaptive Boosting (AdaBoost) - Leveraging Weak Learners {-}\n",
    "\n",
    "Boosting is an ensemble technique that works by sequentially combining multiple weak learners (models that are only slightly better than random guessing) into a strong learner. \n",
    "\n",
    "The core idea behind boosting is to correct the mistakes of previous learners in the sequence by giving more weight to the training instances that were misclassified, thereby focusing subsequent models on the harder cases.\n",
    "\n",
    "**Boosting**: Focus on training examples that are hard to classify  \n",
    "- Boosting is said to decrease bias when compared to bagging  \n",
    "- In practice boosting models tend to overfit (high variance)  \n",
    "- Ensemble (the set of classifiers) consists of very simple classifiers (weak learners)    \n",
    "    - E.g. **decision tree stumps** (one level decision trees)  \n",
    "- Let weak learners learn from misclassified training examples to improve the performance of the ensemble  \n",
    "\n",
    "\n",
    "**Sensitivity to Noisy Data**: Because AdaBoost focuses on instances that are hard to classify, it can be sensitive to noise and outliers, as these can receive disproportionately high weights.\n",
    "\n",
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    " \n",
    "\n",
    "\n",
    "## AdaBoost Method {-}\n",
    "\n",
    "- AdaBoost (a special type of boosting) will use complete training dataset to train weak learners  \n",
    "    - Reweight training examples in each iteration to build a strong classifier that learns from the mistakes of the previous weak learners in the ensemble  \n",
    "\n",
    "1. Fig. 1 - train a decision stump to classify two classes  \n",
    "    - Weak learner misclassifies two examples (circles)  \n",
    "2. Fig. 2 - misclassified circles from above are given more weight while every other example given lower weight  \n",
    "    - Fit decision stump 2 to this data -> more focused on the examples which are hard to classify  \n",
    "    - Weak learner 2 misclassifies 3 different examples (circles)  \n",
    "3. Fig. 3 - misclassified examples from Fig 2 given even greater weight  \n",
    "    - Decision stump 3 fitted to this data  \n",
    "4. Continue in this fashion until a desired number of boosting rounds is reached  \n",
    "5. Fig. 4 - combine weak learners trained on different reweighted training subsets by a weighted majority vote  \n",
    "    - Assumes 3 rounds of boosting  \n",
    "    \n",
    "<img src=\"images/07_09.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "## Applying AdaBoost using scikit-learn {-} \n",
    "- [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
    "- Use `from sklearn.ensemble import AdaBoostClassifier`  \n",
    "\n",
    "- Need to specify parameters  \n",
    "    - `estimator` - The base estimator from which the boosted ensemble is built, usually set to **decision tree**  \n",
    "    - `n_estimators` - The maximum number of estimators at which boosting is terminated  \n",
    "    - `learning_rate` - Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "## Applying AdaBoost to Classify Examples in the Wine Dataset {-}\n",
    "\n",
    "<span style='background:orange'>  **------------------ Exercise 3 ------------------**  \n",
    " \n",
    "1. Initialize an AdaBoostClassifier  \n",
    "    - base_estimator = DecisionTree  \n",
    "    - n_estimators = 500  \n",
    "    - learning_rate = 0.1  \n",
    "    \n",
    "2. Fit and compute accuracy_score on the training and test datasets for both single DecisionTree and AdaBoostClassifier   \n",
    "3. Plot decision regions and the training dataset  \n",
    "\n",
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "    \n",
    "```\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', \n",
    "                              max_depth=1,\n",
    "                              random_state=1)\n",
    "\n",
    "ada = AdaBoostClassifier(estimator=tree,\n",
    "                         n_estimators=500, \n",
    "                         learning_rate=0.1,\n",
    "                         random_state=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "2. Fit and compute accuracy_score on the training and test datasets for both single DecisionTree and AdaBoostClassifier \n",
    "\n",
    "```\n",
    "tree = tree.fit(X_train, y_train)\n",
    "\n",
    "tree_train_accuracy = tree.score(X_train, y_train)\n",
    "tree_test_accuracy = tree.score(X_test, y_test)\n",
    "\n",
    "print(f'Decision tree train/test accuracies {tree_train_accuracy:.3f}/{tree_test_accuracy:.3f}')\n",
    "\n",
    "ada = ada.fit(X_train, y_train)\n",
    "\n",
    "ada_train_accuracy = ada.score(X_train, y_train)\n",
    "ada_test_accuracy = ada.score(X_test, y_test)\n",
    "\n",
    "print(f'AdaBoost train/test accuracies {ada_train_accuracy:.3f}/{ada_test_accuracy:.3f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "3. Plot decision regions and the training dataset\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(8, 3))\n",
    "\n",
    "\n",
    "for idx, clf, tt in zip([0, 1],\n",
    "                        [tree, ada],\n",
    "                        ['Decision tree', 'AdaBoost']):\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n",
    "    axarr[idx].scatter(X_train[y_train == 0, 0],\n",
    "                       X_train[y_train == 0, 1],\n",
    "                       c='blue', marker='^')\n",
    "    axarr[idx].scatter(X_train[y_train == 1, 0],\n",
    "                       X_train[y_train == 1, 1],\n",
    "                       c='green', marker='o')\n",
    "    axarr[idx].set_title(tt)\n",
    "\n",
    "axarr[0].set_ylabel('Alcohol', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.text(0, -0.2,\n",
    "         s='OD280/OD315 of diluted wines',\n",
    "         ha='center',\n",
    "         va='center',\n",
    "         fontsize=12,\n",
    "         transform=axarr[1].transAxes)\n",
    "\n",
    "#plt.savefig('images/07_11.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
