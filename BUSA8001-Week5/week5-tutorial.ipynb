{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f76d664-d35b-452d-8faa-0ce85012aa29",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Week 5 Computer Lab/Tutorial {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69655379-80bd-47b2-8768-b8759348ad88",
   "metadata": {},
   "source": [
    "---\n",
    "1. Sample Short-Answer Questions (5 min)\n",
    "2. Practice Quiz (15 min)\n",
    "3. Python Exercises\n",
    "4.  Discuss Programming Assignment 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7623e-e76f-47c3-875a-d53b45f487a4",
   "metadata": {},
   "source": [
    "Continue with credit score cards analysis from Week 4 tutorial\n",
    "\n",
    "\n",
    "\n",
    "Credit score cards are used as a risk control method in the financial industry. Personal information submitted by credit card applicants are used to predict the probability of future defaults. The bank employs such data to decide whether to issue a credit card to the applicant or not.\n",
    "\n",
    "\n",
    "| Feature Name         | Explanation     | Additional Remarks |\n",
    "|--------------|-----------|-----------|\n",
    "| ID | Randomly allocated client number      |         |\n",
    "| Income   | Annual income  |  |\n",
    "| Gender   | Applicant's Gender   | Male = 0, Female = 1  |\n",
    "| Car | Car Ownership | Yes = 1, No = 0 | \n",
    "| Children | Number of Children | |\n",
    "| Real Estate | Real Estate Ownership | Yes = 1, No = 0 \n",
    "| Days Since Birth | No. of Days | Count backwards from current day (0), -1 means yesterday\n",
    "| Days Employed | No. of Days | Count backwards from current day(0). If positive, it means the person is currently unemployed.\n",
    "| Payment Default | Whether a client has overdue credit card payments | Yes = 1, No = 0\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1d5c65-a222-46cf-8f58-94672a5a58a3",
   "metadata": {},
   "source": [
    "Before we continue with the analysis of the credit data, we need to execute the code we ran last week in order to get the data in the right format\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_excel('data/credit_data.xlsx')\n",
    "\n",
    "df.drop_duplicates(subset= ['ID'], inplace = True)\n",
    "\n",
    "df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "df.loc[df['Days Employed'] > 0, 'Days Employed'] = 0\n",
    "\n",
    "df.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b58777b-5b4a-4923-b350-a068aefd4fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6160bde8-56ad-4ef6-857a-4122cba59a1a",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 1**\n",
    "\n",
    "Create two new variables in `df` named \n",
    "\n",
    "1. `Age`;\n",
    "2. `Years in Employment`,\n",
    "\n",
    "which measure age and employment length in **years** (positive decimal numbers) from `Days Since Birth` and `Days Employed` by applying approapriate transformations on these variables. \n",
    "\n",
    "Delete the original variables `Days Since Birth` and `Days Employed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b39999e-9de0-4427-a096-aeb94a660c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b806fd-f4a1-4036-8289-f8225b852fd7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "- Create a **one**-dimensional NumPy array named `y` by exporting the first 5,000 observations of `Payment Default`. (Hint: see `ravel()` function)\n",
    "- Create a NumPy array named `X` by exporting the first 5,000 observations of the following columns `Gender`, `Car`, `Real Estate`, `Children`, `Income`, `Age`, `Years in Employment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601bcd88-aa9e-4420-ab2a-50b8aeb585d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffafd5b-31a1-46e1-8495-0a0699f4ada4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 3** \n",
    "\n",
    "- Use an appropriate `scikit-learn` library we learned in class to create the following NumPy arrays: `y_train`, `y_test`, `X_train` and `X_test` by splitting the data into 70% training and 30% test datasets. \n",
    "- Set `random_state` to 0 and stratify subsamples so that train and test datasets have roughly equal proportions of the target's class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfffd5b-6e9c-417a-b666-97efbfbee7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce8385-1e9c-4952-86f2-c33c5373c8dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "- Create new variables by using an appropriate `scikit-learn` library we learned in class to standardize the features from the training and test datasets to mean zero and variance one. Name the new variables by appending '_scaled' to the original variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850ee132-4c2a-48b3-992b-d6e1a1eaa030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8071322c-610d-4f9c-bae2-52f2a8f0f9a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 5**\n",
    "\n",
    "Fit the following two classifiers to the transformed training dataset using `scikit-learn` libraries.\n",
    "\n",
    "- Perceptron - name your instance `pc` set `random_state=7`\n",
    "- Logistic Regression - name your instance `lr` set `random_state=7`\n",
    "\n",
    "When initializing instances of the above classifiers only set the parameters referenced above and nothing else.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de4d13f-142d-4c50-961d-b178630b6c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f9d49-0af7-4f59-9e54-f635839aa4dc",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "- Using a method built into each of the two classifiers compute their prediction accuracies on both training and test data;\n",
    "\n",
    "- Store the accuracy values into variables named according to the following pattern: `classifier_name_accuracy_train`, e.g. you should have `lr_accuracy_train`; \n",
    "- Store the accuracy values into variables named according to the following pattern: `classifier_name_accuracy_test`, e.g. you should have `lr_accuracy_test`.\n",
    "  \n",
    "- Print the accuracy **variables** along with their brief descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa3e386-091f-47cc-aee3-d9dafc318153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4ea38-afb3-4ef3-9718-47db5f4e9ba6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 7** \n",
    "\n",
    "Using nicely formated text in Markdown comment on the accuracies computed in Question 6 making sure you address:\n",
    "1. Training vs test set datasets for each classifier; (5 marks)\n",
    "2. Perceptrion vs Logistic Regression. (5 marks) \n",
    "3. Are the results as expected, and why or why not? (Hint: You are not expected to comment on why a particular model is better. 10 marks) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d860bdb-e93f-4152-b075-77c4770b30f3",
   "metadata": {},
   "source": [
    "--- add your text answer here\n",
    "\n",
    "\n",
    "1. **Training vs test datasets**: \n",
    "\n",
    "2. **Perceptron vs Logistic Regression**: \n",
    "\n",
    "3. **Are the results as expected**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2328010f-f40e-4d5a-be7f-24b47fae7c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2651f9-1ef9-4799-a538-cda90a099172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b04652e-8940-428f-bd25-f67f6de2f346",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Solutions\n",
    "\n",
    "\n",
    "**Question 1**\n",
    "  \n",
    "```\n",
    "df['Age']= -df['Days Since Birth']/365\n",
    "df['Years in Employment']= -df['Days Employed']/365\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "del df['Days Since Birth']\n",
    "del df['Days Employed']\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Question 2**\n",
    "  \n",
    "```\n",
    "y = df.loc[:4999, ['Payment Default']].values.ravel()\n",
    "X = df.loc[:4999, [ 'Gender', 'Car', 'Real Estate', 'Children', 'Income', 'Age', 'Years in Employment']].values\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Question 3**\n",
    "  \n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0, stratify = y)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_scaled = sc.transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Question 5**\n",
    "\n",
    "```\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "pc = Perceptron(random_state=7)\n",
    "pc.fit(X_train_scaled, y_train)\n",
    "\n",
    "lr = LogisticRegression(random_state=7)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Question 6**\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "lr_accuracy_train = lr.score(X_train_scaled, y_train)\n",
    "lr_accuracy_test = lr.score(X_test_scaled, y_test)\n",
    "\n",
    "pc_accuracy_train = pc.score(X_train_scaled, y_train)\n",
    "pc_accuracy_test = pc.score(X_test_scaled, y_test)\n",
    "\n",
    "\n",
    "print(f'Accuracy Training Dataset: Perceptron = {pc_accuracy_train:.3f}')\n",
    "print(f'Accuracy Training Dataset: Logistic Regresssion = {lr_accuracy_train:.3f}')\n",
    "\n",
    "\n",
    "print(f'Accuracy Test Dataset - Perceptron = {pc_accuracy_test:.3f}')\n",
    "print(f'Accuracy Test Dataset - Logistic Regresssion = {lr_accuracy_test:.3f}')\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Question 7**\n",
    "\n",
    "```\n",
    "\n",
    "1) The training and test datasets used in fitting Perceptron and Logistic Regression are the same. Considering perceptron output we observe the training dataset accuracy of 0.520 compared with the test accuracy of 0.503. Logistic regression training accuracy is 0.609 while the test accuracy is 0.617.\n",
    "\n",
    "2) Comparing Perceptron vs Logistic Regression based on the computed accuracies we oberve that Logistic Regression generates predictions of higher accuracy, both for the training and test datasets. Thus, Logistic Regression is the preferred model for our dataset.\n",
    "\n",
    "\n",
    "3) Since we are not required to comment on why Logistic Regression is better I will comment on the following 2 points\n",
    "\n",
    "- Considering accuracy measures for Perceptron across the training and test datasets, we see that the training accuracy is somewhat higher, which is expected since the model is optimised for the training dataset. Regarding Logistic Regression the test accuracy is higher than the training accuracy and while this may appear unusual the difference is rather small and can be attributed to random variation in the datasets. Furthermore, in both the Perceptron and Logistic Regression cases the differences between training and test accuracies are small enough to indicate that overfitting is not an issue with our models.\n",
    "    \n",
    "\n",
    "- The ranking for the models is consistent across the training and test datasets (Logistic Regression ourperforms) which gives us confidence that the best model (chosen based on the training accuracy) is correctly selected and continues to be preferred on new data. This is expected in a good model section procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc622f3-9cf7-4193-9b43-daadeb178a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
