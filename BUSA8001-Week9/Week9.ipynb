{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Working with Unlabeled Data: Clustering Analysis  \n",
    "\n",
    "\n",
    "### Unit Convenor & Lecturer {-}\n",
    "[George Milunovich](https://www.georgemilunovich.com)  \n",
    "[george.milunovich@mq.edu.au](mailto:george.milunovich@mq.edu.au)\n",
    "\n",
    "### References {-}\n",
    "\n",
    "1. Python Machine Learning 3rd Edition by Raschka & Mirjalili - Chapter 11  \n",
    "2. Various open-source material  \n",
    "\n",
    "### Overview {-}\n",
    "\n",
    "- Grouping objects by similarity using k-means  \n",
    "  - K-means clustering using scikit-learn  \n",
    "  - A smarter way of placing the initial cluster centroids using k-means++  \n",
    "  - Using the elbow method and silhouette plots to find the optimal number of clusters  \n",
    "- Organizing clusters as a hierarchical tree  \n",
    "  - Grouping clusters in bottom-up fashion  \n",
    "  - Performing hierarchical clustering on a distance matrix  \n",
    "  - Attaching dendrograms to a heat map  \n",
    "  - Applying agglomerative clustering via scikit-learn  \n",
    "- Locating regions of high density via DBSCAN  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Working with Unlabeled Data\n",
    "\n",
    "Working with unlabeled data presents a unique set of challenges and opportunities, notably through clustering analysis.\n",
    "\n",
    "- **Clustering** is a technique that organises data into clusters or groups based on similarity without prior knowledge of group assignments.\n",
    "    - Similar to sorting different objects into distinct categories based solely on their features, without a predefined list of object types.\n",
    "- The process involves algorithms like **K-means** or **hierarchical clustering** to detect patterns and relationships that aren't immediately apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering is a type of unsupervised learning** \n",
    "\n",
    "\n",
    "- So far we have considered supervised learning techniques to build machine learning models where the **target variable is known**  \n",
    "- In clustering:\n",
    "  - We don't know what the target variable is  \n",
    "  - We can find some commonalities/patterns in the data\n",
    "  \n",
    "**Objective: Find groupings in the data so that items in the same cluster are more similar to each other than to items from different clusters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cluster Analysis in Business**\n",
    "\n",
    "Clustering is particularly valuable in business applications. Some examples include the following:\n",
    "\n",
    "- **Customer Segmentation**: Businesses use clustering to segment customers based on demographics, purchasing behaviors, preferences, and responsiveness to marketing. This helps in tailoring marketing strategies, improving customer engagement, and enhancing service delivery.\n",
    "- **Market Segmentation**: Clustering helps identify different market segments, allowing companies to target specific clusters with customised products or services. This approach can increase market penetration and optimise investments.\n",
    "- **Inventory Management**: Clustering algorithms can categorise inventory based on movement velocity, value, and other characteristics. This enables more efficient inventory control and optimisation of the supply chain.\n",
    "- **Product Recommendation Systems**: E-commerce platforms use clustering to understand customer preferences and behavior, which helps in recommending products that customers are more likely to purchase.\n",
    "- **Risk Management**: In insurance and finance, clustering helps in identifying risk profiles and categorising customers into risk groups, aiding in setting premiums and coverage policies.\n",
    "- **Human Resources**: HR departments apply clustering to group employees based on skills, performance, and behavior. This can help in talent management, team formation, and identifying training needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "--- \n",
    "## Grouping Objects by Similarity Using k-Means \n",
    "\n",
    "\n",
    "**k-Means** method  \n",
    "- Category of **prototype-based clustering**  \n",
    "- Each cluster is represented by a prototype:  \n",
    "    - **Centroid** (average) in the case of continuous features  \n",
    "    - **Medoid** (the most representative point) in the case of categorical features  \n",
    "- We need to specify the number of clusters $k$ *a priori* (before the analysis)   \n",
    "    - When dealing with more than 3 dimensions it becomes impossible to visualise data effectively  \n",
    "    - This is a significant *limitation* of the method as it can be difficult to choose the right $k$\n",
    "    - Inappropriate choice of $k$ can result in a poor clustering performance  \n",
    "        - The elbow method and silhouette plots can be used to evaluate the quality of clustering and **determine $k$**  \n",
    "    - One or more clusters can potentially be empty  \n",
    "        - Although scikit-learn implements a solution for this  \n",
    "- Clusters cannot be hierarchical (inside one another)       \n",
    "    \n",
    "**k-Means Algorithm**  \n",
    "1. Randomly pick $k$ points as initial cluster centres $\\mu^{(j)}, j\\in{1,2,\\dots,k}$   \n",
    "2. Assign each example to the nearest centroid $\\mu^{(j)}$    \n",
    "3. Move centroids/medoids to the center of the examples that were assigned to it  \n",
    "4. Repeat steps 2 and 3 until the centroids/medoids don't change and examples don't move between clusters  \n",
    "    - Or a user-defined tolerance or maximum number of iterations is reached (if no convergence)  \n",
    "- Watch [https://youtu.be/RD0nNK51Fp8](https://youtu.be/RD0nNK51Fp8) for a good explanation of this algorithm  \n",
    "\n",
    "<img src=\"images/11_17.png\" alt=\"Drawing\" style=\"width: 600px;\"/>  \n",
    "\n",
    "\n",
    "**Measuring Distance**  \n",
    "- Measuring distance in one dimensional data is easy -> we use the absolute value  \n",
    "    - E.g. what is the distance between 3 and -2?  \n",
    "        - $|3 - (-2)|=|-2-3|=5$  \n",
    "- However what if we multi-dimensional data? Consider the following 2-d case, where we have 2 variables:  \n",
    "    - E.g. $\\mathbf{x}=\\left(\\begin{array}{c} \n",
    "\\text{Sale Amount}\\\\\n",
    "\\text{Customer Income}\n",
    "\\end{array}\\right)$\n",
    "\n",
    "  \n",
    "- To measure the distance between two m-dimensional vectors $x$ and $y$ we typically use **squared Euclidean distance**  \n",
    "    - $d(x, y)^2=\\sum_{j=1}^m(x_j-y_j)^2=||x-y||_2^2$  \n",
    "    - Note that this produces the square of the hypotenuse of a right-angle triangle when $y$ (or $x$) is a vector of zeros and $x$ (or $y$) contains elements $x_1$ and $x_2$ which are the coordinates of the triangle   \n",
    "        - E.g. If $y=[0 \\quad 0 \\quad 0], x=[3 \\quad 4]$ then $d(x,y)^2=[3-0]^2 + [4-0]^2 = 25$  \n",
    "\n",
    "  \n",
    "- Using the Euclidean distance we can define the k-means algorithms as an optimization problem  \n",
    "    - Minimize within-cluster sum of squared errors (SSE) $SSE=\\sum_i^n\\sum_j^kw^{(i,j)}||x^{(i)}-\\mu^{(j)}||_2^2$ where  \n",
    "        - $w^{(i,j)}=\\left\\{ \n",
    "\\begin{array}{cc}\n",
    "1 & \\text{if }x{(i)}\\in j \\\\ \n",
    "0 & \\text{otherwise} \\hfill\n",
    "\\end{array}\n",
    "\\right.$\n",
    "  \n",
    "\n",
    "In scikit-learn use `from sklearn.cluster import KMeans`  \n",
    "    - [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "<span style='background:orange'>  **Example: k-Means clustering**    \n",
    "    \n",
    "1. Import `make_blobs` data from sklearn.datasets and plot it in a scatterplot\n",
    "    - 150 randomly generated points in 2-d that are grouped into 3 regions ($k$) with higher density\n",
    "2. Use `KMeans` from `sklearn` to classify data into 3 clusters\n",
    "3. Plot the clusters and classified data in a scatterplot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "1. Import `make_blobs` data from sklearn.datasets and plot it in a scatterplot\n",
    "- 150 randomly generated points in 2-d that are grouped into 3 regions ($k$) with higher density\n",
    "\n",
    "```\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_blobs # create dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_blobs(n_samples=150, \n",
    "                  n_features=2, \n",
    "                  # n_features=3,\n",
    "                  centers=3, \n",
    "                  cluster_std=0.5, \n",
    "                  shuffle=True, \n",
    "                  random_state=0)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(X[:, 0], X[:, 1], c='steelblue', marker='o', edgecolor='black', s=20)\n",
    "\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "# ax.scatter(X[:, 0], X[:, 1], X[:, 2], c='steelblue', marker='o', edgecolor='black', s=20)\n",
    "\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_01.png', dpi=300)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "y\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "2. Use `KMeans` from `sklearn` to classify data into 3 clusters \n",
    "\n",
    "```\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3, \n",
    "            init='random',  # use standard k-means rather than k-means++ (see below)\n",
    "            n_init=10,      # run 10 times with different random centroids to choose the final model with the lowest SSE\n",
    "            max_iter=300,   # max number of iterations for each run\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "print(y_km)\n",
    "print(f'\\nClusters:\\n {km.cluster_centers_}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "3. Plot the clusters and classified data in a scatterplot \n",
    "\n",
    "```\n",
    "plt.scatter(X[y_km == 0, 0], X[y_km == 0, 1], s=50, c='lightgreen', marker='s', edgecolor='black', label='Cluster 1')\n",
    "plt.scatter(X[y_km == 1, 0], X[y_km == 1, 1], s=50, c='orange', marker='o', edgecolor='black', label='Cluster 2')\n",
    "plt.scatter(X[y_km == 2, 0], X[y_km == 2, 1], s=50, c='lightblue', marker='v', edgecolor='black', label='Cluster 3')\n",
    "# plt.scatter(X[y_km == 3, 0], X[y_km == 3, 1], s=50, c='yellow', marker='*', edgecolor='black', label='Cluster 3')\n",
    "\n",
    "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=250, marker='*', c='red', edgecolor='black', label='Centroids')\n",
    "plt.legend(scatterpoints=1)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_02.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the three dimensional dataset we can plot it as follows:\n",
    "\n",
    "```\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(X[y_km == 0, 0], X[y_km == 0, 1],X[y_km == 0, 2], s=50, c='lightgreen', marker='s', edgecolor='black', label='Cluster 1')\n",
    "ax.scatter(X[y_km == 1, 0], X[y_km == 1, 1],X[y_km == 1, 2], s=50, c='orange', marker='o', edgecolor='black', label='Cluster 2')\n",
    "ax.scatter(X[y_km == 2, 0], X[y_km == 2, 1],X[y_km == 2, 2],s=50, c='lightblue', marker='v', edgecolor='black', label='Cluster 3')\n",
    "\n",
    "ax.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], km.cluster_centers_[:, 2], s=250, marker='*', c='red', edgecolor='black', label='Centroids')\n",
    "plt.legend(scatterpoints=1)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_02.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### A Smarter Way of Placing the Initial Cluster Centroids Using k-Means++ {-}\n",
    "\n",
    "- k-Means++ is an improved method to strategically select initial centroids (rather than randomly choose them as in k-Means)\n",
    "    - Random selection can lead to bad clustering or slow convergence\n",
    "- k-Means++ chooses initial centroids sequentially such that they have high probability of being far away from each other (not belong in the same cluster)\n",
    "    - See textbook for algorithm details if interested\n",
    "- For a good explanation of k-Means++ see here [https://youtu.be/HatwtJSsj5Q](https://youtu.be/HatwtJSsj5Q)\n",
    "- Implement in scikit-learn by setting `init=k-means++` in `KMeans` object (this is default setting)\n",
    "\n",
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "<span style='background:orange'>  **Example: k-means++ clustering**    \n",
    "    \n",
    "1. Repeat the above analysis using k-means++\n",
    "    \n",
    "```\n",
    "km_plus = KMeans(n_clusters=3, \n",
    "            init='k-means++',  # use starndard k-means rather than k-means++ (see below)\n",
    "            n_init=10,      # run 10 times with different random centroids to choose the final model with the lowest SSE\n",
    "            max_iter=300,   # max number of iterations for each run\n",
    "            random_state=0)\n",
    "\n",
    "y_km_plus = km_plus.fit_predict(X)\n",
    "\n",
    "print(y_km_plus)\n",
    "print(f'\\nClusters:\\n {km_plus.cluster_centers_}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "## Using the Elbow Method to Find the Optimal Number of Clusters  {-}\n",
    "\n",
    "- The main challenge in unsupervised learning is that we don't know the values of class labels (classification) or what the target variable is (regression)  \n",
    "    - Need to use metrics, e.g. within-cluster SSE (cluster inertia) to compare the performance of different k-Means clusterings   \n",
    "    - Within-cluster SSE is the sum of the squared distances from each point to its cluster's centroid $SSE=\\sum_i^n\\sum_j^kw^{(i,j)}||x^{(i)}-\\mu^{(j)}||_2^2$\n",
    "    - In scikit-learn use `inertia_` attribute to get within-cluster SSE (sum of squared distances of samples to their closest cluster centre)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Elbow Method** - a heuristic used in determining the number of clusters $k$ using inertia\n",
    "    - Note: A heuristic is an approach to problem-solving that simplifies decision-making to produce efficient, though not necessarily optimal, solutions. \n",
    "    - If $k\\uparrow\\Rightarrow$inertia$\\downarrow$ (but its easy to overfit)\n",
    "    - Plot inertia (within-cluster SSE) vs $k$ (the number of clusters)  \n",
    "    - Choose $k$ so that adding another cluster doesn't provide large improvement in SSE\n",
    "        - **Identify $k$ where inertia stops decreasing rapidly**  \n",
    "        - We say that the elbow of the curve is located at that $k$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "<span style='background:orange'>  **Example: Determining $k$ using the elbow method**  \n",
    "\n",
    "1. Compute within-cluster SSE (inertia) for $k\\in\\{1,2,\\dots,10\\}$ and plot inertia against $k$  \n",
    "2. Determine the optimal $k$ according to the plot.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "1. Compute within-cluster SSE (inertia) for $k\\in\\{1,2,\\dots,10\\}$ and plot inertia against $k$\n",
    "\n",
    "```\n",
    "inertias = [] # empty list\n",
    "\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(n_clusters=i, \n",
    "                init='k-means++', \n",
    "                n_init=10, \n",
    "                max_iter=300, \n",
    "                random_state=0)\n",
    "    km.fit(X)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), inertias, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Cluster inertia (within-cluster SSE)')\n",
    "plt.xticks(range(1,11))\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_03.png', dpi=300)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "2. Determine the optimal $k$ according to the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- According to the above plot the optimal number of clusters is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "## Quantifying the Quality of Clustering  via Silhouette Plots {-}  \n",
    "\n",
    "- Silhouette analysis - graphical tool to plot a measure of how tightly grouped the examples in the clusters are\n",
    "    - Used to assess the quality of clustering in a dataset   \n",
    "    - Tells us how well each object has been classified in its cluster, based on the object's similarity to its own cluster compared to other clusters.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Silhouette coefficient (for an example $x^{(i)}$)   \n",
    "    - Cluster Cohesion - $a^{(i)}$ - the average distance between $x^{(i)}$ and all other points in the same cluster  \n",
    "        - How similar $x^{(i)}$ is to other examples in its own cluster  \n",
    "        - Smaller $a^{(i)}$ is better  \n",
    "    - Cluster Separation - $b^{(i)}$ - the average distance between $x^{(i)}$ and all examples in the nearest cluster  \n",
    "        - Larger $b^{(i)}$ is better  \n",
    "    - Silhouette coefficient - $s^{(i)}=\\frac{b^{(i)}-a^{(i)}}{\\text{max}\\{b^{(i)},a^{(i)}\\}}\\in(-1,1)$\n",
    "        - $s^{(i)}$ close to 1: $x^{(i)}$ is appropriately clustered\n",
    "        - $s^{(i)}$ close to -1: $x^{(i)}$ would be more appropriate if it was clustered in its neighbouring cluster\n",
    "        - $s^{(i)}$ close to 0: $x^{(i)}$ is on the border of two natural clusters         \n",
    "- In scikit-learn use `from sklearn.metrics import silhouette_samples`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<span style='background:orange'>  **Example: Determining $k$ via Silhouette Plots**    \n",
    "\n",
    "1. Group the data into 3 clusters and compute a silhouette coefficient for each example (observation) using `silhouette_samples`  \n",
    "- Plot all silhouette coefficients for each example (from smallest to largest) grouped by clusters  \n",
    "- Compute and plot the average silhouette coefficient across all observations  \n",
    "    \n",
    "2. Repeat 1. by grouping data into only 2 clusters and observe the difference in silhouette coefficients  \n",
    "    \n",
    "\n",
    "---\n",
    "```\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "km = KMeans(n_clusters=3,  \n",
    "            init='k-means++', \n",
    "            n_init=10, \n",
    "            max_iter=300,\n",
    "            tol=1e-04,\n",
    "            random_state=0)\n",
    "\n",
    "y_km = km.fit_predict(X)\n",
    "# print(y_km)\n",
    "\n",
    "cluster_labels = np.unique(y_km)\n",
    "# print(cluster_labels)\n",
    "\n",
    "n_clusters = cluster_labels.shape[0]\n",
    "\n",
    "silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')\n",
    "# print('silhouette_vals\\n', silhouette_vals)\n",
    "\n",
    "## ------- plotting silhouette values -------\n",
    "\n",
    "y_ax_lower, y_ax_upper = 0, 0\n",
    "yticks = []\n",
    "\n",
    "for i, c in enumerate(cluster_labels):\n",
    "    c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "    c_silhouette_vals.sort()\n",
    "    y_ax_upper += len(c_silhouette_vals)\n",
    "    color = cm.jet(float(i) / n_clusters)\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color)\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "    y_ax_lower += len(c_silhouette_vals)\n",
    "    \n",
    "    \n",
    "silhouette_avg = np.mean(silhouette_vals)\n",
    "print(f'silhouette_avg: {silhouette_avg:.2f}')\n",
    "\n",
    "plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\") # plot vertical average line\n",
    "\n",
    "plt.yticks(yticks, cluster_labels + 1)\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Silhouette coefficient')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_04.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- None of the silhouette coefficeint are close to 0 -> an indicator of good clustering\n",
    "- Average silhouette coefficient is 0.83 (out of the maximum of 1) -> also a good sign "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Organizing Clusters as a Hierarchical Tree {-}  \n",
    "\n",
    "- Hierarchical Clustering    \n",
    "    - Method of cluster analysis which seeks to build a hierarchy of clusters    \n",
    "\n",
    "<img src=\"images/11_18.png\" alt=\"Drawing\" style=\"width: 400px;\"/>  \n",
    "\n",
    "- There are two approaches to hierarchical clustering  \n",
    "    - Agglomerative (bottom-up approach) - covered in this lecture  \n",
    "        1. Start by assuming that each example is a single cluster  \n",
    "        2. Merge closest pairs of clusters iteratively until only one cluster remains  \n",
    "    - Divisive (top-down approach)  \n",
    "        1. Start with one cluster  \n",
    "        2. Split the cluster into smaller clusters iteratively until each cluster contains only one example  \n",
    "\n",
    "- For a good explanation of hierarchical clustering see [https://youtu.be/rg2cjfMsCk4](https://youtu.be/rg2cjfMsCk4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "## Grouping Clusters in Agglomerative (Bottom-Up) Approach {-}  \n",
    "\n",
    "- Key Operation: Combine two nearest clusters\n",
    "- Key Problem: How to measure distance between clusters\n",
    "\n",
    "\n",
    "There are two standard algorithms for agglomerative hierarchical clustering:  \n",
    "1. **Single Linkage Approach**    \n",
    "- Compute the distances between the **most similar** members for each pair of clusters and merge the two clusters for which the distance between the most similar members is the smallest  \n",
    "2. **Complete Linkage Approach**    \n",
    "- Compute the distance between the **most dissimilar** members for each pair of clusters and merge the two clusters for which the distance between the most dissimilar members is the smallest  \n",
    "\n",
    "<img src=\"images/11_07.png\" alt=\"Drawing\" style=\"width: 400px;\"/>  \n",
    "\n",
    "**Agglomerative Complete Linkage Algorithm**  \n",
    "1. Compute the distance matrix of all examples containing distances between all data points  \n",
    "2. Represent each data point as a cluster  \n",
    "3. Merge the two closest clusters based on the distance between the most dissimilar members  \n",
    "4. Update the similarity matrix (containing distance metrics)\n",
    "5. Repeat 3 - 4 until only one cluster remains  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualising Hierarchical Clusters**\n",
    "\n",
    "- A **dendrogram** is a tree-like diagram that is used to illustrate the arrangement of the elements or clusters formed during hierarchical clustering analysis. \n",
    "    - Visually displays the sequence of cluster mergings and the distance at which each merging occurred.\n",
    "    - Particularly useful in agglomerative (bottom-up) hierarchical clustering, although they can represent divisive (top-down) clustering processes as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "<span style='background:orange'> **Example: Clustering Data using hierarchical/agglomerative clustering with Euclidean norm & visualize linkage matrix in a dendrogram (using both scipy and scikit-learn)**    \n",
    "\n",
    "1. Generate data for 3 random variables X, Y and Z to be used in hierarchical cluster analysis using [https://numpy.org/doc/stable/reference/random/generated/numpy.random.random_sample.html](https://numpy.org/doc/stable/reference/random/generated/numpy.random.random_sample.html)  \n",
    "2. Compute the distance matrix with Euclidean norm using  \n",
    "[https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html)  \n",
    "3. Use [https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) to compute the **linkage matrix** and perform hierarchical/agglomerative clustering  \n",
    "4. Visualize the linkage matrix in a dendrogram  \n",
    "5. Plot the data in a heatmap and attach the dendrogram to it  \n",
    "6. Repeat the agglomerative clustering analysis in scikit-learn   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "1. Generate data for 3 random variables X, Y and Z to be used in hierarchical cluster analysis using [https://numpy.org/doc/stable/reference/random/generated/numpy.random.random_sample.html](https://numpy.org/doc/stable/reference/random/generated/numpy.random.random_sample.html)\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "variables = ['X', 'Y', 'Z']\n",
    "labels = ['ID_0', 'ID_1', 'ID_2', 'ID_3', 'ID_4']\n",
    "\n",
    "X = np.random.random_sample([5, 3])*10\n",
    "df = pd.DataFrame(X, columns=variables, index=labels)\n",
    "df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "2. Compute the distance matrix using Euclidean norm using\n",
    "[https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html)\n",
    "- Print `squareform`\n",
    "- Print `condensed form`\n",
    "\n",
    "```\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "row_dist = pd.DataFrame(squareform(pdist(df, metric='euclidean')),\n",
    "                        columns=labels,\n",
    "                        index=labels)\n",
    "\n",
    "print(row_dist)\n",
    "\n",
    "\n",
    "pd.Series(pdist(df, metric='euclidean'))\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can check the distance between ID_0 and ID_1\n",
    "\n",
    "```\n",
    "\n",
    "print('ID_0 \\n', df.iloc[0,:])\n",
    "print('ID_1 \\n',df.iloc[1,:])\n",
    "\n",
    "print('(ID_0 - ID_1)^2 \\n',(df.iloc[0,:] - df.iloc[1,:])**2)\n",
    "print('sum((ID_0 - ID_1)^2) \\n', np.sqrt(np.sum((df.iloc[0,:] - df.iloc[1,:])**2)))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "3. Use [https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) to compute the **linkage matrix** and perform hierarchical/agglomerative clustering  \n",
    "\n",
    "- Either pass a condensed distance matrix (upper triangular) from the `pdist` function, or   \n",
    "- Pass the \"original\" data array and define the `metric='euclidean'` argument in `linkage`.   \n",
    "\n",
    "However, we should not pass the squareform distance matrix, which would yield different distance values although the overall clustering could be the same.  \n",
    "\n",
    "```\n",
    "# ---- 1st Approach: input condensed distance matrix ----\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "\n",
    "row_clusters = linkage(pdist(df, metric='euclidean'), method='complete')\n",
    "pd.DataFrame(row_clusters,\n",
    "             columns=['row label 1', 'row label 2',\n",
    "                      'distance', 'no. of items in clust.'],\n",
    "             index=[f'cluster {i + 1}' for i in range(row_clusters.shape[0])])\n",
    "             \n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- 2nd Approach: just input data -----\n",
    "\n",
    "```\n",
    "row_clusters = linkage(df.values, method='complete', metric='euclidean')\n",
    "\n",
    "pd.DataFrame(row_clusters,\n",
    "             columns=['row label 1', 'row label 2',\n",
    "                      'distance', 'no. of items in clust.'],\n",
    "             index=[f'cluster {i + 1}' for i in range(row_clusters.shape[0])])\n",
    "\n",
    "#              index=['cluster %d' % (i + 1)\n",
    "#                     for i in range(row_clusters.shape[0])])             \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  \n",
    "- Each row presents one merge  \n",
    "- 1st and 2nd columns denote most dissimilar members in each cluster  \n",
    "- 3rd column - distance between those most dissimilar members  \n",
    "- 4th column - count of members in each cluster  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "4. Visualize the linkage matrix in a dendrogram\n",
    "- A dendrogram shows how we merged datapoints into clusters\n",
    "\n",
    "```\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make dendrogram black (part 1/2)\n",
    "\n",
    "from scipy.cluster.hierarchy import set_link_color_palette\n",
    "set_link_color_palette(['black'])\n",
    "\n",
    "row_dendr = dendrogram(row_clusters, \n",
    "                       labels=labels,\n",
    "                       # make dendrogram black (part 2/2)\n",
    "                       color_threshold=np.inf\n",
    "                       )\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Euclidean distance')\n",
    "#plt.savefig('images/11_11.png', dpi=300, \n",
    "#            bbox_inches='tight')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "5. Plot the data in a heatmap and attach the dendrogram to it\n",
    "\n",
    "```\n",
    "# plot row dendrogram\n",
    "fig = plt.figure(figsize=(8, 8), facecolor='white')\n",
    "axd = fig.add_axes([0.09, 0.1, 0.2, 0.6])\n",
    "\n",
    "# note: for matplotlib < v1.5.1, please use orientation='right'\n",
    "row_dendr = dendrogram(row_clusters, orientation='left')\n",
    "# print(row_dendr)\n",
    "\n",
    "# reorder data with respect to clustering\n",
    "df_rowclust = df.iloc[row_dendr['leaves'][::-1]]\n",
    "\n",
    "axd.set_xticks([])\n",
    "axd.set_yticks([])\n",
    "\n",
    "# remove axes spines from dendrogram\n",
    "for i in axd.spines.values():\n",
    "    i.set_visible(False)\n",
    "\n",
    "# plot heatmap\n",
    "axm = fig.add_axes([0.23, 0.1, 0.6, 0.6])  # x-pos, y-pos, width, height\n",
    "cax = axm.matshow(df_rowclust, interpolation='nearest', cmap='hot_r')\n",
    "fig.colorbar(cax)\n",
    "axm.set_xticklabels([''] + list(df_rowclust.columns))\n",
    "axm.set_yticklabels([''] + list(df_rowclust.index))\n",
    "\n",
    "#plt.savefig('images/11_12.png', dpi=300)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\"> \n",
    "\n",
    "6. Repeat the agglomerative clustering analysis in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying agglomerative clustering via scikit-learn {-}\n",
    "\n",
    "Apply agglomerative clustering in scikit-learn to identify \n",
    "- 3 clusters\n",
    "- 2 clusters\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# ---------- 2 clusters --------------\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=2, \n",
    "                             affinity='euclidean', \n",
    "                             linkage='complete')\n",
    "labels = ac.fit_predict(X)\n",
    "print('Cluster labels: %s' % labels)\n",
    "\n",
    "# ---------- 3 clusters --------------\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=3, \n",
    "                             affinity='euclidean', \n",
    "                             linkage='complete')\n",
    "labels = ac.fit_predict(X)\n",
    "print('Cluster labels: %s' % labels)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Locating Regions of High Density via DBSCAN  {-}\n",
    "\n",
    "- DBSCAN (Density-based spatial clustering of applications with noise)  \n",
    "    - Group clusters based on dense regions of points  \n",
    "    -  <span style='background:lightgrey'> **Density**: number of points within a specified radius $\\varepsilon$  \n",
    "    \n",
    "- Each observation is classified as follows  \n",
    "    - Core Point: if at least a specified number (MinPts) of neighboring points fall within the specified radius $\\varepsilon$  \n",
    "    - Border Point: has less neighbours than MinPts within $\\varepsilon$, but lies within the $\\varepsilon$ radius of a core point  \n",
    "    - Noise Point: any point that is neither a core nor border point  \n",
    "    \n",
    "<img src=\"images/11_13.png\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "**DBSCAN Algorithm**  \n",
    "\n",
    "1. Specify the radius $\\varepsilon$  \n",
    "1. Set the minimum number of neighbouring points - MinPts   \n",
    "2. Label each point as either a core, border or noise point  \n",
    "3. Form a separate cluster for each core point or connected group of core points (core points are connected if they are no further away than $\\varepsilon$) \n",
    "4. Assign each border point to the cluster of its corresponding core point  \n",
    "\n",
    "Advantages of DBSCAN:   \n",
    "1. Not all points are assigned to a cluster since points can be removed as noise points  \n",
    "2. Does not require *a-priori* specification of number of clusters  \n",
    "\n",
    "In scikit-learn   \n",
    "- `from sklearn.cluster import DBSCAN`  \n",
    "- [https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">   \n",
    "\n",
    "<span style='background:orange'> **Example: Cluster make_moons with K-means,  hierarchical clustering and DBSCAN**    \n",
    "1. Create a linearly inseparable dataset using `make_moons`   \n",
    "2. Attempt to identify the two clusters using K-means and hierarchical clustering in scikit-learn  \n",
    "3. Cluster the data on the basis of DBSCAN  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">\n",
    "1. Create a linearly inseparable dataset using `make_moons` \n",
    "\n",
    "```\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_14.png', dpi=300)\n",
    "plt.show()\n",
    "# X\n",
    "# y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">\n",
    "\n",
    "2. Attempt to identify the two clusters using k-Means and hierarchical clustering in scikit-learn\n",
    "\n",
    "```\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "#----------------- k-means clustering with 2 clusters ------------------\n",
    "km = KMeans(n_clusters=2, random_state=0)\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "\n",
    "ax1.scatter(X[y_km == 0, 0], X[y_km == 0, 1],edgecolor='black', c='lightblue', marker='o', s=40, label='cluster 1')\n",
    "ax1.scatter(X[y_km == 1, 0], X[y_km == 1, 1], edgecolor='black', c='red', marker='s', s=40, label='cluster 2')\n",
    "ax1.set_title('K-means clustering')\n",
    "\n",
    "#----------------- agglomerative clustering using two clusters --------------\n",
    "ac = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')\n",
    "y_ac = ac.fit_predict(X)\n",
    "\n",
    "ax2.scatter(X[y_ac == 0, 0], X[y_ac == 0, 1], c='lightblue', edgecolor='black', marker='o', s=40, label='Cluster 1')\n",
    "ax2.scatter(X[y_ac == 1, 0], X[y_ac == 1, 1], c='red', edgecolor='black', marker='s', s=40, label='Cluster 2')\n",
    "ax2.set_title('Agglomerative clustering')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_15.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print('clusters accroding to KMeans\\n', y_km)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:35%;margin-left:0;\">\n",
    "\n",
    "3. Cluster the data on the basis of DBSCAN\n",
    "\n",
    "```\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=0.3, min_samples=10, metric='euclidean')\n",
    "\n",
    "y_db = db.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[y_db == 0, 0], X[y_db == 0, 1], c='lightblue', marker='o', s=40, edgecolor='black',  label='Cluster 1')\n",
    "plt.scatter(X[y_db == 1, 0], X[y_db == 1, 1], c='red', marker='s', s=40, edgecolor='black', label='Cluster 2')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/11_16.png', dpi=300)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
